{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54deb97e-5df2-4f7e-bf53-eff24a8ad6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05HW] Xw=(439, 3, 15, 32), Yw=(439,), class_cnt=Counter({3: 229, 2: 155, 4: 55})\n",
      "[14IB] Xw=(2459, 3, 15, 32), Yw=(2459,), class_cnt=Counter({2: 1584, 0: 766, 3: 58, 4: 51})\n",
      "[ALL] (2898, 3, 15, 32) Counter({2: 1739, 0: 766, 3: 287, 4: 106})\n",
      "[SPLIT]\n",
      " Train: (2318, 3, 15, 32) Counter({2: 1391, 0: 613, 3: 229, 4: 85})\n",
      " Val  : (290, 3, 15, 32) Counter({2: 174, 0: 77, 3: 29, 4: 10})\n",
      " Test : (290, 3, 15, 32) Counter({2: 174, 0: 76, 3: 29, 4: 11})\n",
      "[OK] Saved to data/train_stgcn.npz, data/val_stgcn.npz, data/test_stgcn.npz\n"
     ]
    }
   ],
   "source": [
    "# ==== Âè™Áî® 05HW & 14IBÔºöÂØπÈΩê -> Á™óÂè£Âåñ -> ÊãºÊé• -> ÂàíÂàÜ -> ‰øùÂ≠ò ====\n",
    "import os, numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "FIVE = ['walk','lie','stand','sit','bend']\n",
    "label_to_id = {s:i for i,s in enumerate(FIVE)}\n",
    "\n",
    "def load_one(poses_path, labels_path, kept_idx_path):\n",
    "    X = np.load(poses_path)                         # (T, 32, 3)  ‚Äî‚Äî ‰Ω†Â∑≤ÁªèÂÅöËøá center/scale ÂíåÂéªÈõ∂\n",
    "    y = np.load(labels_path, allow_pickle=True)     # (T,)\n",
    "    keep = np.load(kept_idx_path)                   # (K,)\n",
    "    # ÂêàÊ≥ïÂåñ keepÔºåÂπ∂ÂØπÈΩê‰∏âËÄÖ\n",
    "    T = len(X)\n",
    "    keep = keep[(keep >= 0) & (keep < T)]\n",
    "    X = X[keep]\n",
    "    y = y[keep]\n",
    "    # Ê†áÁ≠æÊò†Â∞ÑÔºöÂ¶ÇÊûúÂ∑≤ÁªèÊòØ int 0..4 Â∞±‰∏çÂä®ÔºõÂ¶ÇÊûúÊòØÂ≠óÁ¨¶‰∏≤/None Â§ÑÁêÜ‰∏Ä‰∏ã\n",
    "    if y.dtype.kind in ('U','S','O'):\n",
    "        y = np.array([label_to_id.get(str(v), -1) if v is not None else -1 for v in y], dtype=np.int64)\n",
    "    else:\n",
    "        y = y.astype(np.int64, copy=False)\n",
    "    return X, y  # X:(K,32,3), y:(K,)\n",
    "\n",
    "def windowize(X, y, win=15, stride=5, strict=False, mode='center'):\n",
    "    \"\"\"X:(T,32,3), y:(T,), ËøîÂõû Xw:(N,3,win,32), Yw:(N,)\"\"\"\n",
    "    T = len(X)\n",
    "    starts = range(0, max(0, T - win + 1), stride)\n",
    "    Xw, Yw = [], []\n",
    "    for s in starts:\n",
    "        seg = X[s:s+win]           # (win,32,3)\n",
    "        labs = y[s:s+win]          # (win,)\n",
    "        if len(seg) < win:  # ‰øùÈô©Ôºö‰∏çÂ§üÈïøÂ∞±Ë∑≥Ëøá\n",
    "            continue\n",
    "        if strict:\n",
    "            if np.any(labs < 0):   # ÊúâÊó†ÊïàÊ†áÁ≠æ\n",
    "                continue\n",
    "        if mode == 'center':\n",
    "            c = labs[win//2]\n",
    "            if c < 0:              # ‰∏≠ÂøÉÊó†Êïà\n",
    "                continue\n",
    "            yw = int(c)\n",
    "        else:  # majority\n",
    "            votes = labs[labs >= 0]\n",
    "            if len(votes) == 0:\n",
    "                continue\n",
    "            cnt = np.bincount(votes, minlength=5)\n",
    "            yw = int(cnt.argmax())\n",
    "        # (win,32,3) -> (3,win,32)\n",
    "        seg_ctv = np.transpose(seg.astype(np.float32), (2,0,1))\n",
    "        Xw.append(seg_ctv)\n",
    "        Yw.append(yw)\n",
    "    if len(Xw) == 0:\n",
    "        return np.empty((0,3,win,32), np.float32), np.empty((0,), np.int64)\n",
    "    return np.stack(Xw, axis=0), np.asarray(Yw, dtype=np.int64)\n",
    "\n",
    "def prepare_split(name, win=15, stride=5, strict=False, mode='center'):\n",
    "    x_path = os.path.join(DATA_DIR, f\"poses_{name}_cs_nz.npy\")\n",
    "    y_path = os.path.join(DATA_DIR, f\"labels_{name}.npy\")\n",
    "    k_path = os.path.join(DATA_DIR, f\"poses_{name}_kept_idx.npy\")\n",
    "    X, y = load_one(x_path, y_path, k_path)\n",
    "    Xw, Yw = windowize(X, y, win=win, stride=stride, strict=strict, mode=mode)\n",
    "    print(f\"[{name}] Xw={Xw.shape}, Yw={Yw.shape}, class_cnt={Counter(Yw.tolist())}\")\n",
    "    return Xw, Yw\n",
    "\n",
    "# ---- Âè™Áî®‰∏§‰∏™Êñ∞Êï∞ÊçÆ ----\n",
    "X05, y05 = prepare_split(\"05HW\", win=15, stride=5, strict=False, mode='center')\n",
    "X14, y14 = prepare_split(\"14IB\", win=15, stride=5, strict=False, mode='center')\n",
    "\n",
    "# ---- ÂêàÂπ∂Âπ∂ÂàÜÂ±ÇÂàíÂàÜ 8/1/1 ----\n",
    "X_all = np.concatenate([X05, X14], axis=0)\n",
    "y_all = np.concatenate([y05, y14], axis=0)\n",
    "print(\"[ALL]\", X_all.shape, Counter(y_all.tolist()))\n",
    "\n",
    "# ÂÖà train/valtestÔºåÂÜçÊää valtest ÂØπÂçäÂàÜ\n",
    "X_tr, X_vt, y_tr, y_vt = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n",
    "X_va, X_te, y_va, y_te = train_test_split(X_vt, y_vt, test_size=0.5, random_state=42, stratify=y_vt)\n",
    "\n",
    "print(\"[SPLIT]\")\n",
    "print(\" Train:\", X_tr.shape, Counter(y_tr.tolist()))\n",
    "print(\" Val  :\", X_va.shape, Counter(y_va.tolist()))\n",
    "print(\" Test :\", X_te.shape, Counter(y_te.tolist()))\n",
    "\n",
    "# ---- ‰øùÂ≠ò‰∏∫ stgcn Áî® npzÔºàmeta ÈáåÂ∏¶‰∏ä 5 Á±ªÊ†áÁ≠æÈ°∫Â∫èÔºâ----\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "np.savez_compressed(os.path.join(DATA_DIR, \"train_stgcn.npz\"), X=X_tr, Y=y_tr, meta=dict(labels=FIVE))\n",
    "np.savez_compressed(os.path.join(DATA_DIR, \"val_stgcn.npz\"),   X=X_va, Y=y_va, meta=dict(labels=FIVE))\n",
    "np.savez_compressed(os.path.join(DATA_DIR, \"test_stgcn.npz\"),  X=X_te, Y=y_te, meta=dict(labels=FIVE))\n",
    "print(\"[OK] Saved to data/train_stgcn.npz, data/val_stgcn.npz, data/test_stgcn.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0911150-8476-482d-93b4-df5c2cde5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DATASETS = [\n",
    "    \"16GZ\",\n",
    "    \"17JP\",\n",
    "    \"19MM\",\n",
    "    \"09SY\",\n",
    "    \"05HW\",\n",
    "    \"14IB\"\n",
    "]\n",
    "\n",
    "DATA_DIR = \"data\"   # Â¶ÇÊûú‰Ω†ÁöÑ npy Âú® data/\n",
    "\n",
    "def fix_one(name):\n",
    "    print(f\"\\n===== ‰øÆÂ§ç {name} =====\")\n",
    "\n",
    "    pose_path = os.path.join(DATA_DIR, f\"poses_{name}.npy\")\n",
    "    keep_path = os.path.join(DATA_DIR, f\"poses_{name}_kept_idx.npy\")\n",
    "    label_path = os.path.join(DATA_DIR, f\"labels_{name}.npy\")\n",
    "\n",
    "    X = np.load(pose_path)              # (T_pose, 32, 3)\n",
    "    T_pose = len(X)\n",
    "\n",
    "    if os.path.exists(keep_path):\n",
    "        keep = np.load(keep_path)       # (K,)\n",
    "    else:\n",
    "        print(f\"[WARNING] {name} Ê≤°Êúâ kept_idxÔºåË∑≥Ëøá\")\n",
    "        return\n",
    "\n",
    "    # ---- ‰øÆÂ§ç kept_idx Ë∂äÁïå ----\n",
    "    bad = keep >= T_pose\n",
    "    n_bad = bad.sum()\n",
    "\n",
    "    if n_bad > 0:\n",
    "        print(f\"[fix] {name}: ÂèëÁé∞ {n_bad} ‰∏™Ë∂äÁïå kept_idxÔºåÂ∑≤Âà†Èô§\")\n",
    "        keep_clean = keep[~bad]\n",
    "        np.save(keep_path, keep_clean)\n",
    "    else:\n",
    "        print(f\"[OK] {name}: kept_idx ÂÆåÂÖ®ÂêàÊ≥ï\")\n",
    "\n",
    "    # ---- ÈáçÊñ∞ÁªüËÆ° label ÊÉÖÂÜµÔºàÂ¶ÇÊûúÊúâ labelÔºâ ----\n",
    "    if os.path.exists(label_path):\n",
    "        y = np.load(label_path)\n",
    "        print(f\"Ê†áÁ≠æÊï∞Èáè: {len(y)}, poseÊï∞Èáè: {T_pose}, ÊúâÊïàÁéá={len(y)/T_pose:.2f}\")\n",
    "    else:\n",
    "        print(\"Êó† label Êñá‰ª∂\")\n",
    "\n",
    "    print(f\"[ÂÆåÊàê] {name} ‰øÆÂ§çÂÆåÊàê\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ËøêË°åÂÖ®ÈÉ®Êï∞ÊçÆÈõÜ\n",
    "# =====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f6bb14-4858-42d1-b72a-52add1920ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂºÄÂßãÊµãËØïÊâìÂç∞...\n",
      "\n",
      "=====  16GZ  =====\n",
      "pose_path : data\\poses_16GZ.npy Â≠òÂú®Ôºü True\n",
      "keep_path : data\\poses_16GZ_kept_idx.npy Â≠òÂú®Ôºü True\n",
      "label_path: data\\labels_16GZ.npy Â≠òÂú®Ôºü True\n",
      "\n",
      "=====  17JP  =====\n",
      "pose_path : data\\poses_17JP.npy Â≠òÂú®Ôºü True\n",
      "keep_path : data\\poses_17JP_kept_idx.npy Â≠òÂú®Ôºü True\n",
      "label_path: data\\labels_17JP.npy Â≠òÂú®Ôºü True\n",
      "\n",
      "=====  19MM  =====\n",
      "pose_path : data\\poses_19MM.npy Â≠òÂú®Ôºü True\n",
      "keep_path : data\\poses_19MM_kept_idx.npy Â≠òÂú®Ôºü True\n",
      "label_path: data\\labels_19MM.npy Â≠òÂú®Ôºü True\n",
      "\n",
      "=====  09SY  =====\n",
      "pose_path : data\\poses_09SY.npy Â≠òÂú®Ôºü True\n",
      "keep_path : data\\poses_09SY_kept_idx.npy Â≠òÂú®Ôºü True\n",
      "label_path: data\\labels_09SY.npy Â≠òÂú®Ôºü True\n",
      "\n",
      "=====  05HW  =====\n",
      "pose_path : data\\poses_05HW.npy Â≠òÂú®Ôºü True\n",
      "keep_path : data\\poses_05HW_kept_idx.npy Â≠òÂú®Ôºü True\n",
      "label_path: data\\labels_05HW.npy Â≠òÂú®Ôºü True\n",
      "\n",
      "=====  14IB  =====\n",
      "pose_path : data\\poses_14IB.npy Â≠òÂú®Ôºü True\n",
      "keep_path : data\\poses_14IB_kept_idx.npy Â≠òÂú®Ôºü True\n",
      "label_path: data\\labels_14IB.npy Â≠òÂú®Ôºü True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATASETS = [\"16GZ\",\"17JP\",\"19MM\",\"09SY\",\"05HW\",\"14IB\"]\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "print(\"ÂºÄÂßãÊµãËØïÊâìÂç∞...\")\n",
    "\n",
    "for name in DATASETS:\n",
    "    pose_path = os.path.join(DATA_DIR, f\"poses_{name}.npy\")\n",
    "    keep_path = os.path.join(DATA_DIR, f\"poses_{name}_kept_idx.npy\")\n",
    "    label_path = os.path.join(DATA_DIR, f\"labels_{name}.npy\")\n",
    "\n",
    "    print(\"\\n===== \", name, \" =====\")\n",
    "    print(\"pose_path :\", pose_path, \"Â≠òÂú®Ôºü\", os.path.exists(pose_path))\n",
    "    print(\"keep_path :\", keep_path, \"Â≠òÂú®Ôºü\", os.path.exists(keep_path))\n",
    "    print(\"label_path:\", label_path, \"Â≠òÂú®Ôºü\", os.path.exists(label_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9da7e12-a2c2-41ed-b700-d92ac0a51d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== ÂºÄÂßã‰øÆÂ§çÊâÄÊúâÊï∞ÊçÆÈõÜ ========\n",
      "\n",
      "\n",
      "===== ‰øÆÂ§ç 16GZ =====\n",
      "16GZ: pose=33409, kept_idx=16970\n",
      "16GZ: bad kept_idx=0\n",
      "[OK] Êó†Ë∂äÁïå\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m flush_print(\u001b[33m\"\u001b[39m\u001b[33m======== ÂºÄÂßã‰øÆÂ§çÊâÄÊúâÊï∞ÊçÆÈõÜ ========\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m DATASETS:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     fix_one(name)\n\u001b[32m     56\u001b[39m flush_print(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ ÂÖ®ÈÉ®Êï∞ÊçÆÈõÜÂ∑≤‰øÆÂ§çÂÆåÊØï\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mfix_one\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     42\u001b[39m     flush_print(\u001b[33m\"\u001b[39m\u001b[33m[OK] Êó†Ë∂äÁïå\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(label_path):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     y = np.load(label_path)\n\u001b[32m     46\u001b[39m     flush_print(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: labels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, valid_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y)/K\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:483\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.open_memmap(file, mode=mmap_mode,\n\u001b[32m    481\u001b[39m                                   max_header_size=max_header_size)\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.read_array(fid, allow_pickle=allow_pickle,\n\u001b[32m    484\u001b[39m                                  pickle_kwargs=pickle_kwargs,\n\u001b[32m    485\u001b[39m                                  max_header_size=max_header_size)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:833\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.hasobject:\n\u001b[32m    831\u001b[39m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mObject arrays cannot be loaded when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    834\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33mallow_pickle=False\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    836\u001b[39m         pickle_kwargs = {}\n",
      "\u001b[31mValueError\u001b[39m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "DATASETS = [\"16GZ\",\"17JP\",\"19MM\",\"09SY\",\"05HW\",\"14IB\"]\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "def flush_print(*msg):\n",
    "    print(*msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def fix_one(name):\n",
    "    flush_print(f\"\\n===== ‰øÆÂ§ç {name} =====\")\n",
    "\n",
    "    pose_path = os.path.join(DATA_DIR, f\"poses_{name}.npy\")\n",
    "    keep_path = os.path.join(DATA_DIR, f\"poses_{name}_kept_idx.npy\")\n",
    "    label_path = os.path.join(DATA_DIR, f\"labels_{name}.npy\")\n",
    "\n",
    "    if not os.path.exists(pose_path):\n",
    "        flush_print(f\"[ERR] Áº∫Â∞ë pose: {pose_path}\")\n",
    "        return\n",
    "    if not os.path.exists(keep_path):\n",
    "        flush_print(f\"[WARN] Áº∫Â∞ë kept_idx: {keep_path}\")\n",
    "        return\n",
    "\n",
    "    X = np.load(pose_path)\n",
    "    K = len(X)\n",
    "    keep = np.load(keep_path)\n",
    "\n",
    "    flush_print(f\"{name}: pose={K}, kept_idx={len(keep)}\")\n",
    "\n",
    "    bad = keep >= K\n",
    "    n_bad = bad.sum()\n",
    "\n",
    "    flush_print(f\"{name}: bad kept_idx={n_bad}\")\n",
    "\n",
    "    if n_bad > 0:\n",
    "        keep = keep[~bad]\n",
    "        np.save(keep_path, keep)\n",
    "        flush_print(f\"[FIXED] Êñ∞ kept_idx={len(keep)}\")\n",
    "    else:\n",
    "        flush_print(\"[OK] Êó†Ë∂äÁïå\")\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        y = np.load(label_path)\n",
    "        flush_print(f\"{name}: labels={len(y)}, valid_rate={len(y)/K:.2f}\")\n",
    "    else:\n",
    "        flush_print(\"[WARN] Êó† label Êñá‰ª∂\")\n",
    "\n",
    "\n",
    "flush_print(\"======== ÂºÄÂßã‰øÆÂ§çÊâÄÊúâÊï∞ÊçÆÈõÜ ========\\n\")\n",
    "\n",
    "for name in DATASETS:\n",
    "    fix_one(name)\n",
    "\n",
    "flush_print(\"\\n‚úÖ ÂÖ®ÈÉ®Êï∞ÊçÆÈõÜÂ∑≤‰øÆÂ§çÂÆåÊØï\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba19de2-4d72-4de4-bcdf-e3dac5a5f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 16GZ =====\n",
      "16GZ: after keep ‚Üí X=(9797, 32, 3), y=9797, class_cnt=Counter({2: 4948, 0: 1694, 3: 1104, 4: 184})\n",
      "16GZ: window ‚Üí Xw=(1662, 3, 15, 32), Yw=(1662,), class_cnt=Counter({np.int64(2): 1013, np.int64(0): 356, np.int64(3): 234, np.int64(4): 59})\n",
      "\n",
      "===== 17JP =====\n",
      "17JP: after keep ‚Üí X=(14189, 32, 3), y=14189, class_cnt=Counter({2: 2668, 0: 2549, 3: 784, 4: 384})\n",
      "17JP: window ‚Üí Xw=(1442, 3, 15, 32), Yw=(1442,), class_cnt=Counter({np.int64(2): 587, np.int64(0): 554, np.int64(3): 174, np.int64(4): 127})\n",
      "\n",
      "===== 19MM =====\n",
      "19MM: after keep ‚Üí X=(7654, 32, 3), y=7654, class_cnt=Counter({2: 2950, 0: 2291, 3: 503, 4: 385, 1: 4})\n",
      "19MM: window ‚Üí Xw=(1331, 3, 15, 32), Yw=(1331,), class_cnt=Counter({np.int64(2): 628, np.int64(0): 470, np.int64(4): 122, np.int64(3): 109, np.int64(1): 2})\n",
      "\n",
      "===== 09SY =====\n",
      "09SY: after keep ‚Üí X=(11520, 32, 3), y=11520, class_cnt=Counter({2: 2752, 0: 489})\n",
      "09SY: window ‚Üí Xw=(651, 3, 15, 32), Yw=(651,), class_cnt=Counter({np.int64(2): 554, np.int64(0): 97})\n",
      "\n",
      "===== 05HW =====\n",
      "05HW: after keep ‚Üí X=(13292, 32, 3), y=13292, class_cnt=Counter({3: 1159, 2: 785, 4: 279})\n",
      "05HW: window ‚Üí Xw=(548, 3, 15, 32), Yw=(548,), class_cnt=Counter({np.int64(3): 257, np.int64(2): 213, np.int64(4): 78})\n",
      "\n",
      "===== 14IB =====\n",
      "14IB: after keep ‚Üí X=(14594, 32, 3), y=14594, class_cnt=Counter({2: 7895, 0: 3857, 3: 294, 4: 262})\n",
      "14IB: window ‚Üí Xw=(2614, 3, 15, 32), Yw=(2614,), class_cnt=Counter({np.int64(2): 1635, np.int64(0): 806, np.int64(4): 92, np.int64(3): 81})\n",
      "\n",
      "===== ALL merged =====\n",
      "X=(8248, 3, 15, 32), Y=(8248,), class_cnt=Counter({np.int64(2): 4630, np.int64(0): 2283, np.int64(3): 855, np.int64(4): 478, np.int64(1): 2})\n",
      "\n",
      "===== Oversample =====\n",
      "After oversample: Counter({np.int64(0): 4630, np.int64(1): 4630, np.int64(2): 4630, np.int64(3): 4630, np.int64(4): 4630})\n",
      "\n",
      "===== FINAL =====\n",
      "Train: (16205, 3, 15, 32) Counter({np.int64(0): 3241, np.int64(1): 3241, np.int64(2): 3241, np.int64(3): 3241, np.int64(4): 3241})\n",
      "Val  : (3470, 3, 15, 32) Counter({np.int64(0): 694, np.int64(1): 694, np.int64(2): 694, np.int64(3): 694, np.int64(4): 694})\n",
      "Test : (3475, 3, 15, 32) Counter({np.int64(0): 695, np.int64(1): 695, np.int64(2): 695, np.int64(3): 695, np.int64(4): 695})\n",
      "\n",
      "‚úÖ DONE! ÂÖ®ÊµÅÁ®ãÂÆåÊàêÔºÅ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "DATASETS = [\"16GZ\", \"17JP\", \"19MM\", \"09SY\", \"05HW\", \"14IB\"]\n",
    "FIVE = [\"walk\", \"lie\", \"stand\", \"sit\", \"bend\"]\n",
    "MAP = {\"walk\":0, \"lie\":1, \"stand\":2, \"sit\":3, \"bend\":4}\n",
    "\n",
    "def load_np(path):\n",
    "    \"\"\"Ëá™Âä® fallback ‚Üí allow_pickle=True\"\"\"\n",
    "    try:\n",
    "        return np.load(path)\n",
    "    except:\n",
    "        return np.load(path, allow_pickle=True)\n",
    "\n",
    "def load_pose(name):\n",
    "    return load_np(os.path.join(DATA_DIR, f\"poses_{name}_cs_nz.npy\"))\n",
    "\n",
    "def load_label(name):\n",
    "    return load_np(os.path.join(DATA_DIR, f\"labels_{name}.npy\"))\n",
    "\n",
    "def load_keep(name):\n",
    "    return load_np(os.path.join(DATA_DIR, f\"poses_{name}_kept_idx.npy\"))\n",
    "\n",
    "def map_label_to_five(raw_list):\n",
    "    \"\"\"Â∞ÜÂ≠óÁ¨¶‰∏≤Ê†áÁ≠æÊò†Â∞ÑÂà∞ 0-4 Á±ª\"\"\"\n",
    "    out = []\n",
    "    for v in raw_list:\n",
    "        if v in MAP:\n",
    "            out.append(MAP[v])\n",
    "        else:\n",
    "            out.append(None)\n",
    "    return np.array(out, dtype=object)\n",
    "\n",
    "def filter_kept(X, y, keep):\n",
    "    \"\"\"Ê†πÊçÆ kept_idx ËøáÊª§Âπ≤ÂáÄÂ∫èÂàó\"\"\"\n",
    "    keep = keep.astype(int)\n",
    "    keep = keep[(keep >= 0) & (keep < len(X))]\n",
    "    return X[keep], y[keep]\n",
    "\n",
    "def window_center(X, y, win=15, stride=5):\n",
    "    \"\"\"Á™óÂè£ÂåñÔºà‰∏≠ÂøÉÂ∏ßÂÜ≥ÂÆöÊ†áÁ≠æÔºâ\"\"\"\n",
    "    N = len(X)\n",
    "    xs, ys = [], []\n",
    "    for i in range(0, N-win+1, stride):\n",
    "        seg = X[i:i+win]     # (15, 32, 3)\n",
    "        lbs = y[i:i+win]\n",
    "        lbs = [v for v in lbs if v is not None]\n",
    "        if len(lbs)==0: continue\n",
    "        label = lbs[len(lbs)//2]      # center label\n",
    "        xs.append(seg)\n",
    "        ys.append(label)\n",
    "    Xw = np.stack(xs).transpose(0, 3, 1, 2)  # ‚Üí (Nw, 3, 15, 32)\n",
    "    Yw = np.array(ys, dtype=int)\n",
    "    return Xw, Yw\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 1: ÂçïÊï∞ÊçÆÈõÜÂÆåÂÖ®Â§ÑÁêÜ\n",
    "# ================================================================\n",
    "def process_one(name):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    X = load_pose(name)                        # (T, 32, 3)\n",
    "    y_raw = load_label(name)                   # (T,)\n",
    "    keep = load_keep(name)                     # (K,)\n",
    "    \n",
    "    y = map_label_to_five(y_raw)               # Êò†Â∞Ñ‰∫îÁ±ª\n",
    "    X, y = filter_kept(X, y, keep)             # NDF ËøáÊª§\n",
    "\n",
    "    print(f\"{name}: after keep ‚Üí X={X.shape}, y={len(y)}, class_cnt={Counter(y[y!=None])}\")\n",
    "\n",
    "    Xw, Yw = window_center(X, y)\n",
    "    print(f\"{name}: window ‚Üí Xw={Xw.shape}, Yw={Yw.shape}, class_cnt={Counter(Yw)}\")\n",
    "    return Xw, Yw\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 2: ÂêàÂπ∂ÂÖ®ÈÉ®Êï∞ÊçÆÈõÜ\n",
    "# ================================================================\n",
    "def combine_all():\n",
    "    XX, YY = [], []\n",
    "    for name in DATASETS:\n",
    "        Xw, Yw = process_one(name)\n",
    "        XX.append(Xw)\n",
    "        YY.append(Yw)\n",
    "    X = np.concatenate(XX, axis=0)\n",
    "    Y = np.concatenate(YY, axis=0)\n",
    "    print(\"\\n===== ALL merged =====\")\n",
    "    print(f\"X={X.shape}, Y={Y.shape}, class_cnt={Counter(Y)}\")\n",
    "    return X, Y\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 3: Ëá™Âä® oversample Â∞ëÊï∞Á±ª\n",
    "# ================================================================\n",
    "def oversample(X, Y):\n",
    "    print(\"\\n===== Oversample =====\")\n",
    "    cnt = Counter(Y)\n",
    "    M = max(cnt.values())  # ÊúÄÂ§ßÁ±ª‰Ωú‰∏∫ÁõÆÊ†áÊï∞Èáè\n",
    "    XX, YY = [], []\n",
    "    for c in range(5):\n",
    "        idx = np.where(Y==c)[0]\n",
    "        if len(idx)==0: continue\n",
    "        if len(idx)<M:\n",
    "            rep = np.random.choice(idx, M-len(idx), replace=True)\n",
    "            idx = np.concatenate([idx, rep])\n",
    "        XX.append(X[idx])\n",
    "        YY.append(np.full(len(idx), c))\n",
    "    Xb = np.concatenate(XX)\n",
    "    Yb = np.concatenate(YY)\n",
    "    print(f\"After oversample: {Counter(Yb)}\")\n",
    "    return Xb, Yb\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 4: stratified train/val/test split\n",
    "# ================================================================\n",
    "def split_stratified(X, Y, tr=0.7, va=0.15, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    for c in range(5):\n",
    "        idx = np.where(Y==c)[0]\n",
    "        np.random.shuffle(idx)\n",
    "        n = len(idx)\n",
    "        t1 = int(n*tr)\n",
    "        t2 = int(n*(tr+va))\n",
    "        train_idx.extend(idx[:t1])\n",
    "        val_idx.extend(idx[t1:t2])\n",
    "        test_idx.extend(idx[t2:])\n",
    "    return (X[train_idx], Y[train_idx]), (X[val_idx], Y[val_idx]), (X[test_idx], Y[test_idx])\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 5: RUN ALL\n",
    "# ================================================================\n",
    "X, Y = combine_all()\n",
    "Xb, Yb = oversample(X, Y)\n",
    "(trainX, trainY), (valX, valY), (testX, testY) = split_stratified(Xb, Yb)\n",
    "\n",
    "print(\"\\n===== FINAL =====\")\n",
    "print(\"Train:\", trainX.shape, Counter(trainY))\n",
    "print(\"Val  :\", valX.shape, Counter(valY))\n",
    "print(\"Test :\", testX.shape, Counter(testY))\n",
    "\n",
    "np.savez(os.path.join(DATA_DIR, \"train_stgcn.npz\"), X=trainX, Y=trainY)\n",
    "np.savez(os.path.join(DATA_DIR, \"val_stgcn.npz\"),   X=valX, Y=valY)\n",
    "np.savez(os.path.join(DATA_DIR, \"test_stgcn.npz\"),  X=testX, Y=testY)\n",
    "\n",
    "print(\"\\n‚úÖ DONE! ÂÖ®ÊµÅÁ®ãÂÆåÊàêÔºÅ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4e9a9e-4627-4744-825d-fbadbdb01b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data/train_stgcn.npz\")\n",
    "X_train, Y_train = data[\"X\"], data[\"Y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d6e1f5-df27-4c6d-8ac1-d18a081faa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in file: ['X', 'Y']\n",
      "Shapes: (16205, 3, 15, 32) (16205,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"data/train_stgcn.npz\", allow_pickle=True)\n",
    "\n",
    "print(\"Keys in file:\", data.files)\n",
    "\n",
    "X_train = data[\"X\"]\n",
    "Y_train = data[\"Y\"]\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b36ed2-968b-40b4-b893-f2211629ae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train: (16205, 3, 15, 32) (16205,)\n",
      "Val  : (1236, 3, 15, 32) (1236,)\n",
      "Test : (1240, 3, 15, 32) (1240,)\n",
      "Adjacency: (1, 32, 32)\n",
      "Class weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Epoch 01/80 | Train F1: 0.5131 | Val F1: 0.3576 | Test F1: 0.2555\n",
      "Epoch 02/80 | Train F1: 0.5968 | Val F1: 0.4034 | Test F1: 0.5082\n",
      "Epoch 03/80 | Train F1: 0.6433 | Val F1: 0.3759 | Test F1: 0.3470\n",
      "Epoch 04/80 | Train F1: 0.6755 | Val F1: 0.3994 | Test F1: 0.3036\n",
      "Epoch 05/80 | Train F1: 0.7040 | Val F1: 0.4897 | Test F1: 0.3848\n",
      "Epoch 06/80 | Train F1: 0.7305 | Val F1: 0.5019 | Test F1: 0.3930\n",
      "Epoch 07/80 | Train F1: 0.7467 | Val F1: 0.5409 | Test F1: 0.4241\n",
      "Epoch 08/80 | Train F1: 0.7675 | Val F1: 0.5428 | Test F1: 0.4439\n",
      "Epoch 09/80 | Train F1: 0.7857 | Val F1: 0.5536 | Test F1: 0.4569\n",
      "Epoch 10/80 | Train F1: 0.7950 | Val F1: 0.5744 | Test F1: 0.4761\n",
      "Epoch 11/80 | Train F1: 0.8054 | Val F1: 0.4583 | Test F1: 0.4488\n",
      "Epoch 12/80 | Train F1: 0.8177 | Val F1: 0.5993 | Test F1: 0.4966\n",
      "Epoch 13/80 | Train F1: 0.8253 | Val F1: 0.5907 | Test F1: 0.4848\n",
      "Epoch 14/80 | Train F1: 0.8310 | Val F1: 0.6514 | Test F1: 0.5237\n",
      "Epoch 15/80 | Train F1: 0.8424 | Val F1: 0.6071 | Test F1: 0.5029\n",
      "Epoch 16/80 | Train F1: 0.8508 | Val F1: 0.5940 | Test F1: 0.4788\n",
      "Epoch 17/80 | Train F1: 0.8558 | Val F1: 0.6412 | Test F1: 0.5238\n",
      "Epoch 18/80 | Train F1: 0.8642 | Val F1: 0.6488 | Test F1: 0.5221\n",
      "Epoch 19/80 | Train F1: 0.8698 | Val F1: 0.5980 | Test F1: 0.5038\n",
      "Epoch 20/80 | Train F1: 0.8740 | Val F1: 0.6546 | Test F1: 0.5187\n",
      "Epoch 21/80 | Train F1: 0.8791 | Val F1: 0.5528 | Test F1: 0.4448\n",
      "Epoch 22/80 | Train F1: 0.8839 | Val F1: 0.6518 | Test F1: 0.5324\n",
      "Epoch 23/80 | Train F1: 0.8875 | Val F1: 0.6427 | Test F1: 0.5433\n",
      "Epoch 24/80 | Train F1: 0.8916 | Val F1: 0.6515 | Test F1: 0.5205\n",
      "Epoch 25/80 | Train F1: 0.8931 | Val F1: 0.6633 | Test F1: 0.5358\n",
      "Epoch 26/80 | Train F1: 0.9007 | Val F1: 0.6595 | Test F1: 0.5444\n",
      "Epoch 27/80 | Train F1: 0.9025 | Val F1: 0.6399 | Test F1: 0.5342\n",
      "Epoch 28/80 | Train F1: 0.9062 | Val F1: 0.6511 | Test F1: 0.5282\n",
      "Epoch 29/80 | Train F1: 0.9077 | Val F1: 0.6509 | Test F1: 0.5397\n",
      "Epoch 30/80 | Train F1: 0.9079 | Val F1: 0.6466 | Test F1: 0.5321\n",
      "Epoch 31/80 | Train F1: 0.9185 | Val F1: 0.6844 | Test F1: 0.5417\n",
      "Epoch 32/80 | Train F1: 0.9183 | Val F1: 0.6643 | Test F1: 0.5498\n",
      "Epoch 33/80 | Train F1: 0.9177 | Val F1: 0.6561 | Test F1: 0.5348\n",
      "Epoch 34/80 | Train F1: 0.9207 | Val F1: 0.6762 | Test F1: 0.5401\n",
      "Epoch 35/80 | Train F1: 0.9255 | Val F1: 0.7040 | Test F1: 0.5752\n",
      "Epoch 36/80 | Train F1: 0.9279 | Val F1: 0.6585 | Test F1: 0.5095\n",
      "Epoch 37/80 | Train F1: 0.9260 | Val F1: 0.6649 | Test F1: 0.5482\n",
      "Epoch 38/80 | Train F1: 0.9309 | Val F1: 0.6784 | Test F1: 0.5605\n",
      "Epoch 39/80 | Train F1: 0.9304 | Val F1: 0.6848 | Test F1: 0.7650\n",
      "Epoch 40/80 | Train F1: 0.9364 | Val F1: 0.6789 | Test F1: 0.5516\n",
      "Epoch 41/80 | Train F1: 0.9340 | Val F1: 0.6897 | Test F1: 0.5619\n",
      "Epoch 42/80 | Train F1: 0.9371 | Val F1: 0.7092 | Test F1: 0.5458\n",
      "Epoch 43/80 | Train F1: 0.9376 | Val F1: 0.6782 | Test F1: 0.5801\n",
      "Epoch 44/80 | Train F1: 0.9424 | Val F1: 0.6862 | Test F1: 0.5584\n",
      "Epoch 45/80 | Train F1: 0.9409 | Val F1: 0.6917 | Test F1: 0.5561\n",
      "Epoch 46/80 | Train F1: 0.9407 | Val F1: 0.6788 | Test F1: 0.5546\n",
      "Epoch 47/80 | Train F1: 0.9436 | Val F1: 0.6917 | Test F1: 0.5778\n",
      "Epoch 48/80 | Train F1: 0.9461 | Val F1: 0.6811 | Test F1: 0.5517\n",
      "Epoch 49/80 | Train F1: 0.9423 | Val F1: 0.6947 | Test F1: 0.5538\n",
      "Epoch 50/80 | Train F1: 0.9449 | Val F1: 0.6942 | Test F1: 0.5722\n",
      "Epoch 51/80 | Train F1: 0.9476 | Val F1: 0.6890 | Test F1: 0.5788\n",
      "Epoch 52/80 | Train F1: 0.9504 | Val F1: 0.6979 | Test F1: 0.6019\n",
      "Epoch 53/80 | Train F1: 0.9508 | Val F1: 0.7134 | Test F1: 0.5885\n",
      "Epoch 54/80 | Train F1: 0.9514 | Val F1: 0.7033 | Test F1: 0.5576\n",
      "Epoch 55/80 | Train F1: 0.9505 | Val F1: 0.6810 | Test F1: 0.5613\n",
      "Epoch 56/80 | Train F1: 0.9514 | Val F1: 0.6937 | Test F1: 0.5559\n",
      "Epoch 57/80 | Train F1: 0.9537 | Val F1: 0.7067 | Test F1: 0.5934\n",
      "Epoch 58/80 | Train F1: 0.9504 | Val F1: 0.6921 | Test F1: 0.5697\n",
      "Epoch 59/80 | Train F1: 0.9523 | Val F1: 0.7092 | Test F1: 0.5590\n",
      "Epoch 60/80 | Train F1: 0.9587 | Val F1: 0.7154 | Test F1: 0.5598\n",
      "Epoch 61/80 | Train F1: 0.9574 | Val F1: 0.7245 | Test F1: 0.5582\n",
      "Epoch 62/80 | Train F1: 0.9566 | Val F1: 0.7124 | Test F1: 0.5610\n",
      "Epoch 63/80 | Train F1: 0.9574 | Val F1: 0.6784 | Test F1: 0.5786\n",
      "Epoch 64/80 | Train F1: 0.9582 | Val F1: 0.6967 | Test F1: 0.5390\n",
      "Epoch 65/80 | Train F1: 0.9570 | Val F1: 0.7078 | Test F1: 0.5900\n",
      "Epoch 66/80 | Train F1: 0.9609 | Val F1: 0.7087 | Test F1: 0.5780\n",
      "Epoch 67/80 | Train F1: 0.9584 | Val F1: 0.6984 | Test F1: 0.5760\n",
      "Epoch 68/80 | Train F1: 0.9602 | Val F1: 0.7053 | Test F1: 0.5653\n",
      "Epoch 69/80 | Train F1: 0.9627 | Val F1: 0.6739 | Test F1: 0.5536\n",
      "Epoch 70/80 | Train F1: 0.9643 | Val F1: 0.7017 | Test F1: 0.5784\n",
      "Epoch 71/80 | Train F1: 0.9597 | Val F1: 0.7239 | Test F1: 0.5833\n",
      "Epoch 72/80 | Train F1: 0.9647 | Val F1: 0.7148 | Test F1: 0.6046\n",
      "Epoch 73/80 | Train F1: 0.9624 | Val F1: 0.7392 | Test F1: 0.5895\n",
      "Epoch 74/80 | Train F1: 0.9602 | Val F1: 0.7029 | Test F1: 0.5682\n",
      "Epoch 75/80 | Train F1: 0.9680 | Val F1: 0.7053 | Test F1: 0.5744\n",
      "Epoch 76/80 | Train F1: 0.9673 | Val F1: 0.7029 | Test F1: 0.5765\n",
      "Epoch 77/80 | Train F1: 0.9626 | Val F1: 0.6972 | Test F1: 0.5898\n",
      "Epoch 78/80 | Train F1: 0.9636 | Val F1: 0.7154 | Test F1: 0.5850\n",
      "Epoch 79/80 | Train F1: 0.9666 | Val F1: 0.7147 | Test F1: 0.5717\n",
      "Epoch 80/80 | Train F1: 0.9661 | Val F1: 0.6854 | Test F1: 0.5641\n",
      "\n",
      "üéØ Test macro-F1: 0.5641\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.67      0.77      0.72       343\n",
      "       a_lie       0.00      0.00      0.00         1\n",
      "     a_stand       0.85      0.77      0.81       695\n",
      "       a_sit       0.77      0.68      0.72       129\n",
      "      a_bend       0.49      0.68      0.57        72\n",
      "\n",
      "    accuracy                           0.76      1240\n",
      "   macro avg       0.56      0.58      0.56      1240\n",
      "weighted avg       0.77      0.76      0.76      1240\n",
      "\n",
      "\n",
      "‚úÖ Saved: data\\best_stgcn.pth\n",
      "‚úÖ Logs:  data\\train_log.csv\n",
      "‚úÖ Plots: data\\confusion_matrix.png, data\\per_class_pr.png\n"
     ]
    }
   ],
   "source": [
    "# === ST-GCN ‰∫îÁ±ªËÆ≠ÁªÉÔºöËØªÂèñnpz / ËÆ≠ÁªÉ / ËØÑ‰º∞ / ÂèØËßÜÂåñÔºàÊ∑∑Ê∑ÜÁü©Èòµ & PRÊõ≤Á∫øÔºâ===\n",
    "\n",
    "import os, json, time, csv, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‰Ω†ÁöÑÁÆ°Á∫øÊ®°ÂùóÔºàÂ∑≤Âú®È°πÁõÆÈáåÔºâ\n",
    "import stgcn_fiveclass_pipeline as P\n",
    "STGCN              = P.STGCN\n",
    "Graph              = P.Graph\n",
    "FIVE_LABELS        = P.FIVE_LABELS  # ['walk','lie','stand','sit','bend']\n",
    "\n",
    "# -------------------\n",
    "# ÈÖçÁΩÆ\n",
    "# -------------------\n",
    "DATA_DIR   = \"data\"\n",
    "TRAIN_NPZ  = os.path.join(DATA_DIR, \"train_stgcn.npz\")\n",
    "VAL_NPZ    = os.path.join(DATA_DIR, \"val_stgcn.npz\")\n",
    "TEST_NPZ   = os.path.join(DATA_DIR, \"test_stgcn.npz\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 80\n",
    "LR         = 5e-4\n",
    "PATIENCE   = 12\n",
    "SAVE_DIR   = DATA_DIR\n",
    "BEST_PATH  = os.path.join(SAVE_DIR, \"best_stgcn.pth\")\n",
    "LOG_CSV    = os.path.join(SAVE_DIR, \"train_log.csv\")\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------------------\n",
    "# Êï∞ÊçÆËØªÂÖ•‰∏é DataLoader\n",
    "# -------------------\n",
    "def load_npz(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    return d[\"X\"], d[\"Y\"]\n",
    "\n",
    "X_tr, Y_tr = load_npz(TRAIN_NPZ)\n",
    "X_va, Y_va = load_npz(VAL_NPZ)\n",
    "X_te, Y_te = load_npz(TEST_NPZ)\n",
    "\n",
    "print(\"Train:\", X_tr.shape, Y_tr.shape)\n",
    "print(\"Val  :\", X_va.shape, Y_va.shape)\n",
    "print(\"Test :\", X_te.shape, Y_te.shape)\n",
    "\n",
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X   # (N, 3, T, V)\n",
    "        self.Y = Y   # (N,)\n",
    "    def __len__(self): return len(self.Y)\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.tensor(self.X[i], dtype=torch.float32)  # (3,T,V)\n",
    "        y = torch.tensor(int(self.Y[i]), dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(NPZDataset(X_tr, Y_tr), batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(NPZDataset(X_va, Y_va), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(NPZDataset(X_te, Y_te), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "# -------------------\n",
    "# ÂõæÔºàK=1 ÂêàÂπ∂Â≠êÂõæÔºâ\n",
    "# -------------------\n",
    "A = np.asarray(Graph().A, dtype=np.float32)\n",
    "if A.ndim == 2:\n",
    "    A = A[None, ...]\n",
    "elif A.ndim == 3 and A.shape[0] != 1:\n",
    "    A = A.sum(axis=0, keepdims=True)  # ÂêàÂπ∂‰∏∫ÂçïÂõæ\n",
    "print(\"Adjacency:\", A.shape)\n",
    "\n",
    "# -------------------\n",
    "# ÈÄöÈÅìÈÄÇÈÖç & GCNÂùóËæìÂÖ•Ëá™Âä®ÂØπÈΩêÔºàÈÅøÂÖç channel mismatchÔºâ\n",
    "# -------------------\n",
    "class InputAdapter(nn.Module):\n",
    "    # Â∞ÜËæìÂÖ•(3,T,V)ÊäïÂΩ±Âà∞16ÈÄöÈÅì\n",
    "    def __init__(self, in_ch=3, out_ch=16):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "    def forward(self, x):  # (N,3,T,V) -> (N,16,T,V)\n",
    "        return self.net(x)\n",
    "\n",
    "def _replicate_A_for_gcn(A_np, num_subset):\n",
    "    if A_np.ndim == 2:\n",
    "        A_np = A_np[None, ...]\n",
    "    if A_np.shape[0] != num_subset:\n",
    "        A_np = np.repeat(A_np, num_subset, axis=0)\n",
    "    return torch.tensor(A_np, dtype=torch.float32)\n",
    "\n",
    "def _set_conv_in(conv2d: nn.Conv2d, in_ch: int) -> nn.Conv2d:\n",
    "    newc = nn.Conv2d(\n",
    "        in_channels=in_ch,\n",
    "        out_channels=conv2d.out_channels,\n",
    "        kernel_size=conv2d.kernel_size,\n",
    "        stride=conv2d.stride,\n",
    "        padding=conv2d.padding,\n",
    "        dilation=conv2d.dilation,\n",
    "        groups=conv2d.groups,\n",
    "        bias=(conv2d.bias is not None),\n",
    "    )\n",
    "    return newc\n",
    "\n",
    "def _block_out_channels(gcn_module) -> int:\n",
    "    # ‰ª•Á¨¨‰∏ÄÊîØconvÁöÑ out_channels ‰Ωú‰∏∫ËØ•ÂùóËæìÂá∫ÈÄöÈÅì\n",
    "    return int(gcn_module.conv_d[0].out_channels)\n",
    "\n",
    "adapter    = InputAdapter(3, 16).to(device)\n",
    "core_model = STGCN(in_channels=16, num_class=len(FIVE_LABELS), A=A).to(device)\n",
    "\n",
    "# A Â§çÂà∂Âà∞ÊØè‰∏™GCNÂàÜÊîØ\n",
    "for m in core_model.modules():\n",
    "    if m.__class__.__name__ == \"GCN\" and hasattr(m, \"A\") and hasattr(m, \"num_subset\"):\n",
    "        m.A = _replicate_A_for_gcn(A, m.num_subset).to(device)\n",
    "\n",
    "# Ëá™Âä®ÂØπÈΩê g2/g3/g4 conv_d ÁöÑËæìÂÖ•ÈÄöÈÅì\n",
    "blocks = [getattr(core_model, n) for n in (\"g1\",\"g2\",\"g3\",\"g4\") if hasattr(core_model, n)]\n",
    "prev_out = _block_out_channels(blocks[0])\n",
    "for blk in blocks[1:]:\n",
    "    newlist = []\n",
    "    for conv in blk.conv_d:\n",
    "        newlist.append(_set_conv_in(conv, prev_out).to(device))\n",
    "    blk.conv_d = nn.ModuleList(newlist)\n",
    "    prev_out = _block_out_channels(blk)\n",
    "\n",
    "class WrappedSTGCN(nn.Module):\n",
    "    def __init__(self, adapter, core):\n",
    "        super().__init__()\n",
    "        self.adapter = adapter\n",
    "        self.core    = core\n",
    "    def forward(self, x):\n",
    "        x = self.adapter(x)      # (N,3,T,V)->(N,16,T,V)\n",
    "        return self.core(x)\n",
    "\n",
    "model = WrappedSTGCN(adapter, core_model).to(device)\n",
    "\n",
    "# -------------------\n",
    "# Á±ªÂà´ÊùÉÈáç & ‰ºòÂåñÂô®\n",
    "# -------------------\n",
    "from collections import Counter\n",
    "cnt = Counter(Y_tr.tolist()); tot = sum(cnt.values())\n",
    "weights = np.array([tot / max(cnt.get(i,1),1) for i in range(len(FIVE_LABELS))], dtype=np.float32)\n",
    "weights = weights / weights.mean()\n",
    "print(\"Class weights:\", np.round(weights,2).tolist())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32, device=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# -------------------\n",
    "# ËØÑ‰º∞/ÂèØËßÜÂåñ\n",
    "# -------------------\n",
    "@torch.no_grad()\n",
    "def eval_loader(loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        y_true.append(yb.cpu().numpy())\n",
    "        y_pred.append(out.argmax(1).cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return f1, y_true, y_pred\n",
    "\n",
    "def plot_confusion(cm, labels, save_path):\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           ylabel='True label', xlabel='Predicted label',\n",
    "           title='Confusion Matrix')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=120)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_pr_bars(y_true, y_pred, labels, save_path):\n",
    "    # ÁîªÊØèÁ±ª precision/recall Êü±Áä∂ÂõæÔºàÁÆÄÂçïÁõ¥ËßÇÔºâ\n",
    "    prec, rec, f1, sup = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=list(range(len(labels))), zero_division=0\n",
    "    )\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    ax.bar(x - w/2, prec, width=w, label='Precision')\n",
    "    ax.bar(x + w/2, rec,  width=w, label='Recall')\n",
    "    for i,v in enumerate(prec):\n",
    "        ax.text(i - w/2, v+0.01, f\"{v:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "    for i,v in enumerate(rec):\n",
    "        ax.text(i + w/2, v+0.01, f\"{v:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=20)\n",
    "    ax.set_ylim(0,1.05)\n",
    "    ax.set_title(\"Per-class Precision / Recall\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=120)\n",
    "    plt.close(fig)\n",
    "\n",
    "# -------------------\n",
    "# ËÆ≠ÁªÉÔºàÊó©ÂÅú + Êó•ÂøóÔºâ\n",
    "# -------------------\n",
    "best_val, best_state, wait = -1.0, None, 0\n",
    "with open(LOG_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f); w.writerow([\"epoch\",\"train_f1\",\"val_f1\",\"test_f1\",\"lr\"])\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    yT, yP = [], []\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        yT.append(yb.detach().cpu().numpy())\n",
    "        yP.append(out.argmax(1).detach().cpu().numpy())\n",
    "    yT = np.concatenate(yT); yP = np.concatenate(yP)\n",
    "    tr_f1 = f1_score(yT, yP, average=\"macro\", zero_division=0)\n",
    "\n",
    "    va_f1, _, _ = eval_loader(val_loader)\n",
    "    te_f1, _, _ = eval_loader(test_loader)\n",
    "    print(f\"Epoch {ep:02d}/{EPOCHS} | Train F1: {tr_f1:.4f} | Val F1: {va_f1:.4f} | Test F1: {te_f1:.4f}\")\n",
    "\n",
    "    # ÂÜôÊó•Âøó\n",
    "    with open(LOG_CSV, 'a', newline='', encoding='utf-8') as f:\n",
    "        csv.writer(f).writerow([ep, f\"{tr_f1:.6f}\", f\"{va_f1:.6f}\", f\"{te_f1:.6f}\", optimizer.param_groups[0]['lr']])\n",
    "\n",
    "    # Êó©ÂÅú\n",
    "    if va_f1 > best_val:\n",
    "        best_val = va_f1\n",
    "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "        torch.save(best_state, BEST_PATH)\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"‚ö†Ô∏è Early stop\")\n",
    "            break\n",
    "\n",
    "# -------------------\n",
    "# Âä†ËΩΩÊúÄ‰ºò & ÊúÄÁªàËØÑ‰º∞‰∏éÂèØËßÜÂåñ\n",
    "# -------------------\n",
    "if best_state is None and os.path.exists(BEST_PATH):\n",
    "    best_state = torch.load(BEST_PATH, map_location=\"cpu\")\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "te_f1, y_true, y_pred = eval_loader(test_loader)\n",
    "print(\"\\nüéØ Test macro-F1:\", round(te_f1,4))\n",
    "names = [f\"a_{n}\" for n in FIVE_LABELS]\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, labels=list(range(len(FIVE_LABELS))),\n",
    "                           target_names=names, digits=2, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(FIVE_LABELS))))\n",
    "plot_confusion(cm, names, os.path.join(SAVE_DIR, \"confusion_matrix.png\"))\n",
    "plot_pr_bars(y_true, y_pred, names, os.path.join(SAVE_DIR, \"per_class_pr.png\"))\n",
    "print(f\"\\n‚úÖ Saved: {BEST_PATH}\")\n",
    "print(f\"‚úÖ Logs:  {LOG_CSV}\")\n",
    "print(f\"‚úÖ Plots: {os.path.join(SAVE_DIR, 'confusion_matrix.png')}, {os.path.join(SAVE_DIR, 'per_class_pr.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eaaf612-8a37-46b6-b6bf-27b445de155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 16GZ =====\n",
      "16GZ: after keep ‚Üí X=(9797, 32, 3), y=9797, valid_frames=7930, class_cnt=Counter({2: 4948, 0: 1694, 3: 1104, 4: 184})\n",
      "16GZ: window ‚Üí Xw=(1662, 3, 15, 32), Yw=(1662,), class_cnt=Counter({np.int64(2): 1013, np.int64(0): 356, np.int64(3): 234, np.int64(4): 59})\n",
      "\n",
      "===== 17JP =====\n",
      "17JP: after keep ‚Üí X=(14189, 32, 3), y=14189, valid_frames=6385, class_cnt=Counter({2: 2668, 0: 2549, 3: 784, 4: 384})\n",
      "17JP: window ‚Üí Xw=(1442, 3, 15, 32), Yw=(1442,), class_cnt=Counter({np.int64(2): 587, np.int64(0): 554, np.int64(3): 174, np.int64(4): 127})\n",
      "\n",
      "===== 19MM =====\n",
      "19MM: after keep ‚Üí X=(7654, 32, 3), y=7654, valid_frames=6133, class_cnt=Counter({2: 2950, 0: 2291, 3: 503, 4: 385, 1: 4})\n",
      "19MM: window ‚Üí Xw=(1331, 3, 15, 32), Yw=(1331,), class_cnt=Counter({np.int64(2): 628, np.int64(0): 470, np.int64(4): 122, np.int64(3): 109, np.int64(1): 2})\n",
      "\n",
      "===== 09SY =====\n",
      "09SY: after keep ‚Üí X=(11520, 32, 3), y=11520, valid_frames=3241, class_cnt=Counter({2: 2752, 0: 489})\n",
      "09SY: window ‚Üí Xw=(651, 3, 15, 32), Yw=(651,), class_cnt=Counter({np.int64(2): 554, np.int64(0): 97})\n",
      "\n",
      "===== 05HW =====\n",
      "05HW: after keep ‚Üí X=(13292, 32, 3), y=13292, valid_frames=2223, class_cnt=Counter({3: 1159, 2: 785, 4: 279})\n",
      "05HW: window ‚Üí Xw=(548, 3, 15, 32), Yw=(548,), class_cnt=Counter({np.int64(3): 257, np.int64(2): 213, np.int64(4): 78})\n",
      "\n",
      "===== 14IB =====\n",
      "14IB: after keep ‚Üí X=(14594, 32, 3), y=14594, valid_frames=12308, class_cnt=Counter({2: 7895, 0: 3857, 3: 294, 4: 262})\n",
      "14IB: window ‚Üí Xw=(2614, 3, 15, 32), Yw=(2614,), class_cnt=Counter({np.int64(2): 1635, np.int64(0): 806, np.int64(4): 92, np.int64(3): 81})\n",
      "\n",
      "===== ALL merged (windowed) =====\n",
      "X=(8248, 3, 15, 32), Y=(8248,), class_cnt=Counter({np.int64(2): 4630, np.int64(0): 2283, np.int64(3): 855, np.int64(4): 478, np.int64(1): 2})\n",
      "\n",
      "===== SPLIT (before oversample) =====\n",
      "Train raw: (5772, 3, 15, 32) Counter({np.int64(2): 3241, np.int64(0): 1598, np.int64(3): 598, np.int64(4): 334, np.int64(1): 1})\n",
      "Val      : (1236, 3, 15, 32) Counter({np.int64(2): 694, np.int64(0): 342, np.int64(3): 128, np.int64(4): 72})\n",
      "Test     : (1240, 3, 15, 32) Counter({np.int64(2): 695, np.int64(0): 343, np.int64(3): 129, np.int64(4): 72, np.int64(1): 1})\n",
      "\n",
      "===== Oversample (TRAIN ONLY) =====\n",
      "Before oversample (train): Counter({np.int64(2): 3241, np.int64(0): 1598, np.int64(3): 598, np.int64(4): 334, np.int64(1): 1})\n",
      "After  oversample (train): Counter({np.int64(0): 3241, np.int64(1): 3241, np.int64(2): 3241, np.int64(3): 3241, np.int64(4): 3241})\n",
      "\n",
      "===== FINAL =====\n",
      "Train (oversampled): (16205, 3, 15, 32) Counter({np.int64(0): 3241, np.int64(1): 3241, np.int64(2): 3241, np.int64(3): 3241, np.int64(4): 3241})\n",
      "Val               : (1236, 3, 15, 32) Counter({np.int64(2): 694, np.int64(0): 342, np.int64(3): 128, np.int64(4): 72})\n",
      "Test              : (1240, 3, 15, 32) Counter({np.int64(2): 695, np.int64(0): 343, np.int64(3): 129, np.int64(4): 72, np.int64(1): 1})\n",
      "\n",
      "‚úÖ DONE! Êó† data leak ÁâàÊú¨Â∑≤ÂÆåÊàêÔºåtrain/val/test ÂùáÂ∑≤‰øùÂ≠ò„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "DATASETS = [\"16GZ\", \"17JP\", \"19MM\", \"09SY\", \"05HW\", \"14IB\"]\n",
    "FIVE = [\"walk\", \"lie\", \"stand\", \"sit\", \"bend\"]\n",
    "MAP = {\"walk\":0, \"lie\":1, \"stand\":2, \"sit\":3, \"bend\":4}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Â∑•ÂÖ∑ÂáΩÊï∞\n",
    "# ------------------------------------------------\n",
    "def load_np(path):\n",
    "    \"\"\"Ëá™Âä® fallback ‚Üí allow_pickle=True\"\"\"\n",
    "    try:\n",
    "        return np.load(path)\n",
    "    except:\n",
    "        return np.load(path, allow_pickle=True)\n",
    "\n",
    "def load_pose(name):\n",
    "    return load_np(os.path.join(DATA_DIR, f\"poses_{name}_cs_nz.npy\"))\n",
    "\n",
    "def load_label(name):\n",
    "    return load_np(os.path.join(DATA_DIR, f\"labels_{name}.npy\"))\n",
    "\n",
    "def load_keep(name):\n",
    "    return load_np(os.path.join(DATA_DIR, f\"poses_{name}_kept_idx.npy\"))\n",
    "\n",
    "def map_label_to_five(raw_list):\n",
    "    \"\"\"Â∞ÜÂ≠óÁ¨¶‰∏≤Ê†áÁ≠æÊò†Â∞ÑÂà∞ 0-4 Á±ªÔºåÂÖ∂‰ªñËÆæ‰∏∫ None\"\"\"\n",
    "    out = []\n",
    "    for v in raw_list:\n",
    "        if v in MAP:\n",
    "            out.append(MAP[v])\n",
    "        else:\n",
    "            out.append(None)\n",
    "    return np.array(out, dtype=object)\n",
    "\n",
    "def filter_kept(X, y, keep):\n",
    "    \"\"\"Ê†πÊçÆ kept_idx ËøáÊª§Âπ≤ÂáÄÂ∫èÂàó\"\"\"\n",
    "    keep = keep.astype(int)\n",
    "    keep = keep[(keep >= 0) & (keep < len(X))]\n",
    "    return X[keep], y[keep]\n",
    "\n",
    "def window_center(X, y, win=15, stride=5):\n",
    "    \"\"\"Á™óÂè£ÂåñÔºà‰∏≠ÂøÉÂ∏ßÂÜ≥ÂÆöÊ†áÁ≠æÔºõÂøΩÁï•Á™óÂè£‰∏≠ÂÖ®ÊòØ None ÁöÑÊÉÖÂÜµÔºâ\"\"\"\n",
    "    N = len(X)\n",
    "    xs, ys = [], []\n",
    "    for i in range(0, N-win+1, stride):\n",
    "        seg = X[i:i+win]     # (win, 32, 3)\n",
    "        lbs = y[i:i+win]\n",
    "        lbs = [v for v in lbs if v is not None]\n",
    "        if len(lbs) == 0:\n",
    "            continue\n",
    "        label = lbs[len(lbs)//2]      # ËøôÈáåÊòØ‚ÄúÊúâÊïàÊ†áÁ≠æ‰∏≠ÁöÑ‰∏≠ÂøÉ‚Äù\n",
    "        xs.append(seg)\n",
    "        ys.append(label)\n",
    "    if len(xs) == 0:\n",
    "        return None, None\n",
    "    Xw = np.stack(xs).transpose(0, 3, 1, 2)  # (Nw, 3, win, 32)\n",
    "    Yw = np.array(ys, dtype=int)\n",
    "    return Xw, Yw\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 1: ÂçïÊï∞ÊçÆÈõÜÂÆåÂÖ®Â§ÑÁêÜ\n",
    "# ================================================================\n",
    "def process_one(name):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    X = load_pose(name)                        # (T, 32, 3)\n",
    "    y_raw = load_label(name)                   # (T,)\n",
    "    keep = load_keep(name)                     # (K,)\n",
    "    \n",
    "    y = map_label_to_five(y_raw)               # Êò†Â∞ÑÂà∞‰∫îÁ±ª / None\n",
    "    X, y = filter_kept(X, y, keep)             # NDF ËøáÊª§\n",
    "    \n",
    "    valid_mask = np.array([v is not None for v in y])\n",
    "    print(f\"{name}: after keep ‚Üí X={X.shape}, y={len(y)}, valid_frames={valid_mask.sum()}, \"\n",
    "          f\"class_cnt={Counter(y[valid_mask])}\")\n",
    "\n",
    "    Xw, Yw = window_center(X, y)\n",
    "    if Xw is None:\n",
    "        print(f\"{name}: no valid windows, skip.\")\n",
    "        return None, None\n",
    "    print(f\"{name}: window ‚Üí Xw={Xw.shape}, Yw={Yw.shape}, class_cnt={Counter(Yw)}\")\n",
    "    return Xw, Yw\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 2: ÂêàÂπ∂ÂÖ®ÈÉ®Êï∞ÊçÆÈõÜÔºàÁ™óÂè£ÂêéÁöÑ X,YÔºâ\n",
    "# ================================================================\n",
    "def combine_all():\n",
    "    XX, YY = [], []\n",
    "    for name in DATASETS:\n",
    "        Xw, Yw = process_one(name)\n",
    "        if Xw is None:\n",
    "            continue\n",
    "        XX.append(Xw)\n",
    "        YY.append(Yw)\n",
    "    X = np.concatenate(XX, axis=0)\n",
    "    Y = np.concatenate(YY, axis=0)\n",
    "    print(\"\\n===== ALL merged (windowed) =====\")\n",
    "    print(f\"X={X.shape}, Y={Y.shape}, class_cnt={Counter(Y)}\")\n",
    "    return X, Y\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 3: stratified train/val/test split  ÔºàÂÖà splitÔºåÂÜç oversampleÔºâ\n",
    "# ================================================================\n",
    "def split_stratified(X, Y, tr=0.7, va=0.15, seed=123):\n",
    "    \"\"\"ÊåâÁ±ªÂà´ÂàÜÂ±ÇÂàíÂàÜ train / val / testÔºà‰∏çÂÅö oversampleÔºâ\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for c in range(5):\n",
    "        idx = np.where(Y == c)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        np.random.shuffle(idx)\n",
    "        n = len(idx)\n",
    "        t1 = int(n * tr)\n",
    "        t2 = int(n * (tr + va))\n",
    "        train_idx.extend(idx[:t1])\n",
    "        val_idx.extend(idx[t1:t2])\n",
    "        test_idx.extend(idx[t2:])\n",
    "    train_idx = np.array(train_idx, dtype=int)\n",
    "    val_idx   = np.array(val_idx, dtype=int)\n",
    "    test_idx  = np.array(test_idx, dtype=int)\n",
    "    return (X[train_idx], Y[train_idx]), (X[val_idx], Y[val_idx]), (X[test_idx], Y[test_idx])\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 4: oversample Âè™ÂØπËÆ≠ÁªÉÈõÜËøõË°å\n",
    "# ================================================================\n",
    "def oversample_train(X, Y, seed=123):\n",
    "    \"\"\"\n",
    "    ‰ªÖÂØπËÆ≠ÁªÉÈõÜÂÅöÁ±ªÂÜÖ oversampleÔºåÁõÆÊ†áÔºöÊääÊØè‰∏ÄÁ±ªË°•Âà∞ÊúÄÂ§ßÁ±ªÊï∞Èáè„ÄÇ\n",
    "    Ê≥®ÊÑèÔºöÂçÉ‰∏á‰∏çË¶ÅÂú®ÂêàÂπ∂ÂÖ®ÈÉ®Êï∞ÊçÆÂêéÂÜç oversample Âê¶Âàô‰ºö data leak„ÄÇ\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    print(\"\\n===== Oversample (TRAIN ONLY) =====\")\n",
    "    cnt = Counter(Y)\n",
    "    print(\"Before oversample (train):\", cnt)\n",
    "    M = max(cnt.values())   # ÁõÆÊ†áÔºöË°•Âà∞ÊúÄÂ§ßÁ±ªÊï∞Èáè\n",
    "\n",
    "    XX, YY = [], []\n",
    "    for c in range(5):\n",
    "        idx = np.where(Y == c)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        if len(idx) < M:\n",
    "            rep = np.random.choice(idx, M - len(idx), replace=True)\n",
    "            idx = np.concatenate([idx, rep])\n",
    "        XX.append(X[idx])\n",
    "        YY.append(np.full(len(idx), c, dtype=int))\n",
    "    Xb = np.concatenate(XX, axis=0)\n",
    "    Yb = np.concatenate(YY, axis=0)\n",
    "    print(\"After  oversample (train):\", Counter(Yb))\n",
    "    return Xb, Yb\n",
    "\n",
    "# ================================================================\n",
    "# ‚úÖ Part 5: RUN ALL ÔºàÊó† data leak ÁâàÊú¨Ôºâ\n",
    "# ================================================================\n",
    "X, Y = combine_all()\n",
    "\n",
    "# 1) ÂÖàÁî®ÂéüÂßãÁ™óÂè£Êï∞ÊçÆÂÅöÂàÜÂ±ÇÂàíÂàÜ\n",
    "(trainX_raw, trainY_raw), (valX, valY), (testX, testY) = split_stratified(X, Y)\n",
    "\n",
    "print(\"\\n===== SPLIT (before oversample) =====\")\n",
    "print(\"Train raw:\", trainX_raw.shape, Counter(trainY_raw))\n",
    "print(\"Val      :\", valX.shape,      Counter(valY))\n",
    "print(\"Test     :\", testX.shape,     Counter(testY))\n",
    "\n",
    "# 2) Âè™ÂØπËÆ≠ÁªÉÈõÜ oversampleÔºåÈ™åËØÅ/ÊµãËØï‰øùÊåÅÁúüÂÆûÂàÜÂ∏É\n",
    "trainX, trainY = oversample_train(trainX_raw, trainY_raw)\n",
    "\n",
    "print(\"\\n===== FINAL =====\")\n",
    "print(\"Train (oversampled):\", trainX.shape, Counter(trainY))\n",
    "print(\"Val               :\", valX.shape,   Counter(valY))\n",
    "print(\"Test              :\", testX.shape,  Counter(testY))\n",
    "\n",
    "# 3) ‰øùÂ≠ò ‚Äî‚Äî ËÆ≠ÁªÉÈõÜÁî® oversampled ÁâàÊú¨Ôºõval/test Áî®ÂéüÂßãÁâàÊú¨\n",
    "np.savez(os.path.join(DATA_DIR, \"train_stgcn.npz\"), X=trainX,      Y=trainY)\n",
    "np.savez(os.path.join(DATA_DIR, \"val_stgcn.npz\"),   X=valX,        Y=valY)\n",
    "np.savez(os.path.join(DATA_DIR, \"test_stgcn.npz\"),  X=testX,       Y=testY)\n",
    "\n",
    "print(\"\\n‚úÖ DONE! Êó† data leak ÁâàÊú¨Â∑≤ÂÆåÊàêÔºåtrain/val/test ÂùáÂ∑≤‰øùÂ≠ò„ÄÇ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785ae96-93db-4ad7-a40e-e1130bef2af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
