{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b501dd8c-0e5d-4328-8bc4-95082d92c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 34001\n",
      "dict_keys(['poses3d', 'poses2d', 'activities', 'num_people', 'rgb_frame_number', 'pc', 'raw_fids', 'radar_session_datetime', 'wallclock'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/19_MM.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(type(data), len(data))\n",
    "print(data[0].keys())   # 看这一帧的字段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e12dc6-05b8-4d2e-8757-f839c0eb4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01-PhysicalState': [], '01-Activity': ['undefined'], '01-Objects': [], '01-Notes': [], '02-PhysicalState': [], '02-Activity': [], '02-Objects': []}\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['activities'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf196955-94eb-4163-b30a-dee34a065d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逐帧标签 shape: (34001,)\n",
      "前20个: ['Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      " 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown'\n",
      " 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown' 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame_labels = []\n",
    "\n",
    "for frame in data:\n",
    "    state = frame['activities'].get('01-PhysicalState', [])\n",
    "    if state:  \n",
    "        frame_labels.append(state[0])   # 取第一个标签\n",
    "    else:\n",
    "        frame_labels.append(\"Unknown\")  # 没标注的帧\n",
    "\n",
    "frame_labels = np.array(frame_labels)\n",
    "print(\"逐帧标签 shape:\", frame_labels.shape)\n",
    "print(\"前20个:\", frame_labels[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9b4de4-62be-4684-8d61-ec25c021427f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/poses_19MM_samples.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m samples = np.load(\u001b[33m\"\u001b[39m\u001b[33mdata/poses_19MM_samples.npy\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# (N, 100, 32, 3)\u001b[39;00m\n\u001b[32m      2\u001b[39m num_samples = samples.shape[\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m window = \u001b[32m100\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28mopen\u001b[39m(os.fspath(file), \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/poses_19MM_samples.npy'"
     ]
    }
   ],
   "source": [
    "samples = np.load(\"data/poses_19MM_samples.npy\")  # (N, 100, 32, 3)\n",
    "num_samples = samples.shape[0]\n",
    "window = 100\n",
    "stride = 50\n",
    "\n",
    "y = []\n",
    "for i in range(num_samples):\n",
    "    start = i * stride\n",
    "    mid_frame = start + window // 2\n",
    "    label = frame_labels[mid_frame]\n",
    "    y.append(label)\n",
    "\n",
    "y = np.array(y)\n",
    "np.save(\"data/y_physicalstate.npy\", y)\n",
    "\n",
    "print(\"保存成功: data/y_physicalstate.npy, shape:\", y.shape)\n",
    "print(\"类别分布:\", np.unique(y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46952b7a-65ad-42c5-a044-1dca5b22f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 ['a_walk']\n",
      "5000 []\n",
      "10000 []\n",
      "20000 ['p_bent']\n"
     ]
    }
   ],
   "source": [
    "for i in [1000, 5000, 10000, 20000]:\n",
    "    print(i, data[i]['activities']['01-PhysicalState'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619f9976-89e6-4493-ba16-98d6226c9999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帧数: 34001\n",
      "逐帧标签 shape: (34001,)\n",
      "标签种类: ['Unknown' 'a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_situp' 'p_stand'\n",
      " 't_bed_turn' 't_bend' 't_lie_to_sit' 't_lie_to_situp' 't_sit_to_lie'\n",
      " 't_sit_to_stand' 't_situp_to_sit' 't_stand_to_sit' 't_straighten']\n",
      "样本数: 33955\n",
      "✅ 已保存标签: data/y_physicalstate.npy, shape=(33955,)\n",
      "类别分布: {np.str_('Unknown'): np.int64(33276), np.str_('a_walk'): np.int64(144), np.str_('p_bent'): np.int64(27), np.str_('p_lie'): np.int64(42), np.str_('p_sit'): np.int64(202), np.str_('p_situp'): np.int64(5), np.str_('p_stand'): np.int64(187), np.str_('t_bed_turn'): np.int64(10), np.str_('t_bend'): np.int64(17), np.str_('t_lie_to_sit'): np.int64(1), np.str_('t_lie_to_situp'): np.int64(2), np.str_('t_sit_to_lie'): np.int64(3), np.str_('t_sit_to_stand'): np.int64(6), np.str_('t_situp_to_sit'): np.int64(4), np.str_('t_stand_to_sit'): np.int64(8), np.str_('t_straighten'): np.int64(21)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# ========= 配置 =========\n",
    "pickle_file = \"data/19_MM.pickle\"          # 你的骨架数据\n",
    "samples_file = \"data/poses_19MM.npy\"  # 之前保存的样本\n",
    "window = 100   # 和 make_samples 保持一致\n",
    "stride = 50    # 和 make_samples 保持一致\n",
    "save_file = \"data/y_physicalstate.npy\"     # 保存路径\n",
    "# ========================\n",
    "\n",
    "# 1. 加载 pickle 数据\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "print(\"帧数:\", len(data))\n",
    "\n",
    "# 2. 逐帧提取 PhysicalState\n",
    "last_label = \"Unknown\"\n",
    "frame_labels = []\n",
    "\n",
    "for frame in data:\n",
    "    state = frame['activities'].get('01-PhysicalState', [])\n",
    "    if state:  \n",
    "        last_label = state[0]   # 更新为当前帧的标签\n",
    "    frame_labels.append(last_label)\n",
    "\n",
    "frame_labels = np.array(frame_labels)\n",
    "print(\"逐帧标签 shape:\", frame_labels.shape)\n",
    "print(\"标签种类:\", np.unique(frame_labels))\n",
    "\n",
    "# 3. 加载样本数据\n",
    "samples = np.load(samples_file)\n",
    "num_samples = samples.shape[0]\n",
    "print(\"样本数:\", num_samples)\n",
    "\n",
    "# 4. 对齐：每个样本用窗口中间帧的标签\n",
    "y = []\n",
    "for i in range(num_samples):\n",
    "    start = i * stride\n",
    "    mid_frame = start + window // 2\n",
    "    if mid_frame < len(frame_labels):\n",
    "        y.append(frame_labels[mid_frame])\n",
    "    else:\n",
    "        y.append(\"Unknown\")\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "# 5. 保存\n",
    "np.save(save_file, y)\n",
    "print(f\"✅ 已保存标签: {save_file}, shape={y.shape}\")\n",
    "print(\"类别分布:\", dict(zip(*np.unique(y, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375a41c7-5153-46f2-ab98-616ecd1270c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 100, 96)\n",
      "(33955,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437945df-c5a1-402e-a87d-78ec7dbcbe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (288, 100, 96)\n",
      "y_samples shape: (339,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 已有数据\n",
    "# X = (288, 100, 96)\n",
    "# y = (33955,) 逐帧标签\n",
    "\n",
    "window = 100\n",
    "stride = 100   # ⚠️ 你的 X 是怎么切的就用什么 stride\n",
    "y_samples = []\n",
    "\n",
    "for start in range(0, len(y) - window + 1, stride):\n",
    "    end = start + window\n",
    "    mid = start + window // 2   # 取中间帧\n",
    "    y_samples.append(y[mid])\n",
    "\n",
    "y_samples = np.array(y_samples)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y_samples shape:\", y_samples.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1cd70a-4887-463e-9549-b2bdc65b38f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始逐帧标签 shape: (34001,)\n",
      "原始骨架 shape: (33955, 32, 3)\n",
      "对齐后帧数: 33955\n",
      "清理后标签数: 14496\n",
      "清理后骨架 shape: (14496, 32, 3)\n",
      "最终 X shape: (144, 100, 32, 3)\n",
      "最终 y shape: (144,)\n",
      "前10个标签: ['a_walk' 'a_walk' 'p_stand' 'a_walk' 'a_walk' 'a_walk' 'a_walk' 'a_walk'\n",
      " 'a_walk' 'a_walk']\n",
      "✅ 已保存到 data/X.npy 和 data/y.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ================================\n",
    "# 1. 载入数据\n",
    "# ================================\n",
    "with open(\"data/19_MM.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# 提取逐帧标签\n",
    "frame_labels = []\n",
    "for frame in data:\n",
    "    acts = frame.get(\"activities\", {})\n",
    "    if \"01-PhysicalState\" in acts and len(acts[\"01-PhysicalState\"]) > 0:\n",
    "        frame_labels.append(acts[\"01-PhysicalState\"][0])  # 取第一个标签\n",
    "    else:\n",
    "        frame_labels.append(\"Unknown\")\n",
    "frame_labels = np.array(frame_labels)\n",
    "print(\"原始逐帧标签 shape:\", frame_labels.shape)\n",
    "\n",
    "# 载入之前保存的骨架数据 (T, 32, 3)\n",
    "poses_raw = np.load(\"data/poses_19MM.npy\")\n",
    "print(\"原始骨架 shape:\", poses_raw.shape)\n",
    "\n",
    "# ================================\n",
    "# 2. 对齐长度 + 清理全零帧\n",
    "# ================================\n",
    "min_len = min(len(frame_labels), len(poses_raw))\n",
    "frame_labels = frame_labels[:min_len]\n",
    "poses_raw = poses_raw[:min_len]\n",
    "\n",
    "mask = ~np.all(poses_raw == 0, axis=(1,2))  # True = 非全零帧\n",
    "labels_clean = frame_labels[mask]\n",
    "poses_clean = poses_raw[mask]\n",
    "\n",
    "print(\"对齐后帧数:\", min_len)\n",
    "print(\"清理后标签数:\", len(labels_clean))\n",
    "print(\"清理后骨架 shape:\", poses_clean.shape)\n",
    "\n",
    "# ================================\n",
    "# 3. 窗口切分 (生成样本)\n",
    "# ================================\n",
    "window = 100   # 窗口长度（帧数）\n",
    "stride = 100   # 步长\n",
    "X_samples, y_samples = [], []\n",
    "\n",
    "for start in range(0, len(poses_clean) - window + 1, stride):\n",
    "    end = start + window\n",
    "    mid = start + window // 2   # 取窗口中间帧作为标签\n",
    "    X_samples.append(poses_clean[start:end])   # shape (100, 32, 3)\n",
    "    y_samples.append(labels_clean[mid])        # 对应标签\n",
    "\n",
    "X = np.array(X_samples)   # (num_samples, 100, 32, 3)\n",
    "y = np.array(y_samples)   # (num_samples, )\n",
    "print(\"最终 X shape:\", X.shape)\n",
    "print(\"最终 y shape:\", y.shape)\n",
    "print(\"前10个标签:\", y[:10])\n",
    "\n",
    "# ================================\n",
    "# 4. 保存结果\n",
    "# ================================\n",
    "np.save(\"data/X.npy\", X)\n",
    "np.save(\"data/y.npy\", y)\n",
    "print(\"✅ 已保存到 data/X.npy 和 data/y.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ee9494-7220-43ef-8ee9-fdfb4ff296c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM 输入 shape: (144, 100, 96)\n",
      "y shape: (144,)\n",
      "✅ 已保存 data/X_lstm.npy 和 data/y_lstm.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 读取刚保存的 X, y\n",
    "X = np.load(\"data/X.npy\")\n",
    "y = np.load(\"data/y.npy\")\n",
    "\n",
    "# reshape: (144, 100, 32, 3) -> (144, 100, 96)\n",
    "X_lstm = X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "print(\"LSTM 输入 shape:\", X_lstm.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# 保存结果\n",
    "np.save(\"data/X_lstm.npy\", X_lstm)\n",
    "np.save(\"data/y_lstm.npy\", y)\n",
    "print(\"✅ 已保存 data/X_lstm.npy 和 data/y_lstm.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70892e4b-303d-4324-a28a-cd96fe848d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (144, 100, 96)\n",
      "y shape: (144,)\n",
      "类别数: 10 类别: ['Unknown' 'a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_stand' 't_bed_turn'\n",
      " 't_bend' 't_stand_to_sit' 't_straighten']\n",
      "Epoch [1/20] Train Loss: 17.3454 | Train Acc: 0.2000 | Val Loss: 2.0719 | Val Acc: 0.3103\n",
      "Epoch [2/20] Train Loss: 15.5080 | Train Acc: 0.3130 | Val Loss: 2.0957 | Val Acc: 0.3103\n",
      "Epoch [3/20] Train Loss: 14.5083 | Train Acc: 0.3130 | Val Loss: 2.0472 | Val Acc: 0.3103\n",
      "Epoch [4/20] Train Loss: 15.2651 | Train Acc: 0.3130 | Val Loss: 1.9990 | Val Acc: 0.3103\n",
      "Epoch [5/20] Train Loss: 15.0481 | Train Acc: 0.3304 | Val Loss: 2.0225 | Val Acc: 0.2414\n",
      "Epoch [6/20] Train Loss: 14.3531 | Train Acc: 0.3217 | Val Loss: 2.0384 | Val Acc: 0.3103\n",
      "Epoch [7/20] Train Loss: 14.0567 | Train Acc: 0.3130 | Val Loss: 2.0278 | Val Acc: 0.3103\n",
      "Epoch [8/20] Train Loss: 13.9372 | Train Acc: 0.3130 | Val Loss: 2.0541 | Val Acc: 0.3103\n",
      "Epoch [9/20] Train Loss: 14.8840 | Train Acc: 0.3130 | Val Loss: 2.0223 | Val Acc: 0.2759\n",
      "Epoch [10/20] Train Loss: 13.9140 | Train Acc: 0.3478 | Val Loss: 1.9789 | Val Acc: 0.4483\n",
      "Epoch [11/20] Train Loss: 15.7457 | Train Acc: 0.3739 | Val Loss: 2.1622 | Val Acc: 0.2759\n",
      "Epoch [12/20] Train Loss: 14.5442 | Train Acc: 0.3565 | Val Loss: 2.0422 | Val Acc: 0.1724\n",
      "Epoch [13/20] Train Loss: 14.2573 | Train Acc: 0.4087 | Val Loss: 1.9827 | Val Acc: 0.3448\n",
      "Epoch [14/20] Train Loss: 13.3324 | Train Acc: 0.4609 | Val Loss: 1.9543 | Val Acc: 0.3793\n",
      "Epoch [15/20] Train Loss: 14.3671 | Train Acc: 0.3652 | Val Loss: 2.1479 | Val Acc: 0.3103\n",
      "Epoch [16/20] Train Loss: 15.2939 | Train Acc: 0.3130 | Val Loss: 2.0477 | Val Acc: 0.3103\n",
      "Epoch [17/20] Train Loss: 14.3110 | Train Acc: 0.3217 | Val Loss: 2.0446 | Val Acc: 0.4138\n",
      "Epoch [18/20] Train Loss: 14.4221 | Train Acc: 0.3391 | Val Loss: 2.0565 | Val Acc: 0.4138\n",
      "Epoch [19/20] Train Loss: 13.7514 | Train Acc: 0.3130 | Val Loss: 2.0390 | Val Acc: 0.3103\n",
      "Epoch [20/20] Train Loss: 13.9373 | Train Acc: 0.3130 | Val Loss: 2.0174 | Val Acc: 0.3103\n",
      "\n",
      "分类报告:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 8, does not match size of target_names, 10. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    101\u001b[39m     y_true = y_val.cpu().numpy()\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m分类报告:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_true, y_pred, target_names=le.classes_))\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m混淆矩阵:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_true, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2970\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2964\u001b[39m         warnings.warn(\n\u001b[32m   2965\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2966\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2967\u001b[39m             )\n\u001b[32m   2968\u001b[39m         )\n\u001b[32m   2969\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2974\u001b[39m         )\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2976\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 8, does not match size of target_names, 10. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ==============================\n",
    "# 1. 加载数据\n",
    "# ==============================\n",
    "X = np.load(\"data/X_lstm.npy\")   # (N, 100, 96)\n",
    "y = np.load(\"data/y_lstm.npy\")   # (N, )\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# 标签编码\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"类别数:\", num_classes, \"类别:\", le.classes_)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=None  # ❌ 不分层\n",
    ")\n",
    "\n",
    "\n",
    "# 转换为 tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# ==============================\n",
    "# 2. 定义 LSTM 模型\n",
    "# ==============================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=96, hidden_dim=128, num_layers=2, num_classes=10):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # out: (batch, seq, hidden)\n",
    "        out = out[:, -1, :]    # 取最后一个时间步\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = LSTMClassifier(input_dim=96, hidden_dim=128,\n",
    "                       num_layers=2, num_classes=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ==============================\n",
    "# 3. 训练循环\n",
    "# ==============================\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    perm = torch.randperm(X_train.size(0))\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == yb).sum().item()\n",
    "\n",
    "    train_acc = correct / X_train.size(0)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        val_loss = criterion(outputs, y_val).item()\n",
    "        val_acc = (outputs.argmax(1) == y_val).sum().item() / X_val.size(0)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "          f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 4. 评估\n",
    "# ==============================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_val).argmax(1).cpu().numpy()\n",
    "    y_true = y_val.cpu().numpy()\n",
    "\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# ==============================\n",
    "# 5. 保存模型和标签映射\n",
    "# ==============================\n",
    "torch.save(model.state_dict(), \"data/lstm_baseline.pth\")\n",
    "np.save(\"data/label_classes.npy\", le.classes_)\n",
    "print(\"✅ 模型和标签映射已保存\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0c4c95-fdcd-453b-aa69-a21d2095aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Unknown       0.00      0.00      0.00         5\n",
      "        a_walk       0.00      0.00      0.00         7\n",
      "        p_bent       0.00      0.00      0.00         2\n",
      "         p_lie       0.00      0.00      0.00         2\n",
      "         p_sit       0.00      0.00      0.00         2\n",
      "       p_stand       0.31      1.00      0.47         9\n",
      "t_stand_to_sit       0.00      0.00      0.00         1\n",
      "  t_straighten       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.31        29\n",
      "     macro avg       0.04      0.12      0.06        29\n",
      "  weighted avg       0.10      0.31      0.15        29\n",
      "\n",
      "混淆矩阵:\n",
      "[[0 0 0 0 0 5 0 0]\n",
      " [0 0 0 0 0 7 0 0]\n",
      " [0 0 0 0 0 2 0 0]\n",
      " [0 0 0 0 0 2 0 0]\n",
      " [0 0 0 0 0 2 0 0]\n",
      " [0 0 0 0 0 9 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# 确保 y_true / y_pred 都是 numpy 数组\n",
    "y_true = y_val.cpu().numpy()\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# 找到验证集中实际出现过的类别\n",
    "unique_labels = np.unique(y_true)\n",
    "\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=unique_labels,\n",
    "    target_names=le.classes_[unique_labels]\n",
    "))\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8d13bd-fd75-42cf-8f30-255aeb22f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('Unknown'): np.int64(17), np.str_('a_walk'): np.int64(28), np.str_('p_bent'): np.int64(6), np.str_('p_lie'): np.int64(6), np.str_('p_sit'): np.int64(27), np.str_('p_stand'): np.int64(45), np.str_('t_bed_turn'): np.int64(3), np.str_('t_bend'): np.int64(7), np.str_('t_stand_to_sit'): np.int64(1), np.str_('t_straighten'): np.int64(4)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.load(\"data/y_lstm.npy\", allow_pickle=True)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32e8f7-a411-4720-9b3a-308c5b2f37fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
