{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d6daef-b96d-4b5e-b263-629f6d36f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始骨架 shape: (33955, 32, 3)\n",
      "原始标签 shape: (34001,)\n",
      "\n",
      "对齐后骨架 shape: (33955, 32, 3)\n",
      "对齐后标签 shape: (33955,)\n",
      "\n",
      "标签分布（前10个类别）:\n",
      "Unknown: 4807\n",
      "a_walk: 4333\n",
      "p_stand: 7965\n",
      "t_stand_to_sit: 324\n",
      "p_sit: 10054\n",
      "t_sit_to_stand: 216\n",
      "t_sit_to_lie: 150\n",
      "p_lie: 2157\n",
      "t_bed_turn: 485\n",
      "t_lie_to_situp: 67\n",
      "总类别数: 16\n",
      "\n",
      "清理后骨架 shape: (29148, 32, 3)\n",
      "清理后标签 shape: (29148,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# 1. 加载原始数据\n",
    "# =========================\n",
    "poses = np.load(\"data/poses_19MM.npy\")          # (33955, 32, 3)\n",
    "labels = np.load(\"data/frame_labels.npy\", allow_pickle=True)  # (34001,)\n",
    "\n",
    "print(\"原始骨架 shape:\", poses.shape)\n",
    "print(\"原始标签 shape:\", labels.shape)\n",
    "\n",
    "# =========================\n",
    "# 2. 对齐长度\n",
    "# =========================\n",
    "min_len = min(len(poses), len(labels))\n",
    "poses = poses[:min_len]\n",
    "labels = labels[:min_len]\n",
    "\n",
    "print(\"\\n对齐后骨架 shape:\", poses.shape)\n",
    "print(\"对齐后标签 shape:\", labels.shape)\n",
    "\n",
    "# =========================\n",
    "# 3. 检查类别分布\n",
    "# =========================\n",
    "counts = Counter(labels)\n",
    "print(\"\\n标签分布（前10个类别）:\")\n",
    "for k, v in list(counts.items())[:10]:\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"总类别数:\", len(counts))\n",
    "\n",
    "# =========================\n",
    "# 4. 去掉 'Unknown'\n",
    "# =========================\n",
    "mask = labels != \"Unknown\"\n",
    "poses_clean = poses[mask]\n",
    "labels_clean = labels[mask]\n",
    "\n",
    "print(\"\\n清理后骨架 shape:\", poses_clean.shape)\n",
    "print(\"清理后标签 shape:\", labels_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88f24091-762a-40ff-8e32-c9f2c20abb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始骨架 shape: (33955, 32, 3)\n",
      "原始标签 shape: (34001,)\n",
      "\n",
      "对齐后骨架 shape: (33955, 32, 3)\n",
      "对齐后标签 shape: (33955,)\n",
      "\n",
      "清理后骨架 shape: (29148, 32, 3)\n",
      "清理后标签 shape: (29148,)\n",
      "最终 X shape: (1534, 19, 96)\n",
      "最终 y shape: (1534,)\n",
      "\n",
      "✅ 已保存到 data/X_lstm.npy 和 data/y_lstm.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# 1. 加载原始数据\n",
    "# =========================\n",
    "poses = np.load(\"data/poses_19MM.npy\")          # (33955, 32, 3)\n",
    "labels = np.load(\"data/frame_labels.npy\", allow_pickle=True)  # (34001,)\n",
    "\n",
    "print(\"原始骨架 shape:\", poses.shape)\n",
    "print(\"原始标签 shape:\", labels.shape)\n",
    "\n",
    "# =========================\n",
    "# 2. 对齐长度\n",
    "# =========================\n",
    "min_len = min(len(poses), len(labels))\n",
    "poses = poses[:min_len]\n",
    "labels = labels[:min_len]\n",
    "\n",
    "print(\"\\n对齐后骨架 shape:\", poses.shape)\n",
    "print(\"对齐后标签 shape:\", labels.shape)\n",
    "\n",
    "# =========================\n",
    "# 3. 去掉 'Unknown'\n",
    "# =========================\n",
    "mask = labels != \"Unknown\"\n",
    "poses_clean = poses[mask]\n",
    "labels_clean = labels[mask]\n",
    "\n",
    "print(\"\\n清理后骨架 shape:\", poses_clean.shape)\n",
    "print(\"清理后标签 shape:\", labels_clean.shape)\n",
    "\n",
    "# =========================\n",
    "# 4. 滑窗切分 (每个样本 15 帧)\n",
    "# =========================\n",
    "# ======================\n",
    "# 切分窗口 (改为 19 帧)\n",
    "# ======================\n",
    "window = 19\n",
    "stride = 19   # 或者用更小 stride 来增加样本数量，例如 stride=10\n",
    "\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "\n",
    "for start in range(0, len(poses_clean) - window + 1, stride):\n",
    "    end = start + window\n",
    "    X_samples.append(poses_clean[start:end])  # (19, 32, 3)\n",
    "    mid = start + window // 2\n",
    "    y_samples.append(labels_clean[mid])       # 取中间帧标签\n",
    "\n",
    "X_samples = np.array(X_samples)  # (N, 19, 32, 3)\n",
    "y_samples = np.array(y_samples)\n",
    "\n",
    "# reshape 成 (N, 19, 96) 给 LSTM 用\n",
    "X_samples = X_samples.reshape(X_samples.shape[0], window, -1)\n",
    "\n",
    "print(\"最终 X shape:\", X_samples.shape)\n",
    "print(\"最终 y shape:\", y_samples.shape)\n",
    "\n",
    "# 保存结果\n",
    "np.save(\"data/X_lstm.npy\", X_samples)\n",
    "np.save(\"data/y_lstm.npy\", y_samples)\n",
    "\n",
    "\n",
    "print(\"\\n✅ 已保存到 data/X_lstm.npy 和 data/y_lstm.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d29d81-3342-4836-b5c9-b23b6e9a34c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别数: 15\n",
      "类别映射: {np.str_('a_walk'): 0, np.str_('p_bent'): 1, np.str_('p_lie'): 2, np.str_('p_sit'): 3, np.str_('p_situp'): 4, np.str_('p_stand'): 5, np.str_('t_bed_turn'): 6, np.str_('t_bend'): 7, np.str_('t_lie_to_sit'): 8, np.str_('t_lie_to_situp'): 9, np.str_('t_sit_to_lie'): 10, np.str_('t_sit_to_stand'): 11, np.str_('t_situp_to_sit'): 12, np.str_('t_stand_to_sit'): 13, np.str_('t_straighten'): 14}\n",
      "✅ 已保存 data/y_lstm_encoded.npy 和 data/label_classes.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取刚才保存的标签\n",
    "y = np.load(\"data/y_lstm.npy\", allow_pickle=True)\n",
    "\n",
    "# 编码成 0,1,2,... \n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(\"类别数:\", len(le.classes_))\n",
    "print(\"类别映射:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "# 保存\n",
    "np.save(\"data/y_lstm_encoded.npy\", y_encoded)\n",
    "np.save(\"data/label_classes.npy\", le.classes_)\n",
    "\n",
    "print(\"✅ 已保存 data/y_lstm_encoded.npy 和 data/label_classes.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4e66417-4926-436c-a33b-6df397313ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修正后 X shape: (1534, 19, 96) y shape: (1534,)\n",
      "类别数: 15 类别: ['a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_situp' 'p_stand' 't_bed_turn'\n",
      " 't_bend' 't_lie_to_sit' 't_lie_to_situp' 't_sit_to_lie' 't_sit_to_stand'\n",
      " 't_situp_to_sit' 't_stand_to_sit' 't_straighten']\n",
      "Epoch 1/30 | Train Loss: 2.6446 Acc: 0.1361 | Val Loss: 2.5403 Acc: 0.0847\n",
      "Epoch 2/30 | Train Loss: 2.5267 Acc: 0.1597 | Val Loss: 2.3858 Acc: 0.1238\n",
      "Epoch 3/30 | Train Loss: 2.4460 Acc: 0.1907 | Val Loss: 2.3910 Acc: 0.2345\n",
      "Epoch 4/30 | Train Loss: 2.4433 Acc: 0.1280 | Val Loss: 2.3972 Acc: 0.1466\n",
      "Epoch 5/30 | Train Loss: 2.4055 Acc: 0.1385 | Val Loss: 2.3639 Acc: 0.2117\n",
      "Epoch 6/30 | Train Loss: 2.4165 Acc: 0.1589 | Val Loss: 2.3466 Acc: 0.2117\n",
      "Epoch 7/30 | Train Loss: 2.4028 Acc: 0.1785 | Val Loss: 2.3596 Acc: 0.1303\n",
      "Epoch 8/30 | Train Loss: 2.3588 Acc: 0.1720 | Val Loss: 2.3555 Acc: 0.1531\n",
      "Epoch 9/30 | Train Loss: 2.3757 Acc: 0.1606 | Val Loss: 2.3834 Acc: 0.1726\n",
      "Epoch 10/30 | Train Loss: 2.3578 Acc: 0.1850 | Val Loss: 2.3469 Acc: 0.1466\n",
      "Epoch 11/30 | Train Loss: 2.3025 Acc: 0.2143 | Val Loss: 2.3322 Acc: 0.1987\n",
      "Epoch 12/30 | Train Loss: 2.3256 Acc: 0.2494 | Val Loss: 2.3806 Acc: 0.2052\n",
      "Epoch 13/30 | Train Loss: 2.2928 Acc: 0.1940 | Val Loss: 2.3603 Acc: 0.1173\n",
      "Epoch 14/30 | Train Loss: 2.3308 Acc: 0.2290 | Val Loss: 2.3396 Acc: 0.2573\n",
      "Epoch 15/30 | Train Loss: 2.2999 Acc: 0.2274 | Val Loss: 2.3536 Acc: 0.1433\n",
      "Epoch 16/30 | Train Loss: 2.2948 Acc: 0.2200 | Val Loss: 2.3287 Acc: 0.2020\n",
      "Epoch 17/30 | Train Loss: 2.2875 Acc: 0.2225 | Val Loss: 2.3215 Acc: 0.1954\n",
      "Epoch 18/30 | Train Loss: 2.2810 Acc: 0.2323 | Val Loss: 2.2932 Acc: 0.1433\n",
      "Epoch 19/30 | Train Loss: 2.2747 Acc: 0.1948 | Val Loss: 2.2919 Acc: 0.1954\n",
      "Epoch 20/30 | Train Loss: 2.2802 Acc: 0.2241 | Val Loss: 2.3100 Acc: 0.2085\n",
      "Epoch 21/30 | Train Loss: 2.2708 Acc: 0.2225 | Val Loss: 2.3140 Acc: 0.1433\n",
      "Epoch 22/30 | Train Loss: 2.2693 Acc: 0.2755 | Val Loss: 2.3301 Acc: 0.2182\n",
      "Epoch 23/30 | Train Loss: 2.2476 Acc: 0.2575 | Val Loss: 2.3164 Acc: 0.2150\n",
      "Epoch 24/30 | Train Loss: 2.2170 Acc: 0.2543 | Val Loss: 2.3123 Acc: 0.2606\n",
      "Epoch 25/30 | Train Loss: 2.2141 Acc: 0.2730 | Val Loss: 2.3105 Acc: 0.2638\n",
      "Epoch 26/30 | Train Loss: 2.2149 Acc: 0.2820 | Val Loss: 2.3022 Acc: 0.2671\n",
      "Epoch 27/30 | Train Loss: 2.1928 Acc: 0.2771 | Val Loss: 2.3204 Acc: 0.2671\n",
      "Epoch 28/30 | Train Loss: 2.1954 Acc: 0.2714 | Val Loss: 2.3221 Acc: 0.1954\n",
      "Epoch 29/30 | Train Loss: 2.2096 Acc: 0.2535 | Val Loss: 2.3139 Acc: 0.1889\n",
      "Epoch 30/30 | Train Loss: 2.1964 Acc: 0.2559 | Val Loss: 2.3163 Acc: 0.2052\n",
      "\n",
      "分类报告:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        a_walk       0.48      0.22      0.30        46\n",
      "        p_bent       0.35      0.38      0.36        16\n",
      "         p_lie       0.11      0.70      0.20        23\n",
      "         p_sit       0.70      0.15      0.25       105\n",
      "       p_situp       0.00      0.00      0.00         3\n",
      "       p_stand       0.60      0.11      0.19        82\n",
      "    t_bed_turn       0.00      0.00      0.00         5\n",
      "        t_bend       0.08      0.38      0.14         8\n",
      "  t_lie_to_sit       0.00      0.00      0.00         0\n",
      "t_lie_to_situp       0.00      0.00      0.00         1\n",
      "  t_sit_to_lie       0.00      0.00      0.00         2\n",
      "t_sit_to_stand       0.00      0.00      0.00         3\n",
      "t_situp_to_sit       0.00      0.00      0.00         2\n",
      "t_stand_to_sit       0.00      0.00      0.00         4\n",
      "  t_straighten       0.30      0.43      0.35         7\n",
      "\n",
      "      accuracy                           0.21       307\n",
      "     macro avg       0.17      0.16      0.12       307\n",
      "  weighted avg       0.51      0.21      0.23       307\n",
      "\n",
      "\n",
      "混淆矩阵:\n",
      "[[10  2 15  3  0  3  0  9  1  0  0  0  0  0  3]\n",
      " [ 0  6  3  2  0  1  0  2  2  0  0  0  0  0  0]\n",
      " [ 0  0 16  0  0  0  0  5  2  0  0  0  0  0  0]\n",
      " [ 7  7 59 16  1  2  0  3 10  0  0  0  0  0  0]\n",
      " [ 1  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  2 32  2  0  9  0  9 23  0  0  0  0  0  3]\n",
      " [ 0  0  3  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  3  3  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  1  0  0  0  0  0  3]]\n",
      "✅ 模型已保存到 data/lstm_bilstm_weighted_final.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# =====================\n",
    "# 1. 加载数据\n",
    "# =====================\n",
    "X = np.load(\"data/X_lstm.npy\")        # (N, T, 32, 3) 或 (N, T, 96)\n",
    "y = np.load(\"data/y_lstm.npy\")        # (N, )\n",
    "\n",
    "# 如果是 4D 就 reshape 成 (N, T, 96)\n",
    "if X.ndim == 4:\n",
    "    N, T, J, C = X.shape\n",
    "    X = X.reshape(N, T, J * C)\n",
    "\n",
    "print(\"修正后 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# 标签编码\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"类别数:\", num_classes, \"类别:\", le.classes_)\n",
    "\n",
    "# 保存类别映射\n",
    "np.save(\"data/label_classes.npy\", le.classes_)\n",
    "\n",
    "# =====================\n",
    "# 2. 划分训练/验证集\n",
    "# =====================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# 转 Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "# =====================\n",
    "# 3. BiLSTM 模型\n",
    "# =====================\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # 双向 ×2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)   # (batch, seq_len, hidden*2)\n",
    "        out = out[:, -1, :]     # 取最后时刻\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# =====================\n",
    "# 4. 损失函数 + 优化器 + 学习率调度\n",
    "# =====================\n",
    "# 类别权重（按训练集统计）\n",
    "class_counts = Counter(y_train.numpy())\n",
    "weights = torch.tensor([1.0 / (class_counts[i] + 1e-6) for i in range(num_classes)],\n",
    "                       dtype=torch.float32)\n",
    "weights = weights / weights.sum() * num_classes  # 归一化\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMClassifier(input_size=X.shape[2], hidden_size=128, num_layers=2,\n",
    "                         num_classes=num_classes, dropout=0.5).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# 5. 训练循环\n",
    "# =====================\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_loss /= total\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_loss /= total\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "# =====================\n",
    "# 6. 评估\n",
    "# =====================\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"\\n分类报告:\")\n",
    "labels = list(range(len(le.classes_)))  # 保证和 target_names 对齐\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=labels,\n",
    "    target_names=le.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(\"\\n混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=labels))\n",
    "\n",
    "# =====================\n",
    "# 7. 保存模型\n",
    "# =====================\n",
    "torch.save(model.state_dict(), \"data/lstm_bilstm_weighted_final.pth\")\n",
    "print(\"✅ 模型已保存到 data/lstm_bilstm_weighted_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b87aed5e-3c59-429f-a484-ff13ca57a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (1534, 19, 96) y shape: (1534,)\n",
      "类别数: 15 类别: ['a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_situp' 'p_stand' 't_bed_turn'\n",
      " 't_bend' 't_lie_to_sit' 't_lie_to_situp' 't_sit_to_lie' 't_sit_to_stand'\n",
      " 't_situp_to_sit' 't_stand_to_sit' 't_straighten']\n",
      "平衡后 X shape: (2383, 19, 96) y shape: (2383,)\n",
      "Epoch 1/30 | Train Loss: 2.5073 Acc: 0.1159 | Val Loss: 2.3586 Acc: 0.1824\n",
      "Epoch 2/30 | Train Loss: 2.3422 Acc: 0.1600 | Val Loss: 2.2286 Acc: 0.1929\n",
      "Epoch 3/30 | Train Loss: 2.2937 Acc: 0.1579 | Val Loss: 2.2577 Acc: 0.1803\n",
      "Epoch 4/30 | Train Loss: 2.2891 Acc: 0.1710 | Val Loss: 2.2549 Acc: 0.1845\n",
      "Epoch 5/30 | Train Loss: 2.2563 Acc: 0.1878 | Val Loss: 2.2271 Acc: 0.2034\n",
      "Epoch 6/30 | Train Loss: 2.2614 Acc: 0.1742 | Val Loss: 2.2169 Acc: 0.1887\n",
      "Epoch 7/30 | Train Loss: 2.2202 Acc: 0.1857 | Val Loss: 2.1773 Acc: 0.1908\n",
      "Epoch 8/30 | Train Loss: 2.2158 Acc: 0.1910 | Val Loss: 2.1873 Acc: 0.1866\n",
      "Epoch 9/30 | Train Loss: 2.1996 Acc: 0.1910 | Val Loss: 2.1831 Acc: 0.1992\n",
      "Epoch 10/30 | Train Loss: 2.1932 Acc: 0.2141 | Val Loss: 2.1934 Acc: 0.1845\n",
      "Epoch 11/30 | Train Loss: 2.1747 Acc: 0.1994 | Val Loss: 2.1743 Acc: 0.1782\n",
      "Epoch 12/30 | Train Loss: 2.1668 Acc: 0.1957 | Val Loss: 2.1642 Acc: 0.1866\n",
      "Epoch 13/30 | Train Loss: 2.1470 Acc: 0.1983 | Val Loss: 2.1555 Acc: 0.1824\n",
      "Epoch 14/30 | Train Loss: 2.1481 Acc: 0.2130 | Val Loss: 2.1462 Acc: 0.2055\n",
      "Epoch 15/30 | Train Loss: 2.1240 Acc: 0.2177 | Val Loss: 2.1480 Acc: 0.1866\n",
      "Epoch 16/30 | Train Loss: 2.1321 Acc: 0.2162 | Val Loss: 2.1681 Acc: 0.1866\n",
      "Epoch 17/30 | Train Loss: 2.1311 Acc: 0.2109 | Val Loss: 2.1271 Acc: 0.2138\n",
      "Epoch 18/30 | Train Loss: 2.1292 Acc: 0.2219 | Val Loss: 2.1313 Acc: 0.1992\n",
      "Epoch 19/30 | Train Loss: 2.1169 Acc: 0.2225 | Val Loss: 2.1444 Acc: 0.2222\n",
      "Epoch 20/30 | Train Loss: 2.1352 Acc: 0.2251 | Val Loss: 2.2121 Acc: 0.1971\n",
      "Epoch 21/30 | Train Loss: 2.1155 Acc: 0.2240 | Val Loss: 2.1184 Acc: 0.2327\n",
      "Epoch 22/30 | Train Loss: 2.0804 Acc: 0.2429 | Val Loss: 2.0838 Acc: 0.2537\n",
      "Epoch 23/30 | Train Loss: 2.0712 Acc: 0.2455 | Val Loss: 2.1047 Acc: 0.2432\n",
      "Epoch 24/30 | Train Loss: 2.0624 Acc: 0.2545 | Val Loss: 2.0904 Acc: 0.2495\n",
      "Epoch 25/30 | Train Loss: 2.0558 Acc: 0.2560 | Val Loss: 2.0876 Acc: 0.2411\n",
      "Epoch 26/30 | Train Loss: 2.0703 Acc: 0.2471 | Val Loss: 2.1488 Acc: 0.2201\n",
      "Epoch 27/30 | Train Loss: 2.1032 Acc: 0.2455 | Val Loss: 2.0855 Acc: 0.2432\n",
      "Epoch 28/30 | Train Loss: 2.0493 Acc: 0.2524 | Val Loss: 2.0753 Acc: 0.2306\n",
      "Epoch 29/30 | Train Loss: 2.0609 Acc: 0.2408 | Val Loss: 2.0945 Acc: 0.2180\n",
      "Epoch 30/30 | Train Loss: 2.0549 Acc: 0.2455 | Val Loss: 2.0778 Acc: 0.2285\n",
      "\n",
      "分类报告:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        a_walk       0.56      0.20      0.29        46\n",
      "        p_bent       0.41      0.45      0.43        20\n",
      "         p_lie       0.13      0.13      0.13        23\n",
      "         p_sit       0.91      0.20      0.33       106\n",
      "       p_situp       0.29      0.10      0.15        20\n",
      "       p_stand       0.67      0.05      0.09        82\n",
      "    t_bed_turn       0.27      0.15      0.19        20\n",
      "        t_bend       0.11      0.15      0.13        20\n",
      "  t_lie_to_sit       0.31      1.00      0.47        20\n",
      "t_lie_to_situp       0.08      1.00      0.15        20\n",
      "  t_sit_to_lie       1.00      0.05      0.10        20\n",
      "t_sit_to_stand       0.00      0.00      0.00        20\n",
      "t_situp_to_sit       1.00      0.05      0.10        20\n",
      "t_stand_to_sit       1.00      0.10      0.18        20\n",
      "  t_straighten       0.35      0.55      0.43        20\n",
      "\n",
      "      accuracy                           0.23       477\n",
      "     macro avg       0.47      0.28      0.21       477\n",
      "  weighted avg       0.58      0.23      0.22       477\n",
      "\n",
      "混淆矩阵:\n",
      "[[ 9  2  4  2  1  0  1  5  2 16  0  0  0  0  4]\n",
      " [ 0  9  1  0  0  0  0  2  0  8  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  1 17  0  0  0  0  2]\n",
      " [ 3  6  1 21  4  1  0  1  8 61  0  0  0  0  0]\n",
      " [ 1  0  0  0  2  0  2  0  0 15  0  0  0  0  0]\n",
      " [ 2  5  2  0  0  4  0 12 22 28  0  0  0  0  7]\n",
      " [ 0  0  4  0  0  0  3  0  0 12  0  0  0  0  1]\n",
      " [ 1  0  1  0  0  1  5  3  4  1  0  0  0  0  4]\n",
      " [ 0  0  0  0  0  0  0  0 20  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 19  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1 16  0  0  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0 19  0  0  1  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  4  9  0  0  0  2  0]\n",
      " [ 0  0  2  0  0  0  0  3  3  1  0  0  0  0 11]]\n",
      "✅ 模型已保存到 data/lstm_bilstm_balanced_augmented.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "\n",
    "# =============== 数据增强函数 ===============\n",
    "def augment_skeleton_sequence(seq, noise_std=0.01, rotate=True):\n",
    "    \"\"\"\n",
    "    对单个骨架序列进行增强\n",
    "    seq: (T, J) or (T, J, C)，时间长度 T，关节数 J，坐标维度 C\n",
    "    \"\"\"\n",
    "    seq_aug = seq.copy()\n",
    "\n",
    "    # 1. 加入高斯噪声\n",
    "    seq_aug = seq_aug + np.random.normal(0, noise_std, seq_aug.shape)\n",
    "\n",
    "    # 2. 随机旋转 (仅绕 Z 轴, 模拟平面旋转)\n",
    "    if rotate:\n",
    "        theta = np.random.uniform(-10, 10) * np.pi / 180  # -10° ~ +10°\n",
    "        cos_t, sin_t = np.cos(theta), np.sin(theta)\n",
    "        R = np.array([[cos_t, -sin_t, 0],\n",
    "                      [sin_t,  cos_t, 0],\n",
    "                      [0,      0,     1]])\n",
    "        if seq_aug.ndim == 3 and seq_aug.shape[2] >= 3:\n",
    "            seq_aug[..., :3] = seq_aug[..., :3] @ R.T\n",
    "\n",
    "    return seq_aug\n",
    "\n",
    "\n",
    "def balance_and_augment(X, y, min_samples=50):\n",
    "    \"\"\"\n",
    "    平衡并增强数据\n",
    "    X: (N, T, F) 骨架序列\n",
    "    y: (N,) 标签\n",
    "    \"\"\"\n",
    "    X_bal, y_bal = [], []\n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    for cls in classes:\n",
    "        X_cls = X[y == cls]\n",
    "        y_cls = y[y == cls]\n",
    "\n",
    "        # 如果类别太少，复制到 min_samples\n",
    "        if len(X_cls) < min_samples:\n",
    "            X_res, y_res = resample(\n",
    "                X_cls, y_cls,\n",
    "                replace=True,\n",
    "                n_samples=min_samples,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            X_res, y_res = X_cls, y_cls\n",
    "\n",
    "        # 数据增强\n",
    "        X_aug = np.array([augment_skeleton_sequence(seq) for seq in X_res])\n",
    "\n",
    "        X_bal.append(X_aug)\n",
    "        y_bal.append(y_res)\n",
    "\n",
    "    X_bal = np.vstack(X_bal)\n",
    "    y_bal = np.hstack(y_bal)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# =============== BiLSTM 模型 ===============\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (batch, seq_len, hidden*2)\n",
    "        out = out[:, -1, :]    # 取最后时刻\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# =============== 主程序 ===============\n",
    "if __name__ == \"__main__\":\n",
    "    # 固定随机种子，保证复现\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    X = np.load(\"data/X_lstm.npy\")\n",
    "    y = np.load(\"data/y_lstm.npy\")\n",
    "\n",
    "\n",
    "    print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "    # 2. 标签编码\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    num_classes = len(le.classes_)\n",
    "    print(\"类别数:\", num_classes, \"类别:\", le.classes_)\n",
    "\n",
    "    # 3. 样本均衡 + 增强\n",
    "    X_bal, y_bal = balance_and_augment(X, y_encoded, min_samples=100)\n",
    "    print(\"平衡后 X shape:\", X_bal.shape, \"y shape:\", y_bal.shape)\n",
    "\n",
    "    # 4. 划分训练/验证集\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_bal, y_bal, test_size=0.2, random_state=42, stratify=y_bal\n",
    "    )\n",
    "\n",
    "    # 转 tensor\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    # 5. DataLoader\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    # 6. 模型、优化器、损失函数\n",
    "    input_size = X.shape[2]\n",
    "    hidden_size = 128\n",
    "    model = BiLSTMClassifier(input_size, hidden_size, num_classes, num_layers=2, dropout=0.3)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 加权交叉熵\n",
    "    class_counts = np.bincount(y_bal)\n",
    "    weights = torch.tensor(1.0 / (class_counts + 1e-6), dtype=torch.float32)\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # 7. 训练\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_loss / total\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_acc = correct / total\n",
    "        val_loss /= total\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # 8. 评估\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    # 9. 保存模型\n",
    "    torch.save(model.state_dict(), \"data/lstm_bilstm_balanced_augmented.pth\")\n",
    "    print(\"✅ 模型已保存到 data/lstm_bilstm_balanced_augmented.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cecc5ec5-3c11-47da-bc63-100a40095485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (1534, 19, 96) y shape: (1534,)\n",
      "原始类别分布: Counter({np.str_('p_sit'): 527, np.str_('p_stand'): 412, np.str_('a_walk'): 231, np.str_('p_lie'): 113, np.str_('p_bent'): 78, np.str_('t_bend'): 41, np.str_('t_straighten'): 35, np.str_('t_bed_turn'): 25, np.str_('t_stand_to_sit'): 18, np.str_('p_situp'): 16, np.str_('t_sit_to_stand'): 13, np.str_('t_situp_to_sit'): 10, np.str_('t_sit_to_lie'): 8, np.str_('t_lie_to_situp'): 4, np.str_('t_lie_to_sit'): 3})\n",
      "过滤后 X shape: (1519, 19, 96) y shape: (1519,)\n",
      "保留类别: ['a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_situp' 'p_stand' 't_bed_turn'\n",
      " 't_bend' 't_sit_to_stand' 't_situp_to_sit' 't_stand_to_sit'\n",
      " 't_straighten']\n",
      "类别数: 12 类别: ['a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_situp' 'p_stand' 't_bed_turn'\n",
      " 't_bend' 't_sit_to_stand' 't_situp_to_sit' 't_stand_to_sit'\n",
      " 't_straighten']\n",
      "Epoch 1/30 | Train Loss: 2.3980 Acc: 0.0872 | Val Loss: 2.4435 Acc: 0.1118\n",
      "Epoch 2/30 | Train Loss: 2.3056 Acc: 0.1490 | Val Loss: 2.3673 Acc: 0.0757\n",
      "Epoch 3/30 | Train Loss: 2.2396 Acc: 0.2263 | Val Loss: 2.3467 Acc: 0.1513\n",
      "Epoch 4/30 | Train Loss: 2.2373 Acc: 0.1638 | Val Loss: 2.2957 Acc: 0.1217\n",
      "Epoch 5/30 | Train Loss: 2.2121 Acc: 0.1860 | Val Loss: 2.3152 Acc: 0.1908\n",
      "Epoch 6/30 | Train Loss: 2.2180 Acc: 0.2099 | Val Loss: 2.3191 Acc: 0.2039\n",
      "Epoch 7/30 | Train Loss: 2.1952 Acc: 0.2247 | Val Loss: 2.3392 Acc: 0.2072\n",
      "Epoch 8/30 | Train Loss: 2.1797 Acc: 0.1860 | Val Loss: 2.3654 Acc: 0.1645\n",
      "Epoch 9/30 | Train Loss: 2.2031 Acc: 0.1844 | Val Loss: 2.2950 Acc: 0.2105\n",
      "Epoch 10/30 | Train Loss: 2.1853 Acc: 0.2667 | Val Loss: 2.3150 Acc: 0.2303\n",
      "Epoch 11/30 | Train Loss: 2.1554 Acc: 0.2091 | Val Loss: 2.3396 Acc: 0.1513\n",
      "Epoch 12/30 | Train Loss: 2.1533 Acc: 0.1605 | Val Loss: 2.3171 Acc: 0.1645\n",
      "Epoch 13/30 | Train Loss: 2.1274 Acc: 0.2296 | Val Loss: 2.3245 Acc: 0.1842\n",
      "Epoch 14/30 | Train Loss: 2.1239 Acc: 0.1967 | Val Loss: 2.3251 Acc: 0.1809\n",
      "Epoch 15/30 | Train Loss: 2.1223 Acc: 0.1885 | Val Loss: 2.3515 Acc: 0.2171\n",
      "Epoch 16/30 | Train Loss: 2.1169 Acc: 0.2008 | Val Loss: 2.3031 Acc: 0.2072\n",
      "Epoch 17/30 | Train Loss: 2.1157 Acc: 0.2584 | Val Loss: 2.3206 Acc: 0.2664\n",
      "Epoch 18/30 | Train Loss: 2.1084 Acc: 0.2502 | Val Loss: 2.3261 Acc: 0.1546\n",
      "Epoch 19/30 | Train Loss: 2.1035 Acc: 0.1852 | Val Loss: 2.2959 Acc: 0.2204\n",
      "Epoch 20/30 | Train Loss: 2.0984 Acc: 0.2428 | Val Loss: 2.2986 Acc: 0.1546\n",
      "Epoch 21/30 | Train Loss: 2.0990 Acc: 0.1819 | Val Loss: 2.2937 Acc: 0.1842\n",
      "Epoch 22/30 | Train Loss: 2.0911 Acc: 0.2346 | Val Loss: 2.2508 Acc: 0.1480\n",
      "Epoch 23/30 | Train Loss: 2.0921 Acc: 0.2099 | Val Loss: 2.3303 Acc: 0.2500\n",
      "Epoch 24/30 | Train Loss: 2.0681 Acc: 0.2733 | Val Loss: 2.2608 Acc: 0.1842\n",
      "Epoch 25/30 | Train Loss: 2.0635 Acc: 0.2634 | Val Loss: 2.3178 Acc: 0.2730\n",
      "Epoch 26/30 | Train Loss: 2.0579 Acc: 0.2741 | Val Loss: 2.2643 Acc: 0.2105\n",
      "Epoch 27/30 | Train Loss: 2.0769 Acc: 0.2658 | Val Loss: 2.2736 Acc: 0.2599\n",
      "Epoch 28/30 | Train Loss: 2.0499 Acc: 0.3029 | Val Loss: 2.3138 Acc: 0.2664\n",
      "Epoch 29/30 | Train Loss: 2.0528 Acc: 0.2724 | Val Loss: 2.3085 Acc: 0.2664\n",
      "Epoch 30/30 | Train Loss: 2.0433 Acc: 0.2774 | Val Loss: 2.3395 Acc: 0.2664\n",
      "\n",
      "分类报告:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        a_walk       0.48      0.22      0.30        46\n",
      "        p_bent       0.32      0.38      0.34        16\n",
      "         p_lie       0.12      0.70      0.20        23\n",
      "         p_sit       0.78      0.17      0.28       105\n",
      "       p_situp       0.00      0.00      0.00         3\n",
      "       p_stand       0.52      0.34      0.41        82\n",
      "    t_bed_turn       0.00      0.00      0.00         5\n",
      "        t_bend       0.03      0.12      0.04         8\n",
      "t_sit_to_stand       0.00      0.00      0.00         3\n",
      "t_situp_to_sit       0.00      0.00      0.00         2\n",
      "t_stand_to_sit       0.00      0.00      0.00         4\n",
      "  t_straighten       0.18      0.29      0.22         7\n",
      "\n",
      "      accuracy                           0.27       304\n",
      "     macro avg       0.20      0.18      0.15       304\n",
      "  weighted avg       0.51      0.27      0.29       304\n",
      "\n",
      "\n",
      "混淆矩阵:\n",
      "[[10  3 14  2  0  4  0  9  0  0  0  4]\n",
      " [ 0  6  3  2  0  3  0  2  0  0  0  0]\n",
      " [ 0  0 16  0  0  1  0  6  0  0  0  0]\n",
      " [ 7  6 59 18  0 11  0  4  0  0  0  0]\n",
      " [ 1  0  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  3 32  1  0 28  0 14  0  0  0  2]\n",
      " [ 0  0  3  0  0  0  0  2  0  0  0  0]\n",
      " [ 1  0  0  0  0  3  0  1  0  0  0  3]\n",
      " [ 0  0  1  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  2  0  1  0  0  0  2]]\n",
      "✅ 模型已保存到 data/lstm_bilstm_filtered.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# =====================\n",
    "# 1. 固定随机种子\n",
    "# =====================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =====================\n",
    "# 2. 加载数据\n",
    "# =====================\n",
    "X = np.load(\"data/X_lstm.npy\")   # shape: (N, T, F)\n",
    "y = np.load(\"data/y_lstm.npy\")   # shape: (N, )\n",
    "\n",
    "print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# =====================\n",
    "# 3. 删除样本过少类别\n",
    "# =====================\n",
    "min_support = 10  # 阈值（可以改成20等）\n",
    "counter = Counter(y)\n",
    "print(\"原始类别分布:\", counter)\n",
    "\n",
    "# 筛选合法类别\n",
    "valid_classes = [cls for cls, cnt in counter.items() if cnt >= min_support]\n",
    "mask = np.isin(y, valid_classes)\n",
    "\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(\"过滤后 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"保留类别:\", np.unique(y))\n",
    "\n",
    "# =====================\n",
    "# 4. 标签编码\n",
    "# =====================\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"类别数:\", num_classes, \"类别:\", le.classes_)\n",
    "\n",
    "# =====================\n",
    "# 5. 转换为 Tensor\n",
    "# =====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, stratify=y_tensor, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# =====================\n",
    "# 6. 定义 BiLSTM 模型\n",
    "# =====================\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)   # (batch, seq_len, hidden*2)\n",
    "        out = out[:, -1, :]     # 取最后时刻\n",
    "        return self.fc(out)\n",
    "\n",
    "# =====================\n",
    "# 7. 初始化\n",
    "# =====================\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "model = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# 类别权重（加权交叉熵）\n",
    "class_counts = np.bincount(y_encoded)\n",
    "weights = torch.tensor(1.0 / (class_counts + 1e-6), dtype=torch.float32).to(device)\n",
    "weights = weights / weights.sum() * num_classes\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# =====================\n",
    "# 8. 训练 & 验证\n",
    "# =====================\n",
    "def train_model(epochs=30):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/30 | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # 最终报告\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "    print(\"\\n混淆矩阵:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    torch.save(model.state_dict(), \"data/lstm_bilstm_filtered.pth\")\n",
    "    print(\"✅ 模型已保存到 data/lstm_bilstm_filtered.pth\")\n",
    "\n",
    "# =====================\n",
    "# 9. 开始训练\n",
    "# =====================\n",
    "train_model(epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8956723e-20f9-431b-80c3-3f9483afe3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (1534, 19, 96) y shape: (1534,)\n",
      "原始类别分布: Counter({np.str_('p_sit'): 527, np.str_('p_stand'): 412, np.str_('a_walk'): 231, np.str_('p_lie'): 113, np.str_('p_bent'): 78, np.str_('t_bend'): 41, np.str_('t_straighten'): 35, np.str_('t_bed_turn'): 25, np.str_('t_stand_to_sit'): 18, np.str_('p_situp'): 16, np.str_('t_sit_to_stand'): 13, np.str_('t_situp_to_sit'): 10, np.str_('t_sit_to_lie'): 8, np.str_('t_lie_to_situp'): 4, np.str_('t_lie_to_sit'): 3})\n",
      "保留类别: [np.str_('p_sit'), np.str_('p_stand'), np.str_('a_walk'), np.str_('p_lie'), np.str_('p_bent')]\n",
      "过滤后 X shape: (1361, 19, 96) y shape: (1361,)\n",
      "过滤后类别分布: Counter({np.str_('p_sit'): 527, np.str_('p_stand'): 412, np.str_('a_walk'): 231, np.str_('p_lie'): 113, np.str_('p_bent'): 78})\n",
      "✅ 已保存到 X_top5.npy / y_top5.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 加载原始数据\n",
    "X = np.load(\"data/X_lstm.npy\")\n",
    "y = np.load(\"data/y_lstm.npy\")\n",
    "\n",
    "print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# 选取前5个类别\n",
    "top5 = [cls for cls, _ in Counter(y).most_common(5)]\n",
    "print(\"保留类别:\", top5)\n",
    "\n",
    "# 过滤数据\n",
    "mask = np.isin(y, top5)\n",
    "X_filtered = X[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "print(\"过滤后 X shape:\", X_filtered.shape, \"y shape:\", y_filtered.shape)\n",
    "print(\"过滤后类别分布:\", Counter(y_filtered))\n",
    "\n",
    "# 保存过滤后的数据\n",
    "np.save(\"X_top5.npy\", X_filtered)\n",
    "np.save(\"y_top5.npy\", y_filtered)\n",
    "\n",
    "print(\"✅ 已保存到 X_top5.npy / y_top5.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c66544-2068-4418-8583-bc07b21342b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (1361, 19, 96) y shape: (1361,)\n",
      "类别分布: Counter({np.str_('p_sit'): 527, np.str_('p_stand'): 412, np.str_('a_walk'): 231, np.str_('p_lie'): 113, np.str_('p_bent'): 78})\n",
      "类别数: 5 类别: ['a_walk' 'p_bent' 'p_lie' 'p_sit' 'p_stand']\n",
      "Epoch 1/20 | Train Loss: 1.4543 Acc: 0.4724 | Val Loss: 1.3705 Acc: 0.4762\n",
      "Epoch 2/20 | Train Loss: 1.3170 Acc: 0.5083 | Val Loss: 1.3198 Acc: 0.4725\n",
      "Epoch 3/20 | Train Loss: 1.2769 Acc: 0.5156 | Val Loss: 1.2936 Acc: 0.4725\n",
      "Epoch 4/20 | Train Loss: 1.2550 Acc: 0.5175 | Val Loss: 1.2843 Acc: 0.4725\n",
      "Epoch 5/20 | Train Loss: 1.2490 Acc: 0.5175 | Val Loss: 1.2804 Acc: 0.4725\n",
      "Epoch 6/20 | Train Loss: 1.2388 Acc: 0.5239 | Val Loss: 1.2838 Acc: 0.4762\n",
      "Epoch 7/20 | Train Loss: 1.2282 Acc: 0.5276 | Val Loss: 1.2753 Acc: 0.4799\n",
      "Epoch 8/20 | Train Loss: 1.2256 Acc: 0.5294 | Val Loss: 1.2768 Acc: 0.4725\n",
      "Epoch 9/20 | Train Loss: 1.2204 Acc: 0.5285 | Val Loss: 1.2832 Acc: 0.4689\n",
      "Epoch 10/20 | Train Loss: 1.2143 Acc: 0.5340 | Val Loss: 1.2837 Acc: 0.4689\n",
      "Epoch 11/20 | Train Loss: 1.2108 Acc: 0.5331 | Val Loss: 1.2892 Acc: 0.4689\n",
      "Epoch 12/20 | Train Loss: 1.2129 Acc: 0.5331 | Val Loss: 1.2817 Acc: 0.4762\n",
      "Epoch 13/20 | Train Loss: 1.2063 Acc: 0.5312 | Val Loss: 1.2643 Acc: 0.4762\n",
      "Epoch 14/20 | Train Loss: 1.2130 Acc: 0.5303 | Val Loss: 1.2620 Acc: 0.4652\n",
      "Epoch 15/20 | Train Loss: 1.1967 Acc: 0.5331 | Val Loss: 1.2577 Acc: 0.4689\n",
      "Epoch 16/20 | Train Loss: 1.1916 Acc: 0.5368 | Val Loss: 1.2696 Acc: 0.4762\n",
      "Epoch 17/20 | Train Loss: 1.1920 Acc: 0.5349 | Val Loss: 1.2712 Acc: 0.4762\n",
      "Epoch 18/20 | Train Loss: 1.1929 Acc: 0.5377 | Val Loss: 1.2622 Acc: 0.4799\n",
      "Epoch 19/20 | Train Loss: 1.1907 Acc: 0.5368 | Val Loss: 1.2668 Acc: 0.4762\n",
      "Epoch 20/20 | Train Loss: 1.1856 Acc: 0.5395 | Val Loss: 1.2604 Acc: 0.4762\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.33      0.02      0.04        46\n",
      "      p_bent       0.46      0.38      0.41        16\n",
      "       p_lie       0.00      0.00      0.00        23\n",
      "       p_sit       0.52      0.75      0.61       106\n",
      "     p_stand       0.42      0.52      0.47        82\n",
      "\n",
      "    accuracy                           0.48       273\n",
      "   macro avg       0.35      0.34      0.31       273\n",
      "weighted avg       0.41      0.48      0.41       273\n",
      "\n",
      "混淆矩阵:\n",
      "[[ 1  2  0 17 26]\n",
      " [ 1  6  0  4  5]\n",
      " [ 0  0  0 16  7]\n",
      " [ 1  4  0 80 21]\n",
      " [ 0  1  0 38 43]]\n",
      "✅ 模型已保存到 data/lstm_baseline_top5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# ====================\n",
    "# 1. 数据加载\n",
    "# ====================\n",
    "X = np.load(\"X_top5.npy\")   # (N, 19, 96)\n",
    "y = np.load(\"y_top5.npy\")   # (N,)\n",
    "\n",
    "print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"类别分布:\", Counter(y))\n",
    "\n",
    "# 标签编码\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"类别数:\", num_classes, \"类别:\", le.classes_)\n",
    "\n",
    "# 划分训练/验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# 转换为 Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# 构建 DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ====================\n",
    "# 2. 定义 LSTM 模型\n",
    "# ====================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)   # (batch, seq_len, hidden)\n",
    "        out = out[:, -1, :]     # 取最后时刻的输出\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# ====================\n",
    "# 3. 训练准备\n",
    "# ====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(input_dim=X.shape[2], hidden_dim=128, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ====================\n",
    "# 4. 训练循环\n",
    "# ====================\n",
    "def train_model(model, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, total = 0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_acc = train_correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss, val_correct, total = 0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        val_acc = val_correct / total\n",
    "        val_loss /= total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/20 | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# ====================\n",
    "# 5. 运行训练\n",
    "# ====================\n",
    "y_true, y_pred = train_model(model, train_loader, val_loader, epochs=20)\n",
    "\n",
    "# ====================\n",
    "# 6. 评估\n",
    "# ====================\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# ====================\n",
    "# 7. 保存模型\n",
    "# ====================\n",
    "torch.save(model.state_dict(), \"data/lstm_baseline_top5.pth\")\n",
    "print(\"✅ 模型已保存到 data/lstm_baseline_top5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3a783b-4a92-401e-9f1f-313219b19ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (1534, 19, 96) y shape: (1534,)\n",
      "过滤后 X shape: (1361, 19, 96) y shape: (1361,)\n",
      "类别分布: Counter({np.str_('p_sit'): 527, np.str_('p_stand'): 412, np.str_('a_walk'): 231, np.str_('p_lie'): 113, np.str_('p_bent'): 78})\n",
      "过采样后: Counter({np.str_('a_walk'): 527, np.str_('p_stand'): 527, np.str_('p_sit'): 527, np.str_('p_lie'): 527, np.str_('p_bent'): 527})\n",
      "Epoch 1/30 | Train Loss: 1.4993 Acc: 0.3401 | Val Loss: 1.4882 Acc: 0.3378\n",
      "Epoch 2/30 | Train Loss: 1.4233 Acc: 0.3904 | Val Loss: 1.4716 Acc: 0.3776\n",
      "Epoch 3/30 | Train Loss: 1.4122 Acc: 0.4056 | Val Loss: 1.4563 Acc: 0.3776\n",
      "Epoch 4/30 | Train Loss: 1.3973 Acc: 0.4028 | Val Loss: 1.4266 Acc: 0.3909\n",
      "Epoch 5/30 | Train Loss: 1.3719 Acc: 0.4151 | Val Loss: 1.4468 Acc: 0.3966\n",
      "Epoch 6/30 | Train Loss: 1.3728 Acc: 0.4250 | Val Loss: 1.4127 Acc: 0.3776\n",
      "Epoch 7/30 | Train Loss: 1.3778 Acc: 0.4127 | Val Loss: 1.4485 Acc: 0.3548\n",
      "Epoch 8/30 | Train Loss: 1.3889 Acc: 0.3843 | Val Loss: 1.3897 Acc: 0.3852\n",
      "Epoch 9/30 | Train Loss: 1.3542 Acc: 0.4241 | Val Loss: 1.3592 Acc: 0.4175\n",
      "Epoch 10/30 | Train Loss: 1.3301 Acc: 0.4412 | Val Loss: 1.3330 Acc: 0.4137\n",
      "Epoch 11/30 | Train Loss: 1.3698 Acc: 0.4137 | Val Loss: 1.4173 Acc: 0.3643\n",
      "Epoch 12/30 | Train Loss: 1.3414 Acc: 0.4241 | Val Loss: 1.3629 Acc: 0.3795\n",
      "Epoch 13/30 | Train Loss: 1.3165 Acc: 0.4469 | Val Loss: 1.3918 Acc: 0.3871\n",
      "Epoch 14/30 | Train Loss: 1.3216 Acc: 0.4383 | Val Loss: 1.3409 Acc: 0.4288\n",
      "Epoch 15/30 | Train Loss: 1.3100 Acc: 0.4516 | Val Loss: 1.3497 Acc: 0.4080\n",
      "Epoch 16/30 | Train Loss: 1.3026 Acc: 0.4521 | Val Loss: 1.3667 Acc: 0.4231\n",
      "Epoch 17/30 | Train Loss: 1.3251 Acc: 0.4497 | Val Loss: 1.3607 Acc: 0.3966\n",
      "Epoch 18/30 | Train Loss: 1.3103 Acc: 0.4435 | Val Loss: 1.3787 Acc: 0.4061\n",
      "Epoch 19/30 | Train Loss: 1.3209 Acc: 0.4336 | Val Loss: 1.3616 Acc: 0.4175\n",
      "Epoch 20/30 | Train Loss: 1.3071 Acc: 0.4440 | Val Loss: 1.3576 Acc: 0.4250\n",
      "Epoch 21/30 | Train Loss: 1.2914 Acc: 0.4473 | Val Loss: 1.3409 Acc: 0.4004\n",
      "Epoch 22/30 | Train Loss: 1.2797 Acc: 0.4559 | Val Loss: 1.3226 Acc: 0.4213\n",
      "Epoch 23/30 | Train Loss: 1.2707 Acc: 0.4635 | Val Loss: 1.2799 Acc: 0.4402\n",
      "Epoch 24/30 | Train Loss: 1.2623 Acc: 0.4587 | Val Loss: 1.2993 Acc: 0.4137\n",
      "Epoch 25/30 | Train Loss: 1.2727 Acc: 0.4507 | Val Loss: 1.3097 Acc: 0.4213\n",
      "Epoch 26/30 | Train Loss: 1.2477 Acc: 0.4730 | Val Loss: 1.2822 Acc: 0.4535\n",
      "Epoch 27/30 | Train Loss: 1.2664 Acc: 0.4744 | Val Loss: 1.3100 Acc: 0.4213\n",
      "Epoch 28/30 | Train Loss: 1.2520 Acc: 0.4692 | Val Loss: 1.2867 Acc: 0.4516\n",
      "Epoch 29/30 | Train Loss: 1.2429 Acc: 0.4848 | Val Loss: 1.2794 Acc: 0.4592\n",
      "Epoch 30/30 | Train Loss: 1.2333 Acc: 0.4791 | Val Loss: 1.2929 Acc: 0.4668\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.65      0.34      0.45       106\n",
      "      p_bent       0.86      0.35      0.50       105\n",
      "       p_lie       0.33      0.95      0.49       105\n",
      "       p_sit       0.84      0.20      0.32       106\n",
      "     p_stand       0.50      0.50      0.50       105\n",
      "\n",
      "    accuracy                           0.47       527\n",
      "   macro avg       0.64      0.47      0.45       527\n",
      "weighted avg       0.64      0.47      0.45       527\n",
      "\n",
      "混淆矩阵:\n",
      "[[ 36   1  56   3  10]\n",
      " [  2  37  41   0  25]\n",
      " [  1   0 100   0   4]\n",
      " [  5   2  66  21  12]\n",
      " [ 11   3  38   1  52]]\n",
      "✅ 模型已保存到 data/lstm_baseline_oversampled.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# =============== 固定随机种子 ===============\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# =============== 加载原始数据 ===============\n",
    "X = np.load(\"data/X_lstm.npy\")   # (N, T, F)\n",
    "y = np.load(\"data/y_lstm.npy\")   # (N,)\n",
    "\n",
    "print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# =============== 保留前 5 个类别 ===============\n",
    "top5_classes = ['p_sit', 'p_stand', 'a_walk', 'p_lie', 'p_bent']\n",
    "mask = np.isin(y, top5_classes)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(\"过滤后 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"类别分布:\", Counter(y))\n",
    "\n",
    "# =============== 过采样函数 ===============\n",
    "def balance_upsample(X, y):\n",
    "    counter = Counter(y)\n",
    "    max_count = max(counter.values())  # 最多类别样本数\n",
    "    indices = []\n",
    "\n",
    "    for label in counter.keys():\n",
    "        label_idx = np.where(y == label)[0]\n",
    "        sampled_idx = np.random.choice(label_idx, max_count, replace=True)\n",
    "        indices.extend(sampled_idx)\n",
    "\n",
    "    indices = np.array(indices)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "X_bal, y_bal = balance_upsample(X, y)\n",
    "print(\"过采样后:\", Counter(y_bal))\n",
    "\n",
    "# =============== 标签编码 ===============\n",
    "le = LabelEncoder()\n",
    "y_bal = le.fit_transform(y_bal)\n",
    "\n",
    "# 转 torch tensor\n",
    "X_tensor = torch.tensor(X_bal, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_bal, dtype=torch.long)\n",
    "\n",
    "# =============== 数据集划分 ===============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, random_state=SEED, stratify=y_tensor\n",
    ")\n",
    "\n",
    "# =============== LSTM 模型定义 ===============\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)   # (batch, seq_len, hidden)\n",
    "        out = out[:, -1, :]     # 取最后时刻\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# =============== 模型初始化 ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(input_dim=X.shape[2], hidden_dim=128, num_layers=2, num_classes=len(le.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# =============== DataLoader ===============\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_data = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# =============== 训练循环 ===============\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_loss /= total\n",
    "\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_loss /= total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "# =============== 评估 ===============\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# =============== 保存模型 ===============\n",
    "torch.save(model.state_dict(), \"data/lstm_baseline_oversampled.pth\")\n",
    "print(\"✅ 模型已保存到 data/lstm_baseline_oversampled.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c73536-7c92-4693-abc1-936c8ec6b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (1534, 19, 96) y shape: (1534,)\n",
      "过滤后 X shape: (1361, 19, 96) y shape: (1361,)\n",
      "类别分布: Counter({np.int64(3): 527, np.int64(4): 412, np.int64(0): 231, np.int64(2): 113, np.int64(1): 78})\n",
      "过采样后: Counter({np.int64(0): 527, np.int64(4): 527, np.int64(3): 527, np.int64(2): 527, np.int64(1): 527})\n",
      "Epoch 1/30 | Train Loss: 0.9401 Acc: 0.3335 | Val Loss: 0.9204 Acc: 0.3321\n",
      "Epoch 2/30 | Train Loss: 0.8667 Acc: 0.3871 | Val Loss: 0.9037 Acc: 0.3510\n",
      "Epoch 3/30 | Train Loss: 0.8531 Acc: 0.4004 | Val Loss: 0.8886 Acc: 0.3890\n",
      "Epoch 4/30 | Train Loss: 0.8369 Acc: 0.4151 | Val Loss: 0.8751 Acc: 0.3757\n",
      "Epoch 5/30 | Train Loss: 0.8317 Acc: 0.4213 | Val Loss: 0.8735 Acc: 0.4099\n",
      "Epoch 6/30 | Train Loss: 0.8375 Acc: 0.4156 | Val Loss: 0.8864 Acc: 0.3757\n",
      "Epoch 7/30 | Train Loss: 0.8241 Acc: 0.4141 | Val Loss: 0.8758 Acc: 0.3947\n",
      "Epoch 8/30 | Train Loss: 0.8150 Acc: 0.4274 | Val Loss: 0.8768 Acc: 0.3985\n",
      "Epoch 9/30 | Train Loss: 0.8118 Acc: 0.4326 | Val Loss: 0.8590 Acc: 0.3890\n",
      "Epoch 10/30 | Train Loss: 0.8034 Acc: 0.4341 | Val Loss: 0.8573 Acc: 0.4156\n",
      "Epoch 11/30 | Train Loss: 0.7938 Acc: 0.4469 | Val Loss: 0.8411 Acc: 0.3833\n",
      "Epoch 12/30 | Train Loss: 0.7876 Acc: 0.4426 | Val Loss: 0.8458 Acc: 0.3985\n",
      "Epoch 13/30 | Train Loss: 0.7864 Acc: 0.4483 | Val Loss: 0.8359 Acc: 0.4194\n",
      "Epoch 14/30 | Train Loss: 0.7932 Acc: 0.4464 | Val Loss: 0.8478 Acc: 0.3833\n",
      "Epoch 15/30 | Train Loss: 0.7927 Acc: 0.4379 | Val Loss: 0.8514 Acc: 0.4042\n",
      "Epoch 16/30 | Train Loss: 0.7970 Acc: 0.4478 | Val Loss: 0.8509 Acc: 0.4099\n",
      "Epoch 17/30 | Train Loss: 0.7888 Acc: 0.4492 | Val Loss: 0.8347 Acc: 0.4231\n",
      "Epoch 18/30 | Train Loss: 0.7960 Acc: 0.4492 | Val Loss: 0.8562 Acc: 0.4250\n",
      "Epoch 19/30 | Train Loss: 0.8033 Acc: 0.4454 | Val Loss: 0.8585 Acc: 0.4099\n",
      "Epoch 20/30 | Train Loss: 0.7948 Acc: 0.4478 | Val Loss: 0.8515 Acc: 0.4080\n",
      "Epoch 21/30 | Train Loss: 0.7877 Acc: 0.4431 | Val Loss: 0.8495 Acc: 0.4099\n",
      "Epoch 22/30 | Train Loss: 0.7815 Acc: 0.4564 | Val Loss: 0.8431 Acc: 0.4080\n",
      "Epoch 23/30 | Train Loss: 0.7759 Acc: 0.4592 | Val Loss: 0.8357 Acc: 0.4326\n",
      "Epoch 24/30 | Train Loss: 0.7780 Acc: 0.4535 | Val Loss: 0.8421 Acc: 0.4099\n",
      "Epoch 25/30 | Train Loss: 0.7747 Acc: 0.4592 | Val Loss: 0.8464 Acc: 0.3947\n",
      "Epoch 26/30 | Train Loss: 0.7700 Acc: 0.4545 | Val Loss: 0.8319 Acc: 0.4156\n",
      "Epoch 27/30 | Train Loss: 0.7586 Acc: 0.4606 | Val Loss: 0.8179 Acc: 0.4326\n",
      "Epoch 28/30 | Train Loss: 0.7520 Acc: 0.4687 | Val Loss: 0.8173 Acc: 0.4326\n",
      "Epoch 29/30 | Train Loss: 0.7493 Acc: 0.4706 | Val Loss: 0.8147 Acc: 0.4326\n",
      "Epoch 30/30 | Train Loss: 0.7470 Acc: 0.4801 | Val Loss: 0.8139 Acc: 0.4307\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.75      0.37      0.49       106\n",
      "      p_bent       0.77      0.29      0.42       105\n",
      "       p_lie       0.30      0.94      0.46       105\n",
      "       p_sit       0.70      0.18      0.29       106\n",
      "     p_stand       0.49      0.38      0.43       105\n",
      "\n",
      "    accuracy                           0.43       527\n",
      "   macro avg       0.60      0.43      0.42       527\n",
      "weighted avg       0.60      0.43      0.42       527\n",
      "\n",
      "混淆矩阵:\n",
      "[[39  6 54  1  6]\n",
      " [ 1 30 54  4 16]\n",
      " [ 0  0 99  1  5]\n",
      " [ 4  2 67 19 14]\n",
      " [ 8  1 54  2 40]]\n",
      "✅ 模型已保存到 data/lstm_bilstm_attention_focal.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# ========== 1. 固定随机种子 ==========\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========== 2. 加载数据 ==========\n",
    "X = np.load(\"data/X_lstm.npy\")   # (N, T, F)\n",
    "y = np.load(\"data/y_lstm.npy\")   # (N,)\n",
    "print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# 保留前5类\n",
    "keep_labels = ['p_sit', 'p_stand', 'a_walk', 'p_lie', 'p_bent']\n",
    "mask = np.isin(y, keep_labels)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# 编码类别\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(\"过滤后 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"类别分布:\", Counter(y))\n",
    "\n",
    "# ========== 3. 过采样 ==========\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "ros = RandomOverSampler(random_state=seed)\n",
    "X_res, y_res = ros.fit_resample(X_flat, y)\n",
    "X_res = X_res.reshape(-1, X.shape[1], X.shape[2])\n",
    "\n",
    "print(\"过采样后:\", Counter(y_res))\n",
    "\n",
    "# ========== 4. 划分训练集/验证集 ==========\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=seed, stratify=y_res\n",
    ")\n",
    "\n",
    "# 转换为 Tensor\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
    "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "# ========== 5. 定义模型 ==========\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1)  # BiLSTM 输出 hidden*2\n",
    "    def forward(self, lstm_out):\n",
    "        weights = torch.softmax(self.attn(lstm_out), dim=1)   # (B, T, 1)\n",
    "        context = torch.sum(weights * lstm_out, dim=1)        # (B, H*2)\n",
    "        return context\n",
    "\n",
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = AttentionLayer(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.attn(out)  # attention pooling\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# ========== 6. 定义 Focal Loss ==========\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n",
    "\n",
    "# ========== 7. 训练 ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM_Attention(input_dim=X.shape[2], hidden_dim=64, num_classes=len(le.classes_)).to(device)\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * Xb.size(0)\n",
    "        correct += (outputs.argmax(1) == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    train_acc = correct / total\n",
    "    train_loss /= total\n",
    "\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            outputs = model(Xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            val_loss += loss.item() * Xb.size(0)\n",
    "            correct += (outputs.argmax(1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "    val_acc = correct / total\n",
    "    val_loss /= total\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ========== 8. 评估 ==========\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "torch.save(model.state_dict(), \"data/lstm_bilstm_attention_focal.pth\")\n",
    "print(\"✅ 模型已保存到 data/lstm_bilstm_attention_focal.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8b0fb4-529f-4e3a-aa7b-058a4adb9a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in d:\\conda\\miniconda3\\envs\\panoptes_pc_hpe\\lib\\site-packages (from imbalanced-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in d:\\conda\\miniconda3\\envs\\panoptes_pc_hpe\\lib\\site-packages (from imbalanced-learn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in d:\\conda\\miniconda3\\envs\\panoptes_pc_hpe\\lib\\site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in d:\\conda\\miniconda3\\envs\\panoptes_pc_hpe\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in d:\\conda\\miniconda3\\envs\\panoptes_pc_hpe\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda978ae-8a30-42f7-a24b-2cc9d6521438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 没找到缓存文件，正在保存...\n",
      "数据 shape: (1361, 19, 96) (1361,) 类别数: 5 类别分布: Counter({np.int64(3): 527, np.int64(4): 412, np.int64(0): 231, np.int64(2): 113, np.int64(1): 78})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ============ 1. 数据准备 ============\n",
    "# 这里替换成你的原始 X, y 数据来源\n",
    "# 假设 X.shape = (1534, 19, 96), y.shape = (1534,)\n",
    "# 如果已经有 X, y，就不用再生成，直接用\n",
    "# X, y = ...  # 你之前的预处理代码\n",
    "\n",
    "# 文件名\n",
    "X_file = \"X_train.npy\"\n",
    "y_file = \"y_train.npy\"\n",
    "\n",
    "# 如果文件不存在，就保存\n",
    "if not os.path.exists(X_file) or not os.path.exists(y_file):\n",
    "    print(\"⚠️ 没找到缓存文件，正在保存...\")\n",
    "    np.save(X_file, X)\n",
    "    np.save(y_file, y)\n",
    "\n",
    "# 加载\n",
    "X = np.load(X_file)\n",
    "y = np.load(y_file)\n",
    "\n",
    "print(\"数据 shape:\", X.shape, y.shape, \n",
    "      \"类别数:\", len(np.unique(y)), \n",
    "      \"类别分布:\", Counter(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb8fa06-5012-44f2-9603-c2813bef1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "原始类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('Unknown'): 17, np.str_('t_bend'): 7, np.str_('p_lie'): 6, np.str_('p_bent'): 6, np.str_('t_straighten'): 4, np.str_('t_bed_turn'): 3, np.str_('t_stand_to_sit'): 1})\n",
      "过滤后 X shape: (112, 100, 96) y shape: (112,)\n",
      "过滤后类别分布: Counter({np.int64(1): 45, np.int64(2): 28, np.int64(0): 27, np.int64(3): 6, np.int64(4): 6})\n",
      "过采样后 X shape: (225, 100, 96) y shape: (225,)\n",
      "过采样后类别分布: Counter({np.int64(2): 45, np.int64(1): 45, np.int64(0): 45, np.int64(3): 45, np.int64(4): 45})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Conda\\miniconda3\\envs\\panoptes_pc_hpe\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.5930 Acc: 0.2444 | Val Loss: 1.5683 Acc: 0.2222\n",
      "Epoch 2/30 | Train Loss: 1.4679 Acc: 0.4056 | Val Loss: 1.4524 Acc: 0.3778\n",
      "Epoch 3/30 | Train Loss: 1.3278 Acc: 0.4944 | Val Loss: 1.4099 Acc: 0.4667\n",
      "Epoch 4/30 | Train Loss: 1.2309 Acc: 0.5556 | Val Loss: 1.4214 Acc: 0.4444\n",
      "Epoch 5/30 | Train Loss: 1.1778 Acc: 0.5389 | Val Loss: 1.3363 Acc: 0.4444\n",
      "Epoch 6/30 | Train Loss: 1.0839 Acc: 0.6111 | Val Loss: 1.2882 Acc: 0.4889\n",
      "Epoch 7/30 | Train Loss: 1.0953 Acc: 0.6000 | Val Loss: 1.2049 Acc: 0.5111\n",
      "Epoch 8/30 | Train Loss: 0.9861 Acc: 0.6667 | Val Loss: 1.1126 Acc: 0.6222\n",
      "Epoch 9/30 | Train Loss: 0.8936 Acc: 0.6722 | Val Loss: 1.0200 Acc: 0.5778\n",
      "Epoch 10/30 | Train Loss: 0.8583 Acc: 0.7333 | Val Loss: 1.0751 Acc: 0.6222\n",
      "Epoch 11/30 | Train Loss: 0.7878 Acc: 0.7111 | Val Loss: 0.9554 Acc: 0.6667\n",
      "Epoch 12/30 | Train Loss: 0.7027 Acc: 0.7500 | Val Loss: 1.0097 Acc: 0.6222\n",
      "Epoch 13/30 | Train Loss: 0.6835 Acc: 0.7500 | Val Loss: 0.9873 Acc: 0.6000\n",
      "Epoch 14/30 | Train Loss: 0.6464 Acc: 0.7722 | Val Loss: 0.9128 Acc: 0.6667\n",
      "Epoch 15/30 | Train Loss: 0.6117 Acc: 0.8000 | Val Loss: 0.8871 Acc: 0.7111\n",
      "Epoch 16/30 | Train Loss: 0.5747 Acc: 0.7889 | Val Loss: 0.8915 Acc: 0.6889\n",
      "Epoch 17/30 | Train Loss: 0.5611 Acc: 0.8222 | Val Loss: 0.8814 Acc: 0.6889\n",
      "Epoch 18/30 | Train Loss: 0.5554 Acc: 0.7889 | Val Loss: 0.8192 Acc: 0.7778\n",
      "Epoch 19/30 | Train Loss: 0.5386 Acc: 0.8444 | Val Loss: 0.8497 Acc: 0.6667\n",
      "Epoch 20/30 | Train Loss: 0.5267 Acc: 0.8111 | Val Loss: 0.8478 Acc: 0.7111\n",
      "Epoch 21/30 | Train Loss: 0.4697 Acc: 0.8556 | Val Loss: 0.7708 Acc: 0.8000\n",
      "Epoch 22/30 | Train Loss: 0.4370 Acc: 0.8667 | Val Loss: 0.7799 Acc: 0.7111\n",
      "Epoch 23/30 | Train Loss: 0.4615 Acc: 0.8389 | Val Loss: 0.7969 Acc: 0.6667\n",
      "Epoch 24/30 | Train Loss: 0.4686 Acc: 0.8722 | Val Loss: 0.7413 Acc: 0.7778\n",
      "Epoch 25/30 | Train Loss: 0.4568 Acc: 0.8611 | Val Loss: 0.7652 Acc: 0.7111\n",
      "Epoch 26/30 | Train Loss: 0.4388 Acc: 0.8833 | Val Loss: 0.7586 Acc: 0.7111\n",
      "Epoch 27/30 | Train Loss: 0.4360 Acc: 0.8667 | Val Loss: 0.7588 Acc: 0.7111\n",
      "Epoch 28/30 | Train Loss: 0.4053 Acc: 0.8722 | Val Loss: 0.7683 Acc: 0.6667\n",
      "Epoch 29/30 | Train Loss: 0.4389 Acc: 0.8333 | Val Loss: 0.7532 Acc: 0.6889\n",
      "⏹ Early stopping triggered\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       p_sit       0.80      0.44      0.57         9\n",
      "     p_stand       0.33      0.56      0.42         9\n",
      "      a_walk       0.71      0.56      0.62         9\n",
      "       p_lie       0.90      1.00      0.95         9\n",
      "      p_bent       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.69        45\n",
      "   macro avg       0.75      0.69      0.70        45\n",
      "weighted avg       0.75      0.69      0.70        45\n",
      "\n",
      "混淆矩阵:\n",
      "[[4 5 0 0 0]\n",
      " [1 5 2 1 0]\n",
      " [0 4 5 0 0]\n",
      " [0 0 0 9 0]\n",
      " [0 1 0 0 8]]\n",
      "✅ 模型已保存到 data/transformer_top5.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ============ 1. 数据准备 ============\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 假设数据在 data 文件夹\n",
    "X = np.load(\"data/X.npy\")   # (N, T, F) 或 (N, 100, 32, 3)\n",
    "y = np.load(\"data/y.npy\")   # (N,)\n",
    "\n",
    "print(\"原始 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# 如果 X 是 4D (N, T, J, C)，展开为 (N, T, J*C)\n",
    "if X.ndim == 4:\n",
    "    N, T, J, C = X.shape\n",
    "    X = X.reshape(N, T, J*C)\n",
    "\n",
    "# 只保留前 5 个类别\n",
    "top5 = ['p_sit', 'p_stand', 'a_walk', 'p_lie', 'p_bent']\n",
    "label_map = {cls: i for i, cls in enumerate(top5)}\n",
    "\n",
    "mask = np.isin(y, top5)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# 转换为数字编码\n",
    "y = np.array([label_map[label] for label in y])\n",
    "\n",
    "print(\"过滤后 X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"过滤后类别分布:\", Counter(y))\n",
    "\n",
    "# ============ 2. 过采样 ============\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "ros = RandomOverSampler(random_state=seed)\n",
    "X_res, y_res = ros.fit_resample(X_flat, y)\n",
    "X_res = X_res.reshape(X_res.shape[0], X.shape[1], X.shape[2])\n",
    "\n",
    "print(\"过采样后 X shape:\", X_res.shape, \"y shape:\", y_res.shape)\n",
    "print(\"过采样后类别分布:\", Counter(y_res))\n",
    "\n",
    "# ============ 3. 数据划分 ============\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=seed, stratify=y_res\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# ============ 4. Transformer 模型 ============\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads=4, num_layers=2, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F) -> (T, B, F)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        out = self.transformer(x)  # (T, B, F)\n",
    "        out = out.mean(dim=0)      # 池化 (B, F)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = TransformerClassifier(input_dim=X.shape[2], num_classes=len(top5))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ============ 5. 训练配置 ============\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "\n",
    "# EarlyStopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "# ============ 6. 训练循环 ============\n",
    "for epoch in range(30):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * yb.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_loss /= total\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            outputs = model(Xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            val_loss += loss.item() * yb.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_loss /= total\n",
    "    print(f\"Epoch {epoch+1}/30 | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"⏹ Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# ============ 7. 评估 ============\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=top5, zero_division=0))\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "torch.save(model.state_dict(), \"data/transformer_top5.pth\")\n",
    "print(\"✅ 模型已保存到 data/transformer_top5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb58b71-91c8-425d-a8ab-b41f02896a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "原始类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('Unknown'): 17, np.str_('t_bend'): 7, np.str_('p_lie'): 6, np.str_('p_bent'): 6, np.str_('t_straighten'): 4, np.str_('t_bed_turn'): 3, np.str_('t_stand_to_sit'): 1})\n",
      "检测并移除 0 个坏样本（剩余 144 个），日志已保存到 data/bad_samples.txt\n",
      "过滤后 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "过滤后 X shape: (124, 100, 32, 3) y shape: (124,)\n",
      "保留类别: [np.str_('p_stand'), np.str_('a_walk'), np.str_('p_sit'), np.str_('Unknown'), np.str_('t_bend')]\n",
      "类别数: 5 类别映射: {np.str_('Unknown'): 0, np.str_('a_walk'): 1, np.str_('p_sit'): 2, np.str_('p_stand'): 3, np.str_('t_bend'): 4}\n",
      "过采样后 X shape: (225, 100, 96) y shape: (225,)\n",
      "过采样后类别分布: Counter({np.int64(1): 45, np.int64(3): 45, np.int64(2): 45, np.int64(0): 45, np.int64(4): 45})\n",
      "Train: torch.Size([180, 100, 96]) Val: torch.Size([45, 100, 96])\n",
      "Epoch 01/40 | Train Loss: 1.7452 Acc: 0.2278 | Val Loss: 1.6312 Acc: 0.2889 | LR: [0.001]\n",
      "Epoch 02/40 | Train Loss: 1.5895 Acc: 0.2944 | Val Loss: 1.5302 Acc: 0.3333 | LR: [0.001]\n",
      "Epoch 03/40 | Train Loss: 1.4668 Acc: 0.3889 | Val Loss: 1.4354 Acc: 0.4444 | LR: [0.001]\n",
      "Epoch 04/40 | Train Loss: 1.4158 Acc: 0.4000 | Val Loss: 1.4127 Acc: 0.4000 | LR: [0.001]\n",
      "Epoch 05/40 | Train Loss: 1.4025 Acc: 0.4111 | Val Loss: 1.3474 Acc: 0.4222 | LR: [0.001]\n",
      "Epoch 06/40 | Train Loss: 1.3134 Acc: 0.4889 | Val Loss: 1.2383 Acc: 0.5111 | LR: [0.001]\n",
      "Epoch 07/40 | Train Loss: 1.2502 Acc: 0.5000 | Val Loss: 1.1839 Acc: 0.5333 | LR: [0.001]\n",
      "Epoch 08/40 | Train Loss: 1.2566 Acc: 0.4944 | Val Loss: 1.2603 Acc: 0.4667 | LR: [0.001]\n",
      "Epoch 09/40 | Train Loss: 1.1957 Acc: 0.5222 | Val Loss: 1.2739 Acc: 0.4000 | LR: [0.001]\n",
      "Epoch 10/40 | Train Loss: 1.1839 Acc: 0.5000 | Val Loss: 1.1920 Acc: 0.4667 | LR: [0.0005]\n",
      "Epoch 11/40 | Train Loss: 1.1109 Acc: 0.5556 | Val Loss: 1.0594 Acc: 0.5778 | LR: [0.0005]\n",
      "Epoch 12/40 | Train Loss: 0.9995 Acc: 0.5556 | Val Loss: 0.9497 Acc: 0.6444 | LR: [0.0005]\n",
      "Epoch 13/40 | Train Loss: 0.9937 Acc: 0.5833 | Val Loss: 0.8898 Acc: 0.6222 | LR: [0.0005]\n",
      "Epoch 14/40 | Train Loss: 0.9699 Acc: 0.5667 | Val Loss: 0.7956 Acc: 0.7333 | LR: [0.0005]\n",
      "Epoch 15/40 | Train Loss: 0.9017 Acc: 0.5833 | Val Loss: 0.9035 Acc: 0.4889 | LR: [0.0005]\n",
      "Epoch 16/40 | Train Loss: 0.9317 Acc: 0.5889 | Val Loss: 0.8669 Acc: 0.6667 | LR: [0.0005]\n",
      "Epoch 17/40 | Train Loss: 0.7957 Acc: 0.7000 | Val Loss: 0.7624 Acc: 0.7111 | LR: [0.0005]\n",
      "Epoch 18/40 | Train Loss: 0.7550 Acc: 0.6944 | Val Loss: 0.8417 Acc: 0.6222 | LR: [0.0005]\n",
      "Epoch 19/40 | Train Loss: 0.6945 Acc: 0.7278 | Val Loss: 0.7343 Acc: 0.7111 | LR: [0.0005]\n",
      "Epoch 20/40 | Train Loss: 0.7219 Acc: 0.6778 | Val Loss: 0.7374 Acc: 0.7333 | LR: [0.00025]\n",
      "Epoch 21/40 | Train Loss: 0.6644 Acc: 0.7722 | Val Loss: 0.8060 Acc: 0.7111 | LR: [0.00025]\n",
      "Epoch 22/40 | Train Loss: 0.6271 Acc: 0.7333 | Val Loss: 0.7187 Acc: 0.7556 | LR: [0.00025]\n",
      "Epoch 23/40 | Train Loss: 0.6002 Acc: 0.7278 | Val Loss: 0.7019 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 24/40 | Train Loss: 0.5742 Acc: 0.7833 | Val Loss: 0.6497 Acc: 0.7778 | LR: [0.00025]\n",
      "Epoch 25/40 | Train Loss: 0.5912 Acc: 0.7556 | Val Loss: 0.6813 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 26/40 | Train Loss: 0.5645 Acc: 0.7778 | Val Loss: 0.7059 Acc: 0.7778 | LR: [0.00025]\n",
      "Epoch 27/40 | Train Loss: 0.5670 Acc: 0.7778 | Val Loss: 0.6612 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 28/40 | Train Loss: 0.5569 Acc: 0.7778 | Val Loss: 0.6737 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 29/40 | Train Loss: 0.5025 Acc: 0.8111 | Val Loss: 0.6482 Acc: 0.7556 | LR: [0.00025]\n",
      "Epoch 30/40 | Train Loss: 0.5213 Acc: 0.7833 | Val Loss: 0.6780 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 31/40 | Train Loss: 0.4624 Acc: 0.8167 | Val Loss: 0.6744 Acc: 0.7556 | LR: [0.000125]\n",
      "Epoch 32/40 | Train Loss: 0.4341 Acc: 0.8444 | Val Loss: 0.6202 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 33/40 | Train Loss: 0.4372 Acc: 0.8111 | Val Loss: 0.6535 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 34/40 | Train Loss: 0.4075 Acc: 0.8333 | Val Loss: 0.6473 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 35/40 | Train Loss: 0.3968 Acc: 0.8444 | Val Loss: 0.6476 Acc: 0.8444 | LR: [0.000125]\n",
      "Epoch 36/40 | Train Loss: 0.3935 Acc: 0.8500 | Val Loss: 0.6601 Acc: 0.8667 | LR: [0.000125]\n",
      "Epoch 37/40 | Train Loss: 0.3734 Acc: 0.8833 | Val Loss: 0.6611 Acc: 0.8444 | LR: [0.000125]\n",
      "Epoch 38/40 | Train Loss: 0.3887 Acc: 0.8667 | Val Loss: 0.6314 Acc: 0.8444 | LR: [0.000125]\n",
      "Epoch 39/40 | Train Loss: 0.3804 Acc: 0.8556 | Val Loss: 0.6462 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 40/40 | Train Loss: 0.3710 Acc: 0.8611 | Val Loss: 0.6483 Acc: 0.8222 | LR: [6.25e-05]\n",
      "\n",
      "✅ 最佳验证准确率: 0.8667\n",
      "模型已保存到 data/transformer_top5_filtered.pth\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       0.75      1.00      0.86         9\n",
      "      a_walk       0.69      1.00      0.82         9\n",
      "       p_sit       0.88      0.78      0.82         9\n",
      "     p_stand       1.00      0.33      0.50         9\n",
      "      t_bend       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.82        45\n",
      "   macro avg       0.86      0.82      0.80        45\n",
      "weighted avg       0.86      0.82      0.80        45\n",
      "\n",
      "混淆矩阵:\n",
      "[[9 0 0 0 0]\n",
      " [0 9 0 0 0]\n",
      " [1 1 7 0 0]\n",
      " [2 3 1 3 0]\n",
      " [0 0 0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ===========================================================\n",
    "# Step 1. 自动检测并加载数据\n",
    "# ===========================================================\n",
    "if os.path.exists(\"data/pose_data.npy\") and os.path.exists(\"data/pose_labels.npy\"):\n",
    "    data_path, label_path = \"data/pose_data.npy\", \"data/pose_labels.npy\"\n",
    "elif os.path.exists(\"data/X.npy\") and os.path.exists(\"data/y.npy\"):\n",
    "    data_path, label_path = \"data/X.npy\", \"data/y.npy\"\n",
    "else:\n",
    "    print(\"⚠️ 没找到数据文件！请确认 data/ 下存在 pose_data.npy 或 X.npy\")\n",
    "    print(\"当前目录内容：\", os.listdir(\"data\"))\n",
    "    sys.exit(1)\n",
    "\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "labels = np.load(label_path, allow_pickle=True)\n",
    "X = np.array(data, dtype=np.float32)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"原始 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 2. 改进版坏样本过滤函数（带日志输出）\n",
    "# ===========================================================\n",
    "def filter_invalid_samples(X, y, max_abs=10000.0, min_std=1e-6, log_path=\"data/bad_samples.txt\"):\n",
    "    valid_idx, bad_info = [], []\n",
    "    for i in range(len(X)):\n",
    "        xi = X[i]\n",
    "        reason = None\n",
    "        if np.isnan(xi).any() or np.isinf(xi).any():\n",
    "            reason = \"NaN/Inf\"\n",
    "        elif np.allclose(xi, 0):\n",
    "            reason = \"All zeros\"\n",
    "        elif np.std(xi) < min_std:\n",
    "            reason = f\"Low std ({np.std(xi):.2e})\"\n",
    "        elif np.any(np.abs(xi) > max_abs):\n",
    "            reason = f\"Out of range (max={np.max(np.abs(xi)):.1f})\"\n",
    "\n",
    "        if reason:\n",
    "            bad_info.append(f\"Sample {i}: {reason}\")\n",
    "        else:\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if bad_info:\n",
    "            f.write(\"\\n\".join(bad_info))\n",
    "        else:\n",
    "            f.write(\"✅ 无坏样本\\n\")\n",
    "\n",
    "    print(f\"检测并移除 {len(bad_info)} 个坏样本（剩余 {len(valid_idx)} 个），日志已保存到 {log_path}\")\n",
    "    return X[valid_idx], y[valid_idx]\n",
    "\n",
    "X, y = filter_invalid_samples(X, y)\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 3. 只保留前5类\n",
    "# ===========================================================\n",
    "top5 = [cls for cls, _ in Counter(y).most_common(5)]\n",
    "mask = np.isin(y, top5)\n",
    "X, y = X[mask], y[mask]\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"保留类别:\", top5)\n",
    "\n",
    "# 标签编码（防止类别是字符串）\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(f\"类别数: {len(le.classes_)} 类别映射:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 4. 展平 + 过采样\n",
    "# ===========================================================\n",
    "if X.ndim == 4:\n",
    "    N, T, J, C = X.shape\n",
    "    X = X.reshape(N, T, J * C)\n",
    "elif X.ndim == 3:\n",
    "    N, T, F = X.shape\n",
    "    J, C = F, 1\n",
    "else:\n",
    "    raise ValueError(f\"❌ 输入维度异常: {X.shape}\")\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X.reshape(N, -1), y)\n",
    "X_res = X_res.reshape(-1, T, J * C)\n",
    "print(f\"过采样后 X shape: {X_res.shape} y shape: {y_res.shape}\")\n",
    "print(\"过采样后类别分布:\", Counter(y_res))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 5. 数据划分与Tensor化\n",
    "# ===========================================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n",
    "X_train, X_val = map(torch.tensor, (X_train, X_val))\n",
    "y_train, y_val = map(torch.tensor, (y_train, y_val))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train: {X_train.shape} Val: {X_val.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 6. Transformer 模型定义\n",
    "# ===========================================================\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 7. 训练配置\n",
    "# ===========================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim=X_train.shape[2], num_classes=len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 8. 训练与验证\n",
    "# ===========================================================\n",
    "num_epochs = 40\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        train_correct += (out.argmax(1) == yb).sum().item()\n",
    "\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | Train Loss: {train_loss/len(train_loader.dataset):.4f} \"\n",
    "          f\"Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader.dataset):.4f} \"\n",
    "          f\"Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"data/transformer_top5_filtered.pth\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 9. 输出报告\n",
    "# ===========================================================\n",
    "print(f\"\\n✅ 最佳验证准确率: {best_val_acc:.4f}\")\n",
    "print(\"模型已保存到 data/transformer_top5_filtered.pth\")\n",
    "\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad2e7e4-6a78-459b-8926-062ef54a6293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "原始类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('Unknown'): 17, np.str_('t_bend'): 7, np.str_('p_lie'): 6, np.str_('p_bent'): 6, np.str_('t_straighten'): 4, np.str_('t_bed_turn'): 3, np.str_('t_stand_to_sit'): 1})\n",
      "检测并移除 0 个坏样本（剩余 144 个），日志已保存到 data/bad_samples.txt\n",
      "过滤后 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "保留类别: ['p_sit', 'p_stand', 'a_walk', 'p_lie', 't_bend']\n",
      "过滤后 X shape: (113, 100, 32, 3) y shape: (113,)\n",
      "过滤后类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('t_bend'): 7, np.str_('p_lie'): 6})\n",
      "类别映射: {np.str_('a_walk'): 0, np.str_('p_lie'): 1, np.str_('p_sit'): 2, np.str_('p_stand'): 3, np.str_('t_bend'): 4}\n",
      "过采样后 X shape: (225, 100, 96) y shape: (225,)\n",
      "过采样后类别分布: Counter({np.int64(0): 45, np.int64(3): 45, np.int64(2): 45, np.int64(1): 45, np.int64(4): 45})\n",
      "Train: torch.Size([180, 100, 96]) Val: torch.Size([45, 100, 96])\n",
      "Epoch 01/40 | Train Loss: 1.6863 Acc: 0.2611 | Val Loss: 1.4558 Acc: 0.3778 | LR: [0.001]\n",
      "Epoch 02/40 | Train Loss: 1.5968 Acc: 0.2944 | Val Loss: 1.4228 Acc: 0.3556 | LR: [0.001]\n",
      "Epoch 03/40 | Train Loss: 1.4574 Acc: 0.3833 | Val Loss: 1.3342 Acc: 0.4889 | LR: [0.001]\n",
      "Epoch 04/40 | Train Loss: 1.4021 Acc: 0.3944 | Val Loss: 1.2895 Acc: 0.4000 | LR: [0.001]\n",
      "Epoch 05/40 | Train Loss: 1.2649 Acc: 0.4500 | Val Loss: 1.2624 Acc: 0.4444 | LR: [0.001]\n",
      "Epoch 06/40 | Train Loss: 1.2754 Acc: 0.4556 | Val Loss: 1.2829 Acc: 0.6000 | LR: [0.001]\n",
      "Epoch 07/40 | Train Loss: 1.2313 Acc: 0.4944 | Val Loss: 1.2503 Acc: 0.4667 | LR: [0.001]\n",
      "Epoch 08/40 | Train Loss: 1.1330 Acc: 0.5667 | Val Loss: 1.0896 Acc: 0.5111 | LR: [0.001]\n",
      "Epoch 09/40 | Train Loss: 1.0550 Acc: 0.6000 | Val Loss: 1.0670 Acc: 0.6444 | LR: [0.001]\n",
      "Epoch 10/40 | Train Loss: 1.0202 Acc: 0.6278 | Val Loss: 0.9528 Acc: 0.6889 | LR: [0.0005]\n",
      "Epoch 11/40 | Train Loss: 0.8621 Acc: 0.6722 | Val Loss: 0.9728 Acc: 0.6000 | LR: [0.0005]\n",
      "Epoch 12/40 | Train Loss: 0.7827 Acc: 0.7389 | Val Loss: 0.9007 Acc: 0.6889 | LR: [0.0005]\n",
      "Epoch 13/40 | Train Loss: 0.7724 Acc: 0.7333 | Val Loss: 0.8618 Acc: 0.7111 | LR: [0.0005]\n",
      "Epoch 14/40 | Train Loss: 0.7212 Acc: 0.7222 | Val Loss: 0.9736 Acc: 0.6222 | LR: [0.0005]\n",
      "Epoch 15/40 | Train Loss: 0.6538 Acc: 0.7722 | Val Loss: 0.9526 Acc: 0.6222 | LR: [0.0005]\n",
      "Epoch 16/40 | Train Loss: 0.6659 Acc: 0.7500 | Val Loss: 0.8361 Acc: 0.6889 | LR: [0.0005]\n",
      "Epoch 17/40 | Train Loss: 0.6240 Acc: 0.7722 | Val Loss: 0.7259 Acc: 0.7333 | LR: [0.0005]\n",
      "Epoch 18/40 | Train Loss: 0.6159 Acc: 0.7722 | Val Loss: 0.8547 Acc: 0.6444 | LR: [0.0005]\n",
      "Epoch 19/40 | Train Loss: 0.6867 Acc: 0.7444 | Val Loss: 0.9203 Acc: 0.6444 | LR: [0.0005]\n",
      "Epoch 20/40 | Train Loss: 0.6608 Acc: 0.7611 | Val Loss: 0.7636 Acc: 0.7111 | LR: [0.00025]\n",
      "Epoch 21/40 | Train Loss: 0.5371 Acc: 0.8222 | Val Loss: 0.7954 Acc: 0.7111 | LR: [0.00025]\n",
      "Epoch 22/40 | Train Loss: 0.5248 Acc: 0.8333 | Val Loss: 0.7402 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 23/40 | Train Loss: 0.5094 Acc: 0.8222 | Val Loss: 0.7932 Acc: 0.7333 | LR: [0.00025]\n",
      "Epoch 24/40 | Train Loss: 0.5239 Acc: 0.7889 | Val Loss: 0.7985 Acc: 0.7111 | LR: [0.00025]\n",
      "Epoch 25/40 | Train Loss: 0.4992 Acc: 0.8333 | Val Loss: 0.6622 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 26/40 | Train Loss: 0.4684 Acc: 0.8278 | Val Loss: 0.7113 Acc: 0.7111 | LR: [0.00025]\n",
      "Epoch 27/40 | Train Loss: 0.4696 Acc: 0.8333 | Val Loss: 0.6164 Acc: 0.8444 | LR: [0.00025]\n",
      "Epoch 28/40 | Train Loss: 0.4319 Acc: 0.8278 | Val Loss: 0.5561 Acc: 0.8444 | LR: [0.00025]\n",
      "Epoch 29/40 | Train Loss: 0.4544 Acc: 0.8389 | Val Loss: 0.6765 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 30/40 | Train Loss: 0.4100 Acc: 0.8667 | Val Loss: 0.7173 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 31/40 | Train Loss: 0.3865 Acc: 0.8444 | Val Loss: 0.7966 Acc: 0.7333 | LR: [0.000125]\n",
      "Epoch 32/40 | Train Loss: 0.3781 Acc: 0.8722 | Val Loss: 0.7096 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 33/40 | Train Loss: 0.4147 Acc: 0.8222 | Val Loss: 0.7086 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 34/40 | Train Loss: 0.3996 Acc: 0.8222 | Val Loss: 0.8137 Acc: 0.7333 | LR: [0.000125]\n",
      "Epoch 35/40 | Train Loss: 0.3978 Acc: 0.8667 | Val Loss: 0.7382 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 36/40 | Train Loss: 0.3979 Acc: 0.8500 | Val Loss: 0.7558 Acc: 0.8222 | LR: [0.000125]\n",
      "Epoch 37/40 | Train Loss: 0.3486 Acc: 0.8778 | Val Loss: 0.7305 Acc: 0.8000 | LR: [0.000125]\n",
      "Epoch 38/40 | Train Loss: 0.3123 Acc: 0.8889 | Val Loss: 0.7229 Acc: 0.7778 | LR: [0.000125]\n",
      "Epoch 39/40 | Train Loss: 0.3345 Acc: 0.8611 | Val Loss: 0.7531 Acc: 0.7778 | LR: [0.000125]\n",
      "Epoch 40/40 | Train Loss: 0.3132 Acc: 0.9000 | Val Loss: 0.7036 Acc: 0.8000 | LR: [6.25e-05]\n",
      "\n",
      "✅ 最佳验证准确率: 0.8444\n",
      "模型已保存到 data/transformer_pose5.pth\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.70      0.78      0.74         9\n",
      "       p_lie       0.82      1.00      0.90         9\n",
      "       p_sit       0.88      0.78      0.82         9\n",
      "     p_stand       0.62      0.56      0.59         9\n",
      "      t_bend       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.80        45\n",
      "   macro avg       0.80      0.80      0.80        45\n",
      "weighted avg       0.80      0.80      0.80        45\n",
      "\n",
      "混淆矩阵:\n",
      "[[7 1 0 1 0]\n",
      " [0 9 0 0 0]\n",
      " [0 0 7 2 0]\n",
      " [2 1 1 5 0]\n",
      " [1 0 0 0 8]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ===========================================================\n",
    "# Step 1. 自动检测并加载数据\n",
    "# ===========================================================\n",
    "if os.path.exists(\"data/pose_data.npy\") and os.path.exists(\"data/pose_labels.npy\"):\n",
    "    data_path, label_path = \"data/pose_data.npy\", \"data/pose_labels.npy\"\n",
    "elif os.path.exists(\"data/X.npy\") and os.path.exists(\"data/y.npy\"):\n",
    "    data_path, label_path = \"data/X.npy\", \"data/y.npy\"\n",
    "else:\n",
    "    print(\"⚠️ 没找到数据文件！请确认 data/ 下存在 pose_data.npy 或 X.npy\")\n",
    "    print(\"当前目录内容：\", os.listdir(\"data\"))\n",
    "    sys.exit(1)\n",
    "\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "labels = np.load(label_path, allow_pickle=True)\n",
    "X = np.array(data, dtype=np.float32)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"原始 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 2. 改进版坏样本过滤（带日志）\n",
    "# ===========================================================\n",
    "def filter_invalid_samples(X, y, max_abs=10000.0, min_std=1e-6, log_path=\"data/bad_samples.txt\"):\n",
    "    valid_idx, bad_info = [], []\n",
    "    for i in range(len(X)):\n",
    "        xi = X[i]\n",
    "        reason = None\n",
    "        if np.isnan(xi).any() or np.isinf(xi).any():\n",
    "            reason = \"NaN/Inf\"\n",
    "        elif np.allclose(xi, 0):\n",
    "            reason = \"All zeros\"\n",
    "        elif np.std(xi) < min_std:\n",
    "            reason = f\"Low std ({np.std(xi):.2e})\"\n",
    "        elif np.any(np.abs(xi) > max_abs):\n",
    "            reason = f\"Out of range (max={np.max(np.abs(xi)):.1f})\"\n",
    "\n",
    "        if reason:\n",
    "            bad_info.append(f\"Sample {i}: {reason}\")\n",
    "        else:\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if bad_info:\n",
    "            f.write(\"\\n\".join(bad_info))\n",
    "        else:\n",
    "            f.write(\"✅ 无坏样本\\n\")\n",
    "\n",
    "    print(f\"检测并移除 {len(bad_info)} 个坏样本（剩余 {len(valid_idx)} 个），日志已保存到 {log_path}\")\n",
    "    return X[valid_idx], y[valid_idx]\n",
    "\n",
    "X, y = filter_invalid_samples(X, y)\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 3. 保留指定的五个类别\n",
    "# ===========================================================\n",
    "target_classes = ['p_sit', 'p_stand', 'a_walk', 'p_lie', 't_bend']\n",
    "mask = np.isin(y, target_classes)\n",
    "X, y = X[mask], y[mask]\n",
    "print(f\"保留类别: {target_classes}\")\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"过滤后类别分布:\", Counter(y))\n",
    "\n",
    "# 标签编码\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(f\"类别映射: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 4. 展平 + 过采样\n",
    "# ===========================================================\n",
    "if X.ndim == 4:\n",
    "    N, T, J, C = X.shape\n",
    "    X = X.reshape(N, T, J * C)\n",
    "elif X.ndim == 3:\n",
    "    N, T, F = X.shape\n",
    "    J, C = F, 1\n",
    "else:\n",
    "    raise ValueError(f\"❌ 输入维度异常: {X.shape}\")\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X.reshape(N, -1), y)\n",
    "X_res = X_res.reshape(-1, T, J * C)\n",
    "print(f\"过采样后 X shape: {X_res.shape} y shape: {y_res.shape}\")\n",
    "print(\"过采样后类别分布:\", Counter(y_res))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 5. 数据划分\n",
    "# ===========================================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n",
    "X_train, X_val = map(torch.tensor, (X_train, X_val))\n",
    "y_train, y_val = map(torch.tensor, (y_train, y_val))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train: {X_train.shape} Val: {X_val.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 6. Transformer 模型\n",
    "# ===========================================================\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 7. 训练配置\n",
    "# ===========================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim=X_train.shape[2], num_classes=len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 8. 训练与验证\n",
    "# ===========================================================\n",
    "num_epochs = 40\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        train_correct += (out.argmax(1) == yb).sum().item()\n",
    "\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | Train Loss: {train_loss/len(train_loader.dataset):.4f} \"\n",
    "          f\"Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader.dataset):.4f} \"\n",
    "          f\"Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"data/transformer_pose5.pth\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 9. 输出报告\n",
    "# ===========================================================\n",
    "print(f\"\\n✅ 最佳验证准确率: {best_val_acc:.4f}\")\n",
    "print(\"模型已保存到 data/transformer_pose5.pth\")\n",
    "\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38858a37-984e-41b7-b3bf-6547e404c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "原始类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('Unknown'): 17, np.str_('t_bend'): 7, np.str_('p_lie'): 6, np.str_('p_bent'): 6, np.str_('t_straighten'): 4, np.str_('t_bed_turn'): 3, np.str_('t_stand_to_sit'): 1})\n",
      "检测并移除 0 个坏样本（剩余 144 个），日志已保存到 data/bad_samples.txt\n",
      "过滤后 X shape: (113, 100, 32, 3) y shape: (113,)\n",
      "类别数: 5\n",
      "特征展开后: (113, 100, 96)\n",
      "类别映射: {np.str_('a_walk'): 0, np.str_('p_lie'): 1, np.str_('p_sit'): 2, np.str_('p_stand'): 3, np.str_('t_bend'): 4}\n",
      "过采样后类别分布: Counter({np.int64(0): 45, np.int64(1): 45, np.int64(2): 45, np.int64(3): 45, np.int64(4): 45})\n",
      "Train: (135, 100, 96) Val: (45, 100, 96) Test: (45, 100, 96)\n",
      "Epoch 01/40 | Train Loss: 1.7134 Acc: 0.1778 | Val Loss: 1.5140 Acc: 0.5111 | LR: [0.001]\n",
      "Epoch 02/40 | Train Loss: 1.5441 Acc: 0.3037 | Val Loss: 1.3954 Acc: 0.4222 | LR: [0.001]\n",
      "Epoch 03/40 | Train Loss: 1.4574 Acc: 0.3852 | Val Loss: 1.2542 Acc: 0.4667 | LR: [0.001]\n",
      "Epoch 04/40 | Train Loss: 1.3729 Acc: 0.4000 | Val Loss: 1.2557 Acc: 0.3778 | LR: [0.001]\n",
      "Epoch 05/40 | Train Loss: 1.3931 Acc: 0.3704 | Val Loss: 1.1958 Acc: 0.7111 | LR: [0.001]\n",
      "Epoch 06/40 | Train Loss: 1.3098 Acc: 0.4370 | Val Loss: 1.2354 Acc: 0.4444 | LR: [0.001]\n",
      "Epoch 07/40 | Train Loss: 1.2922 Acc: 0.5185 | Val Loss: 1.0717 Acc: 0.5778 | LR: [0.001]\n",
      "Epoch 08/40 | Train Loss: 1.2185 Acc: 0.5259 | Val Loss: 1.0371 Acc: 0.6667 | LR: [0.001]\n",
      "Epoch 09/40 | Train Loss: 1.1512 Acc: 0.5333 | Val Loss: 1.0799 Acc: 0.6000 | LR: [0.001]\n",
      "Epoch 10/40 | Train Loss: 1.2928 Acc: 0.4444 | Val Loss: 0.9121 Acc: 0.6444 | LR: [0.0005]\n",
      "Epoch 11/40 | Train Loss: 1.1273 Acc: 0.6148 | Val Loss: 0.9332 Acc: 0.7111 | LR: [0.0005]\n",
      "Epoch 12/40 | Train Loss: 1.0013 Acc: 0.6148 | Val Loss: 0.9182 Acc: 0.7333 | LR: [0.0005]\n",
      "Epoch 13/40 | Train Loss: 1.0045 Acc: 0.6370 | Val Loss: 0.8415 Acc: 0.6889 | LR: [0.0005]\n",
      "Epoch 14/40 | Train Loss: 0.9536 Acc: 0.6815 | Val Loss: 0.8114 Acc: 0.7333 | LR: [0.0005]\n",
      "Epoch 15/40 | Train Loss: 0.9349 Acc: 0.6148 | Val Loss: 0.7889 Acc: 0.7556 | LR: [0.0005]\n",
      "Epoch 16/40 | Train Loss: 0.8826 Acc: 0.6889 | Val Loss: 0.7280 Acc: 0.7556 | LR: [0.0005]\n",
      "Epoch 17/40 | Train Loss: 0.9420 Acc: 0.6222 | Val Loss: 0.7114 Acc: 0.7556 | LR: [0.0005]\n",
      "Epoch 18/40 | Train Loss: 0.8326 Acc: 0.6889 | Val Loss: 0.7152 Acc: 0.7778 | LR: [0.0005]\n",
      "Epoch 19/40 | Train Loss: 0.7973 Acc: 0.6963 | Val Loss: 0.7063 Acc: 0.7778 | LR: [0.0005]\n",
      "Epoch 20/40 | Train Loss: 0.7717 Acc: 0.6963 | Val Loss: 0.6650 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 21/40 | Train Loss: 0.7258 Acc: 0.7407 | Val Loss: 0.6880 Acc: 0.7778 | LR: [0.00025]\n",
      "Epoch 22/40 | Train Loss: 0.7131 Acc: 0.7333 | Val Loss: 0.6784 Acc: 0.7778 | LR: [0.00025]\n",
      "Epoch 23/40 | Train Loss: 0.7015 Acc: 0.7407 | Val Loss: 0.6904 Acc: 0.7333 | LR: [0.00025]\n",
      "Epoch 24/40 | Train Loss: 0.7728 Acc: 0.7185 | Val Loss: 0.6555 Acc: 0.7556 | LR: [0.00025]\n",
      "Epoch 25/40 | Train Loss: 0.7001 Acc: 0.7556 | Val Loss: 0.6760 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 26/40 | Train Loss: 0.6965 Acc: 0.7556 | Val Loss: 0.6421 Acc: 0.8000 | LR: [0.00025]\n",
      "Epoch 27/40 | Train Loss: 0.6725 Acc: 0.7778 | Val Loss: 0.6603 Acc: 0.7778 | LR: [0.00025]\n",
      "Epoch 28/40 | Train Loss: 0.6647 Acc: 0.7704 | Val Loss: 0.6680 Acc: 0.8000 | LR: [0.00025]\n",
      "⚠️ 提前停止：验证集不再提升。\n",
      "✅ 最佳验证准确率: 0.8000\n",
      "\n",
      "🎯 最终测试集性能 (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.62      0.56      0.59         9\n",
      "       p_lie       0.67      0.89      0.76         9\n",
      "       p_sit       0.75      0.33      0.46         9\n",
      "     p_stand       0.82      1.00      0.90         9\n",
      "      t_bend       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.73        45\n",
      "   macro avg       0.73      0.73      0.71        45\n",
      "weighted avg       0.73      0.73      0.71        45\n",
      "\n",
      "混淆矩阵:\n",
      "[[5 2 1 0 1]\n",
      " [1 8 0 0 0]\n",
      " [2 2 3 1 1]\n",
      " [0 0 0 9 0]\n",
      " [0 0 0 1 8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_12508\\3346585549.py:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"data/transformer_pose5.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# ===========================================================\n",
    "# Step 1. 加载数据\n",
    "# ===========================================================\n",
    "data_path = \"data\"\n",
    "X = np.load(os.path.join(data_path, \"X.npy\"))   # (N, frames, joints, 3)\n",
    "y = np.load(os.path.join(data_path, \"y.npy\"))\n",
    "\n",
    "print(f\"原始 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 2. 过滤坏样本\n",
    "# ===========================================================\n",
    "def filter_invalid_samples(X, y, max_abs=10000.0, min_std=1e-6, log_path=\"data/bad_samples.txt\"):\n",
    "    valid_idx, bad_info = [], []\n",
    "    for i in range(len(X)):\n",
    "        xi = X[i]\n",
    "        reason = None\n",
    "        if np.isnan(xi).any() or np.isinf(xi).any():\n",
    "            reason = \"NaN/Inf\"\n",
    "        elif np.allclose(xi, 0):\n",
    "            reason = \"All zeros\"\n",
    "        elif np.std(xi) < min_std:\n",
    "            reason = f\"Low std ({np.std(xi):.2e})\"\n",
    "        elif np.any(np.abs(xi) > max_abs):\n",
    "            reason = f\"Out of range (max={np.max(np.abs(xi)):.1f})\"\n",
    "\n",
    "        if reason:\n",
    "            bad_info.append(f\"Sample {i}: {reason}\")\n",
    "        else:\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if bad_info:\n",
    "            f.write(\"\\n\".join(bad_info))\n",
    "        else:\n",
    "            f.write(\"✅ 无坏样本\\n\")\n",
    "\n",
    "    print(f\"检测并移除 {len(bad_info)} 个坏样本（剩余 {len(valid_idx)} 个），日志已保存到 {log_path}\")\n",
    "    return X[valid_idx], y[valid_idx]\n",
    "\n",
    "X, y = filter_invalid_samples(X, y)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 3. 筛选目标类别\n",
    "# ===========================================================\n",
    "target_labels = [\"p_sit\", \"p_stand\", \"a_walk\", \"p_lie\", \"t_bend\"]\n",
    "mask = np.isin(y, target_labels)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"类别数:\", len(set(y)))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 4. 转换为 (frames, features)\n",
    "# ===========================================================\n",
    "N, T, J, C = X.shape\n",
    "X = X.reshape(N, T, J * C)\n",
    "print(f\"特征展开后: {X.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 5. 标签编码 + 过采样\n",
    "# ===========================================================\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(\"类别映射:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "# 简单过采样\n",
    "from sklearn.utils import resample\n",
    "X_bal, y_bal = [], []\n",
    "for cls in np.unique(y):\n",
    "    X_cls = X[y == cls]\n",
    "    y_cls = y[y == cls]\n",
    "    X_up, y_up = resample(X_cls, y_cls, replace=True, n_samples=max(Counter(y).values()), random_state=42)\n",
    "    X_bal.append(X_up)\n",
    "    y_bal.append(y_up)\n",
    "X_res = np.vstack(X_bal)\n",
    "y_res = np.hstack(y_bal)\n",
    "print(\"过采样后类别分布:\", Counter(y_res))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 6. 数据划分 (Train/Val/Test)\n",
    "# ===========================================================\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {X_train.shape} Val: {X_val.shape} Test: {X_test.shape}\")\n",
    "\n",
    "# 转为 Tensor\n",
    "X_train, X_val, X_test = map(lambda x: torch.tensor(x, dtype=torch.float32), [X_train, X_val, X_test])\n",
    "y_train, y_val, y_test = map(lambda y: torch.tensor(y, dtype=torch.long), [y_train, y_val, y_test])\n",
    "\n",
    "# ===========================================================\n",
    "# Step 7. Transformer 模型定义\n",
    "# ===========================================================\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=96, num_classes=5, num_heads=4, num_layers=2, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, input_dim))  # learnable positional encoding\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        out = self.transformer(x)\n",
    "        out = self.norm(out[:, -1, :])  # 取最后一帧\n",
    "        return self.fc(out)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 8. 训练流程\n",
    "# ===========================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PoseTransformer(input_dim=X_train.shape[2], num_classes=len(le.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "\n",
    "best_acc, patience, wait = 0.0, 8, 0\n",
    "for epoch in range(1, 41):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        train_correct += (out.argmax(1) == yb).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            val_correct += (out.argmax(1) == yb).sum().item()\n",
    "\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d}/40 | Train Loss: {train_loss/len(train_loader.dataset):.4f} \"\n",
    "          f\"Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader.dataset):.4f} \"\n",
    "          f\"Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc, wait = val_acc, 0\n",
    "        torch.save(model.state_dict(), \"data/transformer_pose5.pth\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"⚠️ 提前停止：验证集不再提升。\")\n",
    "            break\n",
    "\n",
    "print(f\"✅ 最佳验证准确率: {best_acc:.4f}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 9. Test 阶段\n",
    "# ===========================================================\n",
    "model.load_state_dict(torch.load(\"data/transformer_pose5.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        y_true.extend(yb.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"\\n🎯 最终测试集性能 (Test Set):\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae721a07-4b2c-4c2f-810d-f36b2ef1cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "原始类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('Unknown'): 17, np.str_('t_bend'): 7, np.str_('p_lie'): 6, np.str_('p_bent'): 6, np.str_('t_straighten'): 4, np.str_('t_bed_turn'): 3, np.str_('t_stand_to_sit'): 1})\n",
      "检测并移除 0 个坏样本（剩余 144 个），日志已保存到 data/bad_samples.txt\n",
      "过滤后 X shape: (113, 100, 32, 3) y shape: (113,)\n",
      "特征展开后: (113, 100, 96)\n",
      "类别映射: {np.str_('a_walk'): 0, np.str_('p_lie'): 1, np.str_('p_sit'): 2, np.str_('p_stand'): 3, np.str_('t_bend'): 4}\n",
      "过采样后类别分布: Counter({np.int64(0): 45, np.int64(1): 45, np.int64(2): 45, np.int64(3): 45, np.int64(4): 45})\n",
      "Train: (135, 100, 96) Val: (45, 100, 96) Test: (45, 100, 96)\n",
      "Epoch 01/40 | Train Loss: 1.6411 Acc: 0.2222 | Val Loss: 1.4576 Acc: 0.3111 | Test Acc: 0.2444 | LR: [0.001]\n",
      "Epoch 02/40 | Train Loss: 1.4964 Acc: 0.3407 | Val Loss: 1.2613 Acc: 0.4667 | Test Acc: 0.4444 | LR: [0.001]\n",
      "Epoch 03/40 | Train Loss: 1.4162 Acc: 0.3407 | Val Loss: 1.1601 Acc: 0.4889 | Test Acc: 0.4444 | LR: [0.001]\n",
      "Epoch 04/40 | Train Loss: 1.3901 Acc: 0.3556 | Val Loss: 1.1454 Acc: 0.5778 | Test Acc: 0.5333 | LR: [0.001]\n",
      "Epoch 05/40 | Train Loss: 1.4099 Acc: 0.4074 | Val Loss: 1.1560 Acc: 0.5778 | Test Acc: 0.5778 | LR: [0.001]\n",
      "Epoch 06/40 | Train Loss: 1.2424 Acc: 0.5407 | Val Loss: 1.1026 Acc: 0.6222 | Test Acc: 0.6889 | LR: [0.001]\n",
      "Epoch 07/40 | Train Loss: 1.2207 Acc: 0.5778 | Val Loss: 1.0467 Acc: 0.6444 | Test Acc: 0.6444 | LR: [0.001]\n",
      "Epoch 08/40 | Train Loss: 1.1748 Acc: 0.6000 | Val Loss: 0.9529 Acc: 0.7111 | Test Acc: 0.6889 | LR: [0.001]\n",
      "Epoch 09/40 | Train Loss: 1.1238 Acc: 0.5481 | Val Loss: 0.9577 Acc: 0.6889 | Test Acc: 0.6889 | LR: [0.001]\n",
      "Epoch 10/40 | Train Loss: 1.0498 Acc: 0.6444 | Val Loss: 0.8444 Acc: 0.7556 | Test Acc: 0.6889 | LR: [0.001]\n",
      "Epoch 11/40 | Train Loss: 1.0403 Acc: 0.5926 | Val Loss: 0.8306 Acc: 0.6889 | Test Acc: 0.6444 | LR: [0.0005]\n",
      "Epoch 12/40 | Train Loss: 0.9429 Acc: 0.6148 | Val Loss: 0.8388 Acc: 0.6444 | Test Acc: 0.6222 | LR: [0.0005]\n",
      "Epoch 13/40 | Train Loss: 0.8515 Acc: 0.7185 | Val Loss: 0.7606 Acc: 0.6667 | Test Acc: 0.6667 | LR: [0.0005]\n",
      "Epoch 14/40 | Train Loss: 0.9398 Acc: 0.6889 | Val Loss: 0.8173 Acc: 0.7111 | Test Acc: 0.6222 | LR: [0.0005]\n",
      "Epoch 15/40 | Train Loss: 0.9772 Acc: 0.6444 | Val Loss: 0.7593 Acc: 0.7333 | Test Acc: 0.6667 | LR: [0.0005]\n",
      "Epoch 16/40 | Train Loss: 0.9456 Acc: 0.6444 | Val Loss: 0.7648 Acc: 0.7556 | Test Acc: 0.6444 | LR: [0.0005]\n",
      "Epoch 17/40 | Train Loss: 0.8321 Acc: 0.7259 | Val Loss: 0.7182 Acc: 0.7111 | Test Acc: 0.6667 | LR: [0.0005]\n",
      "Epoch 18/40 | Train Loss: 0.8091 Acc: 0.6963 | Val Loss: 0.7599 Acc: 0.6889 | Test Acc: 0.6222 | LR: [0.0005]\n",
      "⚠️ 提前停止：验证集不再提升。\n",
      "✅ 最佳验证准确率: 0.7556\n",
      "\n",
      "🎯 最终测试集性能 (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       0.80      0.44      0.57         9\n",
      "       p_lie       0.73      0.89      0.80         9\n",
      "       p_sit       0.46      0.67      0.55         9\n",
      "     p_stand       1.00      0.56      0.71         9\n",
      "      t_bend       0.73      0.89      0.80         9\n",
      "\n",
      "    accuracy                           0.69        45\n",
      "   macro avg       0.74      0.69      0.69        45\n",
      "weighted avg       0.74      0.69      0.69        45\n",
      "\n",
      "混淆矩阵:\n",
      "[[4 1 2 0 2]\n",
      " [1 8 0 0 0]\n",
      " [0 2 6 0 1]\n",
      " [0 0 4 5 0]\n",
      " [0 0 1 0 8]]\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "# ===========================================================\n",
    "# Step 1. 数据加载\n",
    "# ===========================================================\n",
    "data_path = \"data\"\n",
    "X = np.load(os.path.join(data_path, \"X.npy\"))\n",
    "y = np.load(os.path.join(data_path, \"y.npy\"))\n",
    "\n",
    "print(f\"原始 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 2. 过滤坏样本\n",
    "# ===========================================================\n",
    "def filter_invalid_samples(X, y, max_abs=10000.0, min_std=1e-6, log_path=\"data/bad_samples.txt\"):\n",
    "    valid_idx, bad_info = [], []\n",
    "    for i in range(len(X)):\n",
    "        xi = X[i]\n",
    "        reason = None\n",
    "        if np.isnan(xi).any() or np.isinf(xi).any():\n",
    "            reason = \"NaN/Inf\"\n",
    "        elif np.allclose(xi, 0):\n",
    "            reason = \"All zeros\"\n",
    "        elif np.std(xi) < min_std:\n",
    "            reason = f\"Low std ({np.std(xi):.2e})\"\n",
    "        elif np.any(np.abs(xi) > max_abs):\n",
    "            reason = f\"Out of range (max={np.max(np.abs(xi)):.1f})\"\n",
    "\n",
    "        if reason:\n",
    "            bad_info.append(f\"Sample {i}: {reason}\")\n",
    "        else:\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if bad_info:\n",
    "            f.write(\"\\n\".join(bad_info))\n",
    "        else:\n",
    "            f.write(\"✅ 无坏样本\\n\")\n",
    "\n",
    "    print(f\"检测并移除 {len(bad_info)} 个坏样本（剩余 {len(valid_idx)} 个），日志已保存到 {log_path}\")\n",
    "    return X[valid_idx], y[valid_idx]\n",
    "\n",
    "X, y = filter_invalid_samples(X, y)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 3. 筛选目标类别\n",
    "# ===========================================================\n",
    "target_labels = [\"p_sit\", \"p_stand\", \"a_walk\", \"p_lie\", \"t_bend\"]\n",
    "mask = np.isin(y, target_labels)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 4. 特征展开\n",
    "# ===========================================================\n",
    "N, T, J, C = X.shape\n",
    "X = X.reshape(N, T, J * C)\n",
    "print(f\"特征展开后: {X.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 5. 标签编码 + 过采样\n",
    "# ===========================================================\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(\"类别映射:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "X_bal, y_bal = [], []\n",
    "for cls in np.unique(y):\n",
    "    X_cls = X[y == cls]\n",
    "    y_cls = y[y == cls]\n",
    "    X_up, y_up = resample(X_cls, y_cls, replace=True, n_samples=max(Counter(y).values()), random_state=42)\n",
    "    X_bal.append(X_up)\n",
    "    y_bal.append(y_up)\n",
    "X_res = np.vstack(X_bal)\n",
    "y_res = np.hstack(y_bal)\n",
    "print(\"过采样后类别分布:\", Counter(y_res))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 6. 数据划分 (Train / Val / Test)\n",
    "# ===========================================================\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "print(f\"Train: {X_train.shape} Val: {X_val.shape} Test: {X_test.shape}\")\n",
    "\n",
    "X_train, X_val, X_test = map(lambda x: torch.tensor(x, dtype=torch.float32), [X_train, X_val, X_test])\n",
    "y_train, y_val, y_test = map(lambda y: torch.tensor(y, dtype=torch.long), [y_train, y_val, y_test])\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 7. Transformer 模型\n",
    "# ===========================================================\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=96, num_classes=5, num_heads=4, num_layers=2, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, input_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        out = self.transformer(x)\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        return self.fc(out)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 8. 训练与验证\n",
    "# ===========================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PoseTransformer(input_dim=X_train.shape[2], num_classes=len(le.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "best_acc, patience, wait = 0.0, 8, 0\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    # ---------- Train ----------\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        train_correct += (out.argmax(1) == yb).sum().item()\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            val_correct += (out.argmax(1) == yb).sum().item()\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "    # ---------- Test (new column) ----------\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            test_correct += (out.argmax(1) == yb).sum().item()\n",
    "    test_acc = test_correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/40 | \"\n",
    "          f\"Train Loss: {train_loss/len(train_loader.dataset):.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader.dataset):.4f} Acc: {val_acc:.4f} | \"\n",
    "          f\"Test Acc: {test_acc:.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---------- Early stopping ----------\n",
    "    if val_acc > best_acc:\n",
    "        best_acc, wait = val_acc, 0\n",
    "        torch.save(model.state_dict(), \"data/transformer_pose5.pth\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"⚠️ 提前停止：验证集不再提升。\")\n",
    "            break\n",
    "\n",
    "print(f\"✅ 最佳验证准确率: {best_acc:.4f}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 9. 测试阶段\n",
    "# ===========================================================\n",
    "state_dict = torch.load(\"data/transformer_pose5.pth\", weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        y_true.extend(yb.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"\\n🎯 最终测试集性能 (Test Set):\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02c63c4-2d26-4599-8d82-70c2ee068920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 X shape: (144, 100, 32, 3) y shape: (144,)\n",
      "原始类别分布: Counter({np.str_('p_stand'): 45, np.str_('a_walk'): 28, np.str_('p_sit'): 27, np.str_('Unknown'): 17, np.str_('t_bend'): 7, np.str_('p_lie'): 6, np.str_('p_bent'): 6, np.str_('t_straighten'): 4, np.str_('t_bed_turn'): 3, np.str_('t_stand_to_sit'): 1})\n",
      "检测并移除 0 个坏样本（剩余 144 个），日志已保存到 data/bad_samples.txt\n",
      "过滤后 X shape: (113, 100, 32, 3) y shape: (113,)\n",
      "特征展开后: (113, 100, 96)\n",
      "类别映射: {np.str_('a_walk'): 0, np.str_('p_lie'): 1, np.str_('p_sit'): 2, np.str_('p_stand'): 3, np.str_('t_bend'): 4}\n",
      "过采样后类别分布: Counter({np.int64(0): 45, np.int64(1): 45, np.int64(2): 45, np.int64(3): 45, np.int64(4): 45})\n",
      "Train: (135, 100, 96) Val: (45, 100, 96) Test: (45, 100, 96)\n",
      "Epoch 01/40 | Train F1: 0.2879 | Val F1: 0.1747 | Test F1: 0.1781 | LR: [0.001]\n",
      "Epoch 02/40 | Train F1: 0.2618 | Val F1: 0.3366 | Test F1: 0.3351 | LR: [0.001]\n",
      "Epoch 03/40 | Train F1: 0.3540 | Val F1: 0.4083 | Test F1: 0.3851 | LR: [0.001]\n",
      "Epoch 04/40 | Train F1: 0.3212 | Val F1: 0.4557 | Test F1: 0.3964 | LR: [0.001]\n",
      "Epoch 05/40 | Train F1: 0.4011 | Val F1: 0.6661 | Test F1: 0.5854 | LR: [0.001]\n",
      "Epoch 06/40 | Train F1: 0.4141 | Val F1: 0.6897 | Test F1: 0.6129 | LR: [0.001]\n",
      "Epoch 07/40 | Train F1: 0.4541 | Val F1: 0.6494 | Test F1: 0.6030 | LR: [0.001]\n",
      "Epoch 08/40 | Train F1: 0.5150 | Val F1: 0.6868 | Test F1: 0.6543 | LR: [0.001]\n",
      "Epoch 09/40 | Train F1: 0.5700 | Val F1: 0.7165 | Test F1: 0.6266 | LR: [0.001]\n",
      "Epoch 10/40 | Train F1: 0.6037 | Val F1: 0.6891 | Test F1: 0.6266 | LR: [0.001]\n",
      "Epoch 11/40 | Train F1: 0.5777 | Val F1: 0.6924 | Test F1: 0.6435 | LR: [0.0005]\n",
      "Epoch 12/40 | Train F1: 0.6083 | Val F1: 0.7133 | Test F1: 0.7028 | LR: [0.0005]\n",
      "Epoch 13/40 | Train F1: 0.6607 | Val F1: 0.7072 | Test F1: 0.6653 | LR: [0.0005]\n",
      "Epoch 14/40 | Train F1: 0.6929 | Val F1: 0.7318 | Test F1: 0.6587 | LR: [0.0005]\n",
      "Epoch 15/40 | Train F1: 0.6542 | Val F1: 0.7337 | Test F1: 0.6844 | LR: [0.0005]\n",
      "Epoch 16/40 | Train F1: 0.6628 | Val F1: 0.7540 | Test F1: 0.6787 | LR: [0.0005]\n",
      "Epoch 17/40 | Train F1: 0.7034 | Val F1: 0.7281 | Test F1: 0.6134 | LR: [0.0005]\n",
      "Epoch 18/40 | Train F1: 0.7133 | Val F1: 0.7318 | Test F1: 0.6887 | LR: [0.0005]\n",
      "Epoch 19/40 | Train F1: 0.7377 | Val F1: 0.7530 | Test F1: 0.7009 | LR: [0.0005]\n",
      "Epoch 20/40 | Train F1: 0.6880 | Val F1: 0.8177 | Test F1: 0.7054 | LR: [0.0005]\n",
      "Epoch 21/40 | Train F1: 0.7640 | Val F1: 0.7743 | Test F1: 0.7040 | LR: [0.00025]\n",
      "Epoch 22/40 | Train F1: 0.7548 | Val F1: 0.7751 | Test F1: 0.6984 | LR: [0.00025]\n",
      "Epoch 23/40 | Train F1: 0.7996 | Val F1: 0.7961 | Test F1: 0.7199 | LR: [0.00025]\n",
      "Epoch 24/40 | Train F1: 0.7696 | Val F1: 0.7961 | Test F1: 0.7199 | LR: [0.00025]\n",
      "Epoch 25/40 | Train F1: 0.7987 | Val F1: 0.7961 | Test F1: 0.7199 | LR: [0.00025]\n",
      "Epoch 26/40 | Train F1: 0.8059 | Val F1: 0.7917 | Test F1: 0.7575 | LR: [0.00025]\n",
      "Epoch 27/40 | Train F1: 0.7975 | Val F1: 0.7961 | Test F1: 0.7238 | LR: [0.00025]\n",
      "Epoch 28/40 | Train F1: 0.7962 | Val F1: 0.8152 | Test F1: 0.7420 | LR: [0.00025]\n",
      "⚠️ Early stop\n",
      "Best F1: 0.8177\n",
      "\n",
      "🎯 最终测试集性能 (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      a_walk       1.00      0.44      0.62         9\n",
      "       p_lie       0.60      1.00      0.75         9\n",
      "       p_sit       0.75      0.33      0.46         9\n",
      "     p_stand       0.82      1.00      0.90         9\n",
      "      t_bend       0.73      0.89      0.80         9\n",
      "\n",
      "    accuracy                           0.73        45\n",
      "   macro avg       0.78      0.73      0.71        45\n",
      "weighted avg       0.78      0.73      0.71        45\n",
      "\n",
      "混淆矩阵:\n",
      "[[4 2 1 0 2]\n",
      " [0 9 0 0 0]\n",
      " [0 4 3 1 1]\n",
      " [0 0 0 9 0]\n",
      " [0 0 0 1 8]]\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "# ===========================================================\n",
    "# Step 1. 数据加载\n",
    "# ===========================================================\n",
    "data_path = \"data\"\n",
    "X = np.load(os.path.join(data_path, \"X.npy\"))\n",
    "y = np.load(os.path.join(data_path, \"y.npy\"))\n",
    "\n",
    "print(f\"原始 X shape: {X.shape} y shape: {y.shape}\")\n",
    "print(\"原始类别分布:\", Counter(y))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 2. 过滤坏样本\n",
    "# ===========================================================\n",
    "def filter_invalid_samples(X, y, max_abs=10000.0, min_std=1e-6, log_path=\"data/bad_samples.txt\"):\n",
    "    valid_idx, bad_info = [], []\n",
    "    for i in range(len(X)):\n",
    "        xi = X[i]\n",
    "        reason = None\n",
    "        if np.isnan(xi).any() or np.isinf(xi).any():\n",
    "            reason = \"NaN/Inf\"\n",
    "        elif np.allclose(xi, 0):\n",
    "            reason = \"All zeros\"\n",
    "        elif np.std(xi) < min_std:\n",
    "            reason = f\"Low std ({np.std(xi):.2e})\"\n",
    "        elif np.any(np.abs(xi) > max_abs):\n",
    "            reason = f\"Out of range (max={np.max(np.abs(xi)):.1f})\"\n",
    "\n",
    "        if reason:\n",
    "            bad_info.append(f\"Sample {i}: {reason}\")\n",
    "        else:\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if bad_info:\n",
    "            f.write(\"\\n\".join(bad_info))\n",
    "        else:\n",
    "            f.write(\"✅ 无坏样本\\n\")\n",
    "\n",
    "    print(f\"检测并移除 {len(bad_info)} 个坏样本（剩余 {len(valid_idx)} 个），日志已保存到 {log_path}\")\n",
    "    return X[valid_idx], y[valid_idx]\n",
    "\n",
    "X, y = filter_invalid_samples(X, y)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 3. 筛选目标类别\n",
    "# ===========================================================\n",
    "target_labels = [\"p_sit\", \"p_stand\", \"a_walk\", \"p_lie\", \"t_bend\"]\n",
    "mask = np.isin(y, target_labels)\n",
    "X, y = X[mask], y[mask]\n",
    "print(f\"过滤后 X shape: {X.shape} y shape: {y.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 4. 特征展开\n",
    "# ===========================================================\n",
    "N, T, J, C = X.shape\n",
    "X = X.reshape(N, T, J * C)\n",
    "print(f\"特征展开后: {X.shape}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 5. 标签编码 + 过采样\n",
    "# ===========================================================\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(\"类别映射:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "X_bal, y_bal = [], []\n",
    "for cls in np.unique(y):\n",
    "    X_cls = X[y == cls]\n",
    "    y_cls = y[y == cls]\n",
    "    X_up, y_up = resample(X_cls, y_cls, replace=True, n_samples=max(Counter(y).values()), random_state=42)\n",
    "    X_bal.append(X_up)\n",
    "    y_bal.append(y_up)\n",
    "X_res = np.vstack(X_bal)\n",
    "y_res = np.hstack(y_bal)\n",
    "print(\"过采样后类别分布:\", Counter(y_res))\n",
    "\n",
    "# ===========================================================\n",
    "# Step 6. 数据划分 (Train / Val / Test)\n",
    "# ===========================================================\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "print(f\"Train: {X_train.shape} Val: {X_val.shape} Test: {X_test.shape}\")\n",
    "\n",
    "X_train, X_val, X_test = map(lambda x: torch.tensor(x, dtype=torch.float32), [X_train, X_val, X_test])\n",
    "y_train, y_val, y_test = map(lambda y: torch.tensor(y, dtype=torch.long), [y_train, y_val, y_test])\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 7. Transformer 模型定义\n",
    "# ===========================================================\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=96, num_classes=5, num_heads=4, num_layers=2, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, input_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        out = self.transformer(x)\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        return self.fc(out)\n",
    "\n",
    "# ===========================================================\n",
    "# Step 8. 训练与验证 (F1-based Early Stopping)\n",
    "# ===========================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PoseTransformer(input_dim=X_train.shape[2], num_classes=len(le.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "best_f1, patience, wait = 0.0, 8, 0\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    # ---------- Train ----------\n",
    "    model.train()\n",
    "    y_train_true, y_train_pred, train_loss = [], [], 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        y_train_true.extend(yb.cpu().numpy())\n",
    "        y_train_pred.extend(out.argmax(1).cpu().numpy())\n",
    "    train_f1 = f1_score(y_train_true, y_train_pred, average='macro')\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    y_val_true, y_val_pred, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            y_val_true.extend(yb.cpu().numpy())\n",
    "            y_val_pred.extend(out.argmax(1).cpu().numpy())\n",
    "    val_f1 = f1_score(y_val_true, y_val_pred, average='macro')\n",
    "\n",
    "    # ---------- Test ----------\n",
    "    y_test_true, y_test_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            y_test_true.extend(yb.cpu().numpy())\n",
    "            y_test_pred.extend(out.argmax(1).cpu().numpy())\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/40 | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Test F1: {test_f1:.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---------- Early stopping based on F1 ----------\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, wait = val_f1, 0\n",
    "        torch.save(model.state_dict(), \"data/transformer_pose5_f1best.pth\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"⚠️ Early stop\")\n",
    "            break\n",
    "\n",
    "print(f\"Best F1: {best_f1:.4f}\")\n",
    "\n",
    "# ===========================================================\n",
    "# Step 9. 测试阶段评估\n",
    "# ===========================================================\n",
    "state_dict = torch.load(\"data/transformer_pose5_f1best.pth\", weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        y_true.extend(yb.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"\\n🎯 最终测试集性能 (Test Set):\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "print(\"混淆矩阵:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3200c5e4-e3c6-416f-b71f-0b2a823cf33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09_SY.pickle] {'total': 47199, 'usable': 0, 'skipped_no3d': 23858, 'skipped_badpose': 0, 'label_dist': {}}\n",
      "[16_GZ.pickle] {'total': 33409, 'usable': 0, 'skipped_no3d': 16439, 'skipped_badpose': 0, 'label_dist': {}}\n",
      "[17_JP.pickle] {'total': 36546, 'usable': 0, 'skipped_no3d': 15128, 'skipped_badpose': 0, 'label_dist': {}}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "没有可用帧，请检查 PoseNDF 阈值/活动映射/文件内容。",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 225\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.basename(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msumm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_X_frames) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, all_X_frames)) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m没有可用帧，请检查 PoseNDF 阈值/活动映射/文件内容。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# 连起来（默认按文件内部时间顺序已是连续的；不同文件之间不连窗）\u001b[39;00m\n\u001b[32m    228\u001b[39m X_total, y_total = [], []\n",
      "\u001b[31mRuntimeError\u001b[39m: 没有可用帧，请检查 PoseNDF 阈值/活动映射/文件内容。"
     ]
    }
   ],
   "source": [
    "import os, math, pickle, json, warnings\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# 0) 基本配置\n",
    "# =========================\n",
    "PICKLE_FILES = [\n",
    "    \"data/09_SY.pickle\",\n",
    "    \"data/16_GZ.pickle\",\n",
    "    \"data/17_JP.pickle\",\n",
    "]\n",
    "\n",
    "SAVE_X = \"data/external_X.npy\"\n",
    "SAVE_Y = \"data/external_y.npy\"\n",
    "\n",
    "# 与训练时保持一致\n",
    "T = 100\n",
    "STRIDE = 25\n",
    "NUM_JOINTS = 32\n",
    "NUM_CH = 3\n",
    "ROOT_J = 0                    # 根关节索引（据你的骨架定义可调整）\n",
    "MAX_ABS_Z = 5.0               # 简单极值过滤\n",
    "POSE_NDF_MIN = 0.10           # 可用 PoseNDF 下限阈值（若值可用）\n",
    "REQUIRE_SINGLE_PERSON = False # 若需要只保留单人帧可改为 True\n",
    "\n",
    "# 仅保留 5 个动作\n",
    "KEEP_LABELS = [\"a_walk\", \"p_lie\", \"p_sit\", \"p_stand\", \"t_bend\"]\n",
    "LABEL2ID = {k:i for i,k in enumerate(KEEP_LABELS)}\n",
    "\n",
    "# =========================\n",
    "# 1) 标签映射（兼容同义词）\n",
    "# =========================\n",
    "SYNONYMS = {\n",
    "    \"a_walk\":       [\"a_walk\", \"walk\", \"walking\"],\n",
    "    \"p_lie\":        [\"p_lie\", \"lying\", \"lay\", \"laying\"],\n",
    "    \"p_sit\":        [\"p_sit\", \"sitting\", \"sit\"],\n",
    "    \"p_stand\":      [\"p_stand\", \"stand\", \"standing\"],\n",
    "    \"t_bend\":       [\"t_bend\", \"bend\", \"bending\", \"p_bent\"],  # 有时“p_bent”被当姿态/短动作\n",
    "}\n",
    "\n",
    "def map_activities_to_label(acts_dict):\n",
    "    \"\"\"\n",
    "    从 activities 字典中找出与 5 类最接近的键。\n",
    "    策略：\n",
    "      1) 直接命中 KEEP_LABELS\n",
    "      2) 命中同义词表\n",
    "      3) 若 acts_dict 是 {label:score}，选分最高的且能映射的\n",
    "    找不到则返回 None（该帧跳过）。\n",
    "    \"\"\"\n",
    "    if not isinstance(acts_dict, dict) or len(acts_dict) == 0:\n",
    "        return None\n",
    "\n",
    "    # 先尝试直接命中\n",
    "    keys_lower = {str(k).lower(): v for k, v in acts_dict.items()}\n",
    "\n",
    "    # 直接命中\n",
    "    for lbl in KEEP_LABELS:\n",
    "        if lbl in keys_lower:\n",
    "            return lbl\n",
    "\n",
    "    # 同义词命中\n",
    "    for lbl, syns in SYNONYMS.items():\n",
    "        for s in syns:\n",
    "            if s in keys_lower:\n",
    "                return lbl\n",
    "\n",
    "    # 选择一个“最像”的：分数最高且能通过同义词表匹配\n",
    "    # 假设 values 是分数/概率/布尔 (True=1)\n",
    "    try:\n",
    "        candidates = sorted(keys_lower.items(), key=lambda kv: float(kv[1]), reverse=True)\n",
    "    except:\n",
    "        candidates = list(keys_lower.items())\n",
    "\n",
    "    for k, _ in candidates:\n",
    "        for lbl, syns in SYNONYMS.items():\n",
    "            if any(s in k for s in syns):\n",
    "                return lbl\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) 坏姿态判定\n",
    "# =========================\n",
    "def is_bad_pose(arr32x3, pose_ndf=None):\n",
    "    \"\"\"简单坏姿态过滤：NaN/零方差/极值/PoseNDF过低\"\"\"\n",
    "    if arr32x3.shape != (NUM_JOINTS, NUM_CH):\n",
    "        return True\n",
    "    if not np.isfinite(arr32x3).all():\n",
    "        return True\n",
    "    # 零方差（整骨架冻结）\n",
    "    if np.allclose(arr32x3.std(axis=0), 0, atol=1e-6):\n",
    "        return True\n",
    "    # 简单极值过滤（可改为按分位数）\n",
    "    if np.max(np.abs(arr32x3[:,2])) > MAX_ABS_Z:\n",
    "        return True\n",
    "    # PoseNDF（若提供）\n",
    "    if pose_ndf is not None:\n",
    "        try:\n",
    "            score = float(pose_ndf)\n",
    "            if score < POSE_NDF_MIN:\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) 逐文件解析 -> 逐帧提取 (32,3) & label\n",
    "# =========================\n",
    "def extract_frames_from_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    frames_pose = []\n",
    "    frames_label = []\n",
    "\n",
    "    skipped_no3d = 0\n",
    "    skipped_bad = 0\n",
    "    mapped = 0\n",
    "\n",
    "    for rec in data:\n",
    "        # 3D pose\n",
    "        p3d = rec.get(\"poses3d\", None)\n",
    "        if p3d is None or np.size(p3d) != 128:\n",
    "            skipped_no3d += 1\n",
    "            continue\n",
    "        p3d = np.asarray(p3d).reshape(32, 4)[:, :3]  # 取 (32,3)\n",
    "\n",
    "        # PoseNDF（可能是 [[x]]、None）\n",
    "        pndf = rec.get(\"PoseNDF_score3d\", None)\n",
    "        if isinstance(pndf, (list, np.ndarray)) and np.size(pndf) >= 1:\n",
    "            pndf_val = np.asarray(pndf).flatten()[0]\n",
    "        else:\n",
    "            pndf_val = None\n",
    "\n",
    "        # 单人约束（可选）\n",
    "        if REQUIRE_SINGLE_PERSON:\n",
    "            n_people = rec.get(\"num_people\", None)\n",
    "            try:\n",
    "                if int(n_people) != 1:\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 活动标签\n",
    "        acts = rec.get(\"activities\", {})\n",
    "        lbl = map_activities_to_label(acts)\n",
    "\n",
    "        # 筛掉非 5 类\n",
    "        if lbl is None or lbl not in KEEP_LABELS:\n",
    "            continue\n",
    "\n",
    "        # 坏姿态过滤\n",
    "        if is_bad_pose(p3d, pose_ndf=pndf_val):\n",
    "            skipped_bad += 1\n",
    "            continue\n",
    "\n",
    "        frames_pose.append(p3d)\n",
    "        frames_label.append(lbl)\n",
    "        mapped += 1\n",
    "\n",
    "    summary = {\n",
    "        \"total\": len(data),\n",
    "        \"usable\": mapped,\n",
    "        \"skipped_no3d\": skipped_no3d,\n",
    "        \"skipped_badpose\": skipped_bad,\n",
    "        \"label_dist\": dict(Counter(frames_label)),\n",
    "    }\n",
    "    return np.array(frames_pose, dtype=np.float32), frames_label, summary\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) 滑窗成序列 (N, T, 32, 3)\n",
    "# =========================\n",
    "def build_sequences(poses_list, labels_list, T=100, stride=25):\n",
    "    \"\"\"\n",
    "    poses_list: 连续帧的 (32,3)；labels_list: 每帧的标签字符串\n",
    "    策略：窗口内标签以“多数表决”为主（或中间帧），这里用“多数表决”\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    n = len(poses_list)\n",
    "    for start in range(0, max(n - T + 1, 0), stride):\n",
    "        end = start + T\n",
    "        block = poses_list[start:end]\n",
    "        labs  = labels_list[start:end]\n",
    "        if len(block) < T:\n",
    "            break\n",
    "\n",
    "        # 简单归一：根关节去中心化（与训练一致）\n",
    "        block = block.copy()  # (T, 32, 3)\n",
    "        block -= block[:, [ROOT_J], :]\n",
    "\n",
    "        # 标签多数表决\n",
    "        maj = Counter(labs).most_common(1)[0][0]\n",
    "\n",
    "        X_seq.append(block)\n",
    "        y_seq.append(LABEL2ID[maj])\n",
    "\n",
    "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.int64)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) 读取三文件 -> 汇总 -> 滑窗 -> 保存\n",
    "# =========================\n",
    "all_X_frames, all_y_frames = [], []\n",
    "summaries = {}\n",
    "\n",
    "for p in PICKLE_FILES:\n",
    "    Xf, Yf, summ = extract_frames_from_pickle(p)\n",
    "    summaries[os.path.basename(p)] = summ\n",
    "    all_X_frames.append(Xf)\n",
    "    all_y_frames += Yf  # 注意：labels 是字符串\n",
    "    print(f\"[{os.path.basename(p)}] {summ}\")\n",
    "\n",
    "if len(all_X_frames) == 0 or sum(map(len, all_X_frames)) == 0:\n",
    "    raise RuntimeError(\"没有可用帧，请检查 PoseNDF 阈值/活动映射/文件内容。\")\n",
    "\n",
    "# 连起来（默认按文件内部时间顺序已是连续的；不同文件之间不连窗）\n",
    "X_total, y_total = [], []\n",
    "offset = 0\n",
    "for i, p in enumerate(PICKLE_FILES):\n",
    "    Xf, Yf, _ = extract_frames_from_pickle(p)\n",
    "    if len(Xf) == 0:\n",
    "        continue\n",
    "    Xseq, yseq = build_sequences(Xf, Yf, T=T, stride=STRIDE)\n",
    "    print(f\"Build sequences from {os.path.basename(p)} -> {Xseq.shape}\")\n",
    "    if len(Xseq):\n",
    "        X_total.append(Xseq)\n",
    "        y_total.append(yseq)\n",
    "\n",
    "if len(X_total) == 0:\n",
    "    raise RuntimeError(\"滑窗后没有可用样本，请降低 T 或 stride，或放宽过滤规则。\")\n",
    "\n",
    "X = np.concatenate(X_total, axis=0)\n",
    "y = np.concatenate(y_total, axis=0)\n",
    "print(\"\\n=== External dataset (after windowing) ===\")\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Label dist:\", {KEEP_LABELS[i]: int(n) for i, n in Counter(y).items()})\n",
    "\n",
    "np.save(SAVE_X, X)\n",
    "np.save(SAVE_Y, y)\n",
    "print(f\"Saved to {SAVE_X} / {SAVE_Y}\")\n",
    "\n",
    "# =========================\n",
    "# 6) 评测：加载你之前的 Transformer 模型并在外部数据上测试\n",
    "#   （下面是与你之前版本一致的轻量 Transformer 分类头）\n",
    "# =========================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, L, d_model)\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, d_model)\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:, :L, :]\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=5, d_model=128, nhead=4, num_layers=2, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        in_dim = NUM_JOINTS * NUM_CH\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 32, 3)\n",
    "        B, T, J, C = x.shape\n",
    "        x = x.reshape(B, T, J*C)\n",
    "        x = self.proj(x)\n",
    "        x = self.pe(x)\n",
    "        h = self.encoder(x)          # (B, T, d_model)\n",
    "        h = h.mean(dim=1)            # 全局平均\n",
    "        return self.cls(h)\n",
    "\n",
    "# 载入外部数据\n",
    "X_ext = np.load(SAVE_X)\n",
    "y_ext = np.load(SAVE_Y)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型定义（参数需与你训练时一致；如你保存的权重路径不同，请修改）\n",
    "model = TinyTransformer(num_classes=len(KEEP_LABELS))\n",
    "ckpt_path = \"data/transformer_pose5.pth\"  # 你之前保存的 transformer 权重（若名称不同改这里）\n",
    "if os.path.exists(ckpt_path):\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    try:\n",
    "        model.load_state_dict(state, strict=True)\n",
    "    except:\n",
    "        # 兼容只存 state_dict 的情况\n",
    "        model.load_state_dict(state.get(\"model\", state))\n",
    "    print(f\"Loaded weights: {ckpt_path}\")\n",
    "else:\n",
    "    print(f\"⚠ 找不到 {ckpt_path}，将使用随机初始化权重，仅做流程演示。\")\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    logits_all, y_all = [], []\n",
    "    BS = 64\n",
    "    for i in range(0, len(X_ext), BS):\n",
    "        xb = torch.tensor(X_ext[i:i+BS], dtype=torch.float32, device=device)\n",
    "        out = model(xb)\n",
    "        logits_all.append(out.cpu())\n",
    "        y_all.append(torch.tensor(y_ext[i:i+BS]))\n",
    "    logits_all = torch.cat(logits_all, dim=0)\n",
    "    y_all = torch.cat(y_all, dim=0)\n",
    "    pred = logits_all.argmax(dim=1).numpy()\n",
    "\n",
    "# 指标\n",
    "names = KEEP_LABELS\n",
    "print(\"\\n=== External Test Report ===\")\n",
    "print(classification_report(y_all, pred, target_names=names, digits=2))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_all, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28d3ace-08ee-4ad4-9276-ded77314f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Checking labels in 09_SY.pickle ======\n",
      "总帧数: 47199\n",
      "检查帧数: 20000\n",
      "有标签的帧: 6098\n",
      "📌 找到的所有 raw label 文本（最多显示前 30 个）:\n",
      "  p_lie: 2304\n",
      "  p_sit: 942\n",
      "  t_bed_turn: 588\n",
      "  p_situp: 509\n",
      "  idle: 415\n",
      "  t_sit_to_lie: 361\n",
      "   floor: 341\n",
      "  t_situp_to_sit: 331\n",
      "  signal: 155\n",
      "  t_lie_to_situp: 151\n",
      "  operate_object: 139\n",
      "  get_object: 109\n",
      "  place_object: 94\n",
      "   sofa: 1\n",
      "   t_situp_to_sit: 1\n",
      "\n",
      "====== Checking labels in 16_GZ.pickle ======\n",
      "总帧数: 33409\n",
      "检查帧数: 20000\n",
      "有标签的帧: 15224\n",
      "📌 找到的所有 raw label 文本（最多显示前 30 个）:\n",
      "  p_stand: 6557\n",
      "   kitchen_table: 3155\n",
      "  a_walk: 2348\n",
      "  p_sit: 1620\n",
      "   stool: 1536\n",
      "  p_bent: 1308\n",
      "  p_situp: 810\n",
      "  p_lie: 741\n",
      "   bed: 673\n",
      "  t_bed_turn: 589\n",
      "   chair: 257\n",
      "  t_straighten: 251\n",
      "  t_bend: 250\n",
      "  t_stand_to_sit: 196\n",
      "  t_sit_to_stand: 179\n",
      "  t_lie_to_situp: 176\n",
      "  t_situp_to_sit: 121\n",
      "  t_sit_to_lie: 76\n",
      "   p_stand: 8\n",
      "   t_bed_turn: 4\n",
      "   t_bend: 3\n",
      "   t_straighten: 3\n",
      "   t_stand_to_sit: 2\n",
      "   t_sit_to_stand: 2\n",
      "  idle: 1\n",
      "  bed: 1\n",
      "   t_sit_to_lie: 1\n",
      "\n",
      "====== Checking labels in 17_JP.pickle ======\n",
      "总帧数: 36546\n",
      "检查帧数: 20000\n",
      "有标签的帧: 16550\n",
      "📌 找到的所有 raw label 文本（最多显示前 30 个）:\n",
      "  idle: 4252\n",
      "  a_walk: 2889\n",
      "  p_stand: 2822\n",
      "  p_lie: 1211\n",
      "  t_bed_turn: 899\n",
      "  p_bent: 885\n",
      "  writing: 653\n",
      "  undefined: 562\n",
      "  p_sit: 524\n",
      "  t_stand_to_sit: 319\n",
      "  t_bend: 314\n",
      "  t_straighten: 283\n",
      "  t_sit_to_stand: 248\n",
      "  t_sit_to_lie: 191\n",
      "  p_situp: 186\n",
      "  t_lie_to_situp: 113\n",
      "   kitchen_table: 108\n",
      "  t_situp_to_sit: 102\n",
      "  signal: 85\n",
      "  get_object: 12\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "from collections import Counter\n",
    "\n",
    "label_keys = [\"physicalstate\",\"PhysicalState\",\"phys_state\",\"physState\"]\n",
    "\n",
    "def extract_all_labels(pickle_path, max_frames=20000):\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(f\"\\n====== Checking labels in {os.path.basename(pickle_path)} ======\")\n",
    "    total = len(data)\n",
    "    found_labels = Counter()\n",
    "    found_raw_texts = Counter()\n",
    "    frames_with_label = 0\n",
    "    frames_checked = 0\n",
    "\n",
    "    for rec in data[:max_frames]:\n",
    "        frames_checked += 1\n",
    "        label = None\n",
    "        # ---- 1) 顶层 physicalstate ----\n",
    "        for k in label_keys:\n",
    "            if k in rec:\n",
    "                label = rec[k]\n",
    "                break\n",
    "        # ---- 2) activities 下 physicalstate ----\n",
    "        if label is None:\n",
    "            acts = rec.get(\"activities\", {})\n",
    "            for k in label_keys:\n",
    "                if k in acts:\n",
    "                    label = acts[k]\n",
    "                    break\n",
    "            # 再检查 activities 里的其他 key 是否为 label\n",
    "            if label is None:\n",
    "                for v in acts.values():\n",
    "                    if v not in [None, {}, []]:\n",
    "                        label = v\n",
    "                        break\n",
    "\n",
    "        if label is not None:\n",
    "            frames_with_label += 1\n",
    "\n",
    "            # 统计原始标签文本（字符串 or dict / list 展开）\n",
    "            if isinstance(label, dict):\n",
    "                for vv in label.values():\n",
    "                    found_raw_texts[str(vv)] += 1\n",
    "            elif isinstance(label, (list, tuple)):\n",
    "                for vv in label:\n",
    "                    found_raw_texts[str(vv)] += 1\n",
    "            else:\n",
    "                found_raw_texts[str(label)] += 1\n",
    "\n",
    "    print(f\"总帧数: {total}\")\n",
    "    print(f\"检查帧数: {frames_checked}\")\n",
    "    print(f\"有标签的帧: {frames_with_label}\")\n",
    "    print(\"📌 找到的所有 raw label 文本（最多显示前 30 个）:\")\n",
    "    for k, v in found_raw_texts.most_common(30):\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return found_raw_texts\n",
    "\n",
    "\n",
    "files = [\n",
    "    \"data/09_SY.pickle\",\n",
    "    \"data/16_GZ.pickle\",\n",
    "    \"data/17_JP.pickle\"\n",
    "]\n",
    "\n",
    "all_outputs = []\n",
    "for f in files:\n",
    "    out = extract_all_labels(f, max_frames=20000)\n",
    "    all_outputs.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc33f883-9ffa-4eba-b120-c8e5b8d1657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 从新 pickle 构造 5 类测试集（宽松+兜底）...\n",
      "16_GZ.pickle 产出窗口: X=(0,) y=(0,) 标签分布=Counter()\n",
      "17_JP.pickle 产出窗口: X=(0,) y=(0,) 标签分布=Counter()\n",
      "各文件产出统计： [('16_GZ.pickle', 0, Counter()), ('17_JP.pickle', 0, Counter())]\n",
      "完整测试集: X_test= (0, 30, 96)  y_test= (0,)\n",
      "标签分布: Counter()\n",
      "💾 已保存到: data/X_test_pose5.npy / data/y_test_pose5.npy\n",
      "⚠️ 测试集为空，跳过评测。\n"
     ]
    }
   ],
   "source": [
    "import os, re, pickle, math, warnings\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =============================\n",
    "# 1) 可调参数（放宽+兜底）\n",
    "# =============================\n",
    "PICKLES = [\"data/16_GZ.pickle\", \"data/17_JP.pickle\"]   # 你也可以加上 \"data/09_SY.pickle\"\n",
    "KEEP = [\"a_walk\",\"p_sit\",\"p_stand\",\"p_lie\",\"t_bend\"]\n",
    "LABEL2ID = {k:i for i,k in enumerate(KEEP)}\n",
    "WINDOW = 30         # 每窗长度（放宽）\n",
    "STRIDE = 10         # 步长（更密集）\n",
    "MAJORITY = 0.40     # 主标签占比阈值（放宽）\n",
    "MIN_SEG_LEN = 10    # 连续同类片段最少帧（放宽）\n",
    "CONF3D_MIN = 0.20   # PoseNDF 3D 阈值（放宽）\n",
    "MISS_TOL = 0.25     # 允许25%关节为 NaN（放宽）\n",
    "SINGLE_PERSON = True  # 先限制单人；采不到再改 False\n",
    "\n",
    "SAVE_X = \"data/X_test_pose5.npy\"\n",
    "SAVE_Y = \"data/y_test_pose5.npy\"\n",
    "\n",
    "# 可选：评测已训练 Transformer 模型\n",
    "EVAL_MODEL = True\n",
    "MODEL_PATH = \"data/transformer_top5_improved.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =============================\n",
    "# 2) 标签规范化（合并同义/过程态→静态）\n",
    "# =============================\n",
    "def norm_label(s: str):\n",
    "    if s is None: return None\n",
    "    s = s.strip().lower()\n",
    "    # 常见噪声或无效\n",
    "    if s in [\"idle\",\"undefined\",\"signal\",\"operate_object\",\"get_object\",\"place_object\"]:\n",
    "        return None\n",
    "    # 家具/场景词忽略\n",
    "    if s in [\"bed\",\"chair\",\"stool\",\"kitchen_table\",\"sofa\",\"floor\",\"p_stand\"]:  # 注意 'p_stand'（带空格）也可能出现在你的数据里\n",
    "        return None\n",
    "    # 过程态映射\n",
    "    map_proc = {\n",
    "        \"t_sit_to_stand\":\"p_stand\",\n",
    "        \"t_stand_to_sit\":\"p_sit\",\n",
    "        \"t_sit_to_lie\":\"p_lie\",\n",
    "        \"t_lie_to_sit\":\"p_sit\",       # 若你的数据有这个\n",
    "        \"t_lie_to_situp\":\"p_sit\",\n",
    "        \"t_situp_to_sit\":\"p_sit\",\n",
    "        \"t_straighten\":\"t_bend\"       # 视作弯腰类\n",
    "    }\n",
    "    if s in map_proc: s = map_proc[s]\n",
    "    # 仅保留我们 5 类\n",
    "    if s in KEEP: return s\n",
    "    # 其他别名处理\n",
    "    alias = {\n",
    "        \"walk\":\"a_walk\",\n",
    "        \"stand\":\"p_stand\",\n",
    "        \"sit\":\"p_sit\",\n",
    "        \"lie\":\"p_lie\",\n",
    "        \"bend\":\"t_bend\"\n",
    "    }\n",
    "    return alias.get(s, None)\n",
    "\n",
    "# =============================\n",
    "# 3) 从单帧 dict 提取一帧 96 维特征（32关节×xyz）\n",
    "# =============================\n",
    "SELECT_32 = None  # 如果你的关节数>32可在此挑选索引；默认取前32并截/零填\n",
    "def frame_to_feat(d):\n",
    "    # 取第一人的3D关键点（你的数据通常是 list/array，长度=关节数）\n",
    "    poses3d = d.get(\"poses3d\", None)\n",
    "    if poses3d is None or len(poses3d)==0:\n",
    "        return None, 1.0  # 无3D，缺失比例=1\n",
    "    # 兼容各种存储：可能是 list/array/np.ndarray；取第一人或第一组\n",
    "    p = poses3d[0] if isinstance(poses3d, (list,tuple)) else poses3d\n",
    "    p = np.array(p)\n",
    "    # 形状兼容：(J,3) or (…,J,3)\n",
    "    if p.ndim==1: \n",
    "        return None, 1.0\n",
    "    if p.ndim>2:\n",
    "        # 取最后两维为 (J,3)\n",
    "        p = p.reshape(-1,3)\n",
    "    # 统一到 32 关节\n",
    "    J = p.shape[0]\n",
    "    if J >= 32:\n",
    "        if SELECT_32 is None:\n",
    "            p = p[:32]\n",
    "        else:\n",
    "            idx = np.array(SELECT_32, dtype=int)\n",
    "            p = p[idx]\n",
    "    else:\n",
    "        pad = np.zeros((32-J,3), dtype=float)\n",
    "        p = np.vstack([p, pad])\n",
    "    # 缺失比例\n",
    "    miss = np.isnan(p).mean()\n",
    "    # NaN→0\n",
    "    p = np.nan_to_num(p, nan=0.0)\n",
    "    feat = p.reshape(-1).astype(np.float32)  # (96,)\n",
    "    return feat, float(miss)\n",
    "\n",
    "def frame_conf3d(d):\n",
    "    sc = d.get(\"PoseNDF_score3d\", None)\n",
    "    if sc is None:\n",
    "        return None\n",
    "    sc = np.array(sc)\n",
    "    try:\n",
    "        return float(sc.squeeze())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def frame_label(d):\n",
    "    # activities 可能是 dict：{label:prob} 或 {label:1}\n",
    "    acts = d.get(\"activities\", {})\n",
    "    cand = []\n",
    "    for k,v in (acts.items() if isinstance(acts,dict) else []):\n",
    "        lab = norm_label(str(k))\n",
    "        if lab: cand.append(lab)\n",
    "    if not cand: \n",
    "        return None\n",
    "    # 多标签时取第一个可用\n",
    "    for lab in cand:\n",
    "        if lab in KEEP:\n",
    "            return lab\n",
    "    return None\n",
    "\n",
    "def frame_num_people(d):\n",
    "    # 优先用num_people，否则用 poses3d 的人数估计\n",
    "    n = d.get(\"num_people\", None)\n",
    "    if isinstance(n, (int,np.integer)): \n",
    "        return int(n)\n",
    "    poses3d = d.get(\"poses3d\", None)\n",
    "    if poses3d is None: \n",
    "        return 0\n",
    "    if isinstance(poses3d, list):\n",
    "        # 可能是 [ (J,3), (J,3) ] 或 [J,3]\n",
    "        if len(poses3d)>0 and isinstance(poses3d[0], (list,tuple,np.ndarray)) and np.array(poses3d[0]).ndim==2:\n",
    "            return len(poses3d)\n",
    "        else:\n",
    "            return 1\n",
    "    return 1\n",
    "\n",
    "# =============================\n",
    "# 4) 从一个 pickle 中抽窗\n",
    "# =============================\n",
    "def extract_from_one_pickle(path):\n",
    "    with open(path,\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    assert isinstance(data, list), f\"{path} 不是 list\"\n",
    "    Nf = len(data)\n",
    "    # 每帧：质量筛选 + 标签\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        lab = frame_label(d)\n",
    "        if lab is None:\n",
    "            frames.append(None); labels.append(None); continue\n",
    "        if SINGLE_PERSON and frame_num_people(d)!=1:\n",
    "            frames.append(None); labels.append(None); continue\n",
    "        feat, miss = frame_to_feat(d)\n",
    "        if feat is None or miss > MISS_TOL:\n",
    "            frames.append(None); labels.append(None); continue\n",
    "        conf = frame_conf3d(d)\n",
    "        if conf is not None and conf < CONF3D_MIN:\n",
    "            frames.append(None); labels.append(None); continue\n",
    "        frames.append(feat); labels.append(lab)\n",
    "\n",
    "    frames = np.array(frames, dtype=object)\n",
    "    # 滑窗（多数占比）\n",
    "    X, y = [], []\n",
    "    for s in range(0, Nf-WINDOW+1, STRIDE):\n",
    "        win_feats = frames[s:s+WINDOW]\n",
    "        win_labs  = labels[s:s+WINDOW]\n",
    "        if any(v is None for v in win_feats):\n",
    "            continue\n",
    "        # 多数占比\n",
    "        c = Counter([L for L in win_labs if L is not None])\n",
    "        if not c: \n",
    "            continue\n",
    "        maj, cnt = c.most_common(1)[0]\n",
    "        if maj in KEEP and cnt/len(win_labs) >= MAJORITY:\n",
    "            X.append(np.stack(win_feats, axis=0))     # (T,96)\n",
    "            y.append(LABEL2ID[maj])\n",
    "\n",
    "    # 兜底：如果没采到，按标签中心截窗（每标签每隔 K 帧取一个中心）\n",
    "    if len(X)==0:\n",
    "        centers = defaultdict(list)\n",
    "        for i,lab in enumerate(labels):\n",
    "            if lab in KEEP and frames[i] is not None:\n",
    "                centers[lab].append(i)\n",
    "        for lab, idxs in centers.items():\n",
    "            for c in idxs[::WINDOW]:  # 适当抽稀\n",
    "                s = max(0, c - WINDOW//2)\n",
    "                e = s + WINDOW\n",
    "                if e <= Nf and all(frames[s:e][j] is not None for j in range(WINDOW)):\n",
    "                    X.append(np.stack(frames[s:e], axis=0))\n",
    "                    y.append(LABEL2ID[lab])\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "\n",
    "def build_test_from_pickles(paths):\n",
    "    Xs, ys = [], []\n",
    "    stat = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"跳过（不存在）：{p}\")\n",
    "            continue\n",
    "        Xp, yp = extract_from_one_pickle(p)\n",
    "        stat.append( (os.path.basename(p), len(yp), Counter(yp.tolist())) )\n",
    "        if len(yp)>0:\n",
    "            Xs.append(Xp); ys.append(yp)\n",
    "        print(f\"{os.path.basename(p)} 产出窗口: X={Xp.shape} y={yp.shape} 标签分布={Counter(yp.tolist())}\")\n",
    "    if Xs:\n",
    "        X = np.concatenate(Xs, axis=0)\n",
    "        y = np.concatenate(ys, axis=0)\n",
    "    else:\n",
    "        X = np.zeros((0, WINDOW, 96), dtype=np.float32)\n",
    "        y = np.zeros((0,), dtype=np.int64)\n",
    "    return X, y, stat\n",
    "\n",
    "# =============================\n",
    "# 5) （可选）Transformer 评测（与之前一致）\n",
    "# =============================\n",
    "class PosEnc1D(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(pos*div)\n",
    "        pe[:,1::2] = torch.cos(pos*div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,D)\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, in_dim=96, d_model=128, nhead=4, num_layers=2, num_classes=5, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Linear(in_dim, d_model)\n",
    "        self.pe  = PosEnc1D(d_model, max_len=1024)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                               dim_feedforward=dim_ff, dropout=dropout,\n",
    "                                               batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.cls = nn.Linear(d_model, num_classes)\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,96)\n",
    "        h = self.inp(x)\n",
    "        h = self.pe(h)\n",
    "        h = self.encoder(h)\n",
    "        h = h.mean(dim=1)    # GAP\n",
    "        return self.cls(h)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, X, y):\n",
    "    model.eval()\n",
    "    loader_idx = np.arange(len(y))\n",
    "    probs = []\n",
    "    for i in range(0, len(loader_idx), 64):\n",
    "        idx = loader_idx[i:i+64]\n",
    "        xb  = torch.tensor(X[idx], dtype=torch.float32, device=device)\n",
    "        out = model(xb)\n",
    "        p = F.softmax(out, dim=-1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"\\n📊 测试集性能：\")\n",
    "    print(classification_report(y, y_pred, target_names=KEEP, digits=2))\n",
    "    print(\"混淆矩阵:\\n\", confusion_matrix(y, y_pred))\n",
    "\n",
    "def load_model(num_classes=5):\n",
    "    model = TinyTransformer(num_classes=num_classes)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "        missing, unexpected = model.load_state_dict(ckpt, strict=False)\n",
    "        if missing:   print(\"⚠️ Missing keys:\", missing)\n",
    "        if unexpected:print(\"⚠️ Unexpected keys:\", unexpected)\n",
    "        print(f\"✅ 已加载模型权重: {MODEL_PATH}\")\n",
    "    else:\n",
    "        print(\"⚠️ 未找到模型权重，使用随机初始化模型。\")\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "# =============================\n",
    "# 6) 主流程\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"▶ 从新 pickle 构造 5 类测试集（宽松+兜底）...\")\n",
    "    X_test, y_test, stat = build_test_from_pickles(PICKLES)\n",
    "    print(\"各文件产出统计：\", stat)\n",
    "    print(\"完整测试集: X_test=\", X_test.shape, \" y_test=\", y_test.shape)\n",
    "    print(\"标签分布:\", Counter(y_test.tolist()))\n",
    "    # 保存\n",
    "    np.save(SAVE_X, X_test)\n",
    "    np.save(SAVE_Y, y_test)\n",
    "    print(f\"💾 已保存到: {SAVE_X} / {SAVE_Y}\")\n",
    "\n",
    "    if EVAL_MODEL and len(y_test)>0:\n",
    "        model = load_model(num_classes=len(KEEP))\n",
    "        evaluate(model, DEVICE, X_test, y_test)\n",
    "    elif EVAL_MODEL:\n",
    "        print(\"⚠️ 测试集为空，跳过评测。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc4db7f-1901-44b7-93fc-440188a6c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== data/16_GZ.pickle ====\n",
      "总帧: 33409\n",
      "有效 3D pose 帧: 0\n",
      "连续 3D 段数量: 0\n",
      "最长连续 3D 段长度: 0\n",
      "所有段长度（前10）: []\n",
      "\n",
      "==== data/17_JP.pickle ====\n",
      "总帧: 36546\n",
      "有效 3D pose 帧: 0\n",
      "连续 3D 段数量: 0\n",
      "最长连续 3D 段长度: 0\n",
      "所有段长度（前10）: []\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "\n",
    "def check_pose3d(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    total = len(data)\n",
    "    valid = 0\n",
    "    valid_idx = []\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        p3d = d.get(\"poses3d\", None)\n",
    "\n",
    "        if p3d is None:\n",
    "            continue\n",
    "\n",
    "        arr = np.array(p3d)\n",
    "        # 检查是否包含真正的 (J,3)\n",
    "        if arr.ndim == 3 and arr.shape[-1] == 3:\n",
    "            valid += 1\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    print(\"\\n====\", path, \"====\")\n",
    "    print(\"总帧:\", total)\n",
    "    print(\"有效 3D pose 帧:\", valid)\n",
    "\n",
    "    # 检查连续段\n",
    "    seqs = []\n",
    "    if valid_idx:\n",
    "        seq = [valid_idx[0]]\n",
    "        for i in range(1, len(valid_idx)):\n",
    "            if valid_idx[i] == valid_idx[i-1] + 1:\n",
    "                seq.append(valid_idx[i])\n",
    "            else:\n",
    "                seqs.append(seq)\n",
    "                seq = [valid_idx[i]]\n",
    "        seqs.append(seq)\n",
    "\n",
    "    seq_lens = [len(s) for s in seqs]\n",
    "    print(\"连续 3D 段数量:\", len(seqs))\n",
    "    print(\"最长连续 3D 段长度:\", max(seq_lens) if seq_lens else 0)\n",
    "    print(\"所有段长度（前10）:\", seq_lens[:10])\n",
    "\n",
    "check_pose3d(\"data/16_GZ.pickle\")\n",
    "check_pose3d(\"data/17_JP.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21ccba7-b3c1-448b-8212-0e4d438ef380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== data/19_MM.pickle ====\n",
      "总帧: 34001\n",
      "有效 3D pose 帧: 0\n",
      "连续 3D 段数量: 0\n",
      "最长连续 3D 段长度: 0\n",
      "所有段长度（前10）: []\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "\n",
    "def check_pose3d(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    total = len(data)\n",
    "    valid = 0\n",
    "    valid_idx = []\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        p3d = d.get(\"poses3d\", None)\n",
    "\n",
    "        if p3d is None:\n",
    "            continue\n",
    "\n",
    "        arr = np.array(p3d)\n",
    "        # 检查是否包含真正的 (J,3)\n",
    "        if arr.ndim == 3 and arr.shape[-1] == 3:\n",
    "            valid += 1\n",
    "            valid_idx.append(i)\n",
    "\n",
    "    print(\"\\n====\", path, \"====\")\n",
    "    print(\"总帧:\", total)\n",
    "    print(\"有效 3D pose 帧:\", valid)\n",
    "\n",
    "    # 检查连续段\n",
    "    seqs = []\n",
    "    if valid_idx:\n",
    "        seq = [valid_idx[0]]\n",
    "        for i in range(1, len(valid_idx)):\n",
    "            if valid_idx[i] == valid_idx[i-1] + 1:\n",
    "                seq.append(valid_idx[i])\n",
    "            else:\n",
    "                seqs.append(seq)\n",
    "                seq = [valid_idx[i]]\n",
    "        seqs.append(seq)\n",
    "\n",
    "    seq_lens = [len(s) for s in seqs]\n",
    "    print(\"连续 3D 段数量:\", len(seqs))\n",
    "    print(\"最长连续 3D 段长度:\", max(seq_lens) if seq_lens else 0)\n",
    "    print(\"所有段长度（前10）:\", seq_lens[:10])\n",
    "\n",
    "check_pose3d(\"data/19_MM.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16830b9-7b7d-43a3-9453-6bde4a726471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
